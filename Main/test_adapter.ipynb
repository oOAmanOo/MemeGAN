{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T19:49:42.543795Z",
     "start_time": "2024-10-14T19:49:42.423064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from distutils.command.register import register\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "# from .Adpater.models import register\n",
    "from extractor import addImagePath, textExtraction, imageExtraction, textExtractReverse\n",
    "#\n",
    "from lavis.models.blip_models.blip import BlipBase\n",
    "from lavis.models.blip_models.blip_outputs import (\n",
    "    BlipOutput,\n",
    "    BlipIntermediateOutput,\n",
    ")\n",
    "from Main.Adpater.med import XBertLMHeadDecoder\n",
    "from Main.Adpater.vit import VisionTransformerEncoder, Block\n",
    "# # from omegaconf import OmegaConf"
   ],
   "id": "75b09252a28c7700",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Load the data and split the data",
   "id": "ed2444f7f1742063"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T19:51:47.582495Z",
     "start_time": "2024-10-14T19:51:47.576752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class OxfordDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, image, funny_score):\n",
    "        self.text = text\n",
    "        self.image = image\n",
    "        self.funny_score = funny_score\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = textExtraction(self.text[idx])\n",
    "        image = imageExtraction(self.image[idx])\n",
    "        funny_score = torch.tensor(float(self.funny_score[idx])).unsqueeze(0)\n",
    "        return text, image, funny_score"
   ],
   "id": "367bb5f9e88c7427",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T19:52:00.051570Z",
     "start_time": "2024-10-14T19:52:00.041097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if args.img - dir == 'Oxford_HIC':\n",
    "#     dirPath = '../Data/Oxford_HIC/oxford_hic_data.csv'\n",
    "#     imgPath = '../Data/Oxford_HIC/oxford_img/'\n",
    "# else:\n",
    "dirPath = '../Data/Instagram/Filter_' + 'wendys' + '.csv'\n",
    "imgPath = '../Data/Instagram/' + 'wendys' + '_img/'\n",
    "# load data\n",
    "data = pd.read_csv(dirPath)\n",
    "data = addImagePath(data, imgPath)\n",
    "# split data\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_text = train['caption'].tolist()\n",
    "train_image = train['image_id'].tolist()\n",
    "train_funny_score = train['funny_score'].tolist()\n",
    "test_text = test['caption'].tolist()\n",
    "test_image = test['image_id'].tolist()\n",
    "test_funny_score = test['funny_score'].tolist()\n",
    "\n",
    "train_dataset = OxfordDataset(train_text, train_image, train_funny_score)\n",
    "test_dataset = OxfordDataset(test_text, test_image, test_funny_score)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "id": "a2e97959ff465d2c",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataL = iter(train_loader)\n",
    "text, imgs, funny_score = next(dataL)\n",
    "print(\"shape of text: \", text.shape)\n",
    "print(\"shape of image: \", imgs.shape)\n",
    "print(\"shape of funny_score: \", funny_score.shape)"
   ],
   "id": "143a760fa17620db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T19:49:53.306562Z",
     "start_time": "2024-10-14T19:49:52.321306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 官方的Gemma #########################################################################################\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\", device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "gemmaConfig =  AutoConfig.from_pretrained('google/gemma-2-2b')\n",
    "########################################################################################################"
   ],
   "id": "2f7615cd627ef66c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tony Lab\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tony Lab\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Tony Lab\\.cache\\huggingface\\hub\\models--google--gemma-2-2b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tokenizer class GemmaTokenizer does not exist or is not currently imported.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m### 官方的Gemma #########################################################################################\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mAutoTokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgoogle/gemma-2-2b\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m gemma \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgoogle/gemma-2-2b\u001B[39m\u001B[38;5;124m\"\u001B[39m, device_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m,  torch_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mbfloat16)\n\u001B[0;32m      4\u001B[0m gemmaConfig \u001B[38;5;241m=\u001B[39m  AutoConfig\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgoogle/gemma-2-2b\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:655\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m    652\u001B[0m         tokenizer_class \u001B[38;5;241m=\u001B[39m tokenizer_class_from_name(tokenizer_class_candidate)\n\u001B[0;32m    654\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tokenizer_class \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 655\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    656\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTokenizer class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtokenizer_class_candidate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not exist or is not currently imported.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    657\u001B[0m         )\n\u001B[0;32m    658\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    660\u001B[0m \u001B[38;5;66;03m# Otherwise we have to be creative.\u001B[39;00m\n\u001B[0;32m    661\u001B[0m \u001B[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: Tokenizer class GemmaTokenizer does not exist or is not currently imported."
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "id": "7db9f299ac22b6ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "@register('blip_caption')\n",
    "class BlipCaption(BlipBase):\n",
    "    \"\"\"\n",
    "    BLIP captioning model for Screen2words.\n",
    "\n",
    "    Supported model types:\n",
    "        - base_coco: fine-tuned BLIP base model on COCO caption dataset (Karparthy split).\n",
    "\n",
    "    Pretrained weight url :\n",
    "        - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP/blip_coco_caption_base.pth\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,vit_type='base', med_config_path='./Adapter/configs/med_config.json',adapter_type=None, bert_adapter=None, visual_projection=None, tune_language=None, prompt=None, max_txt_len=40):\n",
    "        super().__init__()\n",
    "        # self.tokenizer = self.init_tokenizer()\n",
    "        # vision encoder\n",
    "        self.visual_encoder = VisionTransformerEncoder.from_config(vit_type = vit_type, adapter_type=adapter_type)\n",
    "        # text encoder + multimodal decoder\n",
    "        self.text_decoder = XBertLMHeadDecoder.from_config(med_config_path, False)\n",
    "\n",
    "        self.prompt = prompt\n",
    "        self.prompt_length = len(self.tokenizer(self.prompt).input_ids) - 1\n",
    "\n",
    "        self.max_txt_len = max_txt_len\n",
    "        self.adapter_type = adapter_type\n",
    "        self.bert_adapter = bert_adapter\n",
    "        self.tune_language = tune_language\n",
    "        # if((self.bert_adapter or self.tune_language) and self.adapter_type == None):\n",
    "        if visual_projection == \"linear\":\n",
    "            self.VLBridge = nn.Linear(768, 768)\n",
    "        elif visual_projection == \"ViT_block\":\n",
    "            self.VLBridge = Block(dim=768,num_heads=12,)\n",
    "        else:\n",
    "            self.VLBridge = None\n",
    "\n",
    "        # feed forward\n",
    "        self.feedForwardLinear = nn.Linear(768, 768)\n",
    "        self.feedForwardLayerNorm = nn.LayerNorm(768)\n",
    "\n",
    "        # gemma\n",
    "        self.gemmaLinearMaxTokens = nn.Linear(64, 16)\n",
    "        self.gemmaLinearBefore = nn.Linear(768, gemmaConfig.vocab_size)\n",
    "        self.gemmaSoftmax = nn.Softmax(dim=2)\n",
    "        self.gemma = nn.Sequential(*list(gemma.children())[:-1])\n",
    "        self.gemmaLm_head = nn.Sequential(*list(gemma.children())[1:])\n",
    "\n",
    "        # funny score\n",
    "        self.FunnyScorelinear1 = nn.Linear(768, 1)\n",
    "        self.FunnyScorelinear2 = nn.Linear(64, 1)\n",
    "\n",
    "    def gemmaGenerate(self, x):\n",
    "        with torch.no_grad():\n",
    "            # maximum 32 tokens\n",
    "            x = self.gemmaLinearMaxTokens(x.transpose(1, 2)).transpose(1, 2)\n",
    "            x = self.gemmaLinearBefore(x)\n",
    "            x = self.gemmaSoftmax(x)\n",
    "            # get max value of each row, total 32*64\n",
    "            top_k_values, top_k_indices = torch.topk(x, 1, dim=2, largest=True)\n",
    "            toGemma = textExtractReverse(top_k_indices).to(device)\n",
    "            # 使用gemma作為model的一部分\n",
    "            output = self.gemma(toGemma)\n",
    "            # output[0] = last_hidden_state\n",
    "            # output[1] = past_key_values\n",
    "        return output[0]\n",
    "\n",
    "    def forward_encoder(self, samples):\n",
    "        image_embeds = self.visual_encoder.forward_features(samples[\"image\"])\n",
    "        return image_embeds\n",
    "\n",
    "    def forward_decoder(self, text, image_embeds):\n",
    "        # prepare inputs for forwarding decoder\n",
    "        # text = self.tokenizer(\n",
    "        #     raw_text,\n",
    "        #     padding=\"longest\",\n",
    "        #     truncation=True,\n",
    "        #     max_length=self.max_txt_len,\n",
    "        #     return_tensors=\"pt\",\n",
    "        # ).to(self.device)\n",
    "        text.input_ids[:, 0] = self.tokenizer.bos_token_id\n",
    "\n",
    "        # prepare targets for forwarding decoder\n",
    "        decoder_targets = text.input_ids.masked_fill(\n",
    "            text.input_ids == self.tokenizer.pad_token_id, -100\n",
    "        )\n",
    "        decoder_targets[:, : self.prompt_length] = -100\n",
    "\n",
    "        # forward decoder\n",
    "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(\n",
    "            self.device\n",
    "        )\n",
    "        decoder_output = self.text_decoder(\n",
    "            input_ids=text.input_ids,\n",
    "            attention_mask=text.attention_mask,\n",
    "            encoder_hidden_states=image_embeds,\n",
    "            encoder_attention_mask=image_atts,\n",
    "            labels=decoder_targets,\n",
    "            return_dict=True,\n",
    "        )\n",
    "\n",
    "        return decoder_output, decoder_targets\n",
    "\n",
    "    def forward(self, text, image):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            samples (dict): A dictionary containing the following keys:\n",
    "                - image (torch.Tensor): A tensor of shape (batch_size, 3, H, W)\n",
    "                - text_input (list): A list of strings of length batch_size.\n",
    "        Returns:\n",
    "            output (BlipOutput): A BlipOutput object containing the following\n",
    "                attributes:\n",
    "                - loss (torch.Tensor): A scalar tensor containing the total loss. For BlipCaption, this is the same as the LM loss.\n",
    "                - loss_lm (torch.Tensor): A scalar tensor containing the LM loss.\n",
    "                - intermediate_outputs (BlipIntermediateOutput): A BlipIntermediateOutput object containing intermediate outputs.\n",
    "                  see :class:`lavis.models.blip_models.blip_outputs.BlipOutput` for more details.\n",
    "\n",
    "        Example:\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> from lavis.models import load_model_and_preprocess\n",
    "        >>> model, vis_processors, txt_processors = load_model_and_preprocess(\"blip_caption\")\n",
    "        >>> raw_image = Image.open(\"docs/data/merlion.png\").convert(\"RGB\")\n",
    "        >>> image = vis_processors[\"eval\"](raw_image).unsqueeze(0)\n",
    "        >>> text_input = [\"a large statue of a person spraying water from a fountain\"]\n",
    "        >>> samples = {\"image\": image, \"text_input\": text_input}\n",
    "        >>> output = model(samples)\n",
    "        >>> output.keys()\n",
    "        odict_keys(['intermediate_output', 'loss', 'loss_lm'])\n",
    "        >>> output.intermediate_output.image_embeds.shape\n",
    "        torch.Size([1, 577, 768])\n",
    "        >>> output.intermediate_output.decoder_labels.shape\n",
    "        torch.Size([1, 13])\n",
    "        ```\"\"\"\n",
    "\n",
    "        image_embeds = self.forward_encoder(image)\n",
    "        if self.VLBridge != None:\n",
    "            image_embeds = self.VLBridge(image_embeds)\n",
    "        decoder_output, decoder_targets = self.forward_decoder(text, image_embeds)\n",
    "\n",
    "        # decoder_out\n",
    "        # BlipOutput(\n",
    "        #     loss=decoder_output.loss,\n",
    "        #     loss_lm=decoder_output.loss,\n",
    "        #     intermediate_output=BlipIntermediateOutput(\n",
    "        #         image_embeds=image_embeds,\n",
    "        #         decoder_output=decoder_output,\n",
    "        #         decoder_labels=decoder_targets,\n",
    "        #     ),\n",
    "        # )\n",
    "\n",
    "        feature_fusion = self.feedForwardLinear(decoder_output.last_hidden_state)\n",
    "        feature_fusion = self.feedForwardLayerNorm(feature_fusion + feature_fusion)\n",
    "        feature_fusion = feature_fusion.squeeze(-1)\n",
    "        feature_fusion = feature_fusion.transpose(0, 1)\n",
    "        ####################### gemma  generate #######################\n",
    "        last_hidden_state = self.gemmaGenerate(feature_fusion)\n",
    "        output_text = self.gemmaLm_head(last_hidden_state)\n",
    "        ###############################################################\n",
    "\n",
    "        ######################### funny score #########################\n",
    "        output_funny_score = self.FunnyScorelinear1(feature_fusion).squeeze(-1)\n",
    "        output_funny_score = self.FunnyScorelinear2(output_funny_score).squeeze(-1)\n",
    "        ###############################################################\n",
    "\n",
    "        return output_text, output_funny_score\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        image,\n",
    "        use_nucleus_sampling=False,\n",
    "        num_beams=3,\n",
    "        max_length=100,\n",
    "        min_length=10,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.0,\n",
    "        num_captions=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            samples (dict): A dictionary containing the following keys:\n",
    "                - image (torch.Tensor): A tensor of shape (batch_size, 3, H, W)\n",
    "            use_nucleus_sampling (bool): Whether to use nucleus sampling. If False, use top-k sampling.\n",
    "            num_beams (int): Number of beams for beam search. 1 means no beam search.\n",
    "            max_length (int): The maximum length of the sequence to be generated.\n",
    "            min_length (int): The minimum length of the sequence to be generated.\n",
    "            top_p (float): The cumulative probability for nucleus sampling.\n",
    "            repetition_penalty (float): The parameter for repetition penalty. 1.0 means no penalty.\n",
    "            num_captions (int): Number of captions to be generated for each image.\n",
    "        Returns:\n",
    "            captions (list): A list of strings of length batch_size * num_captions.\n",
    "\n",
    "        Example:\n",
    "        ```python\n",
    "        >>> from PIL import Image\n",
    "        >>> from lavis.models import load_model_and_preprocess\n",
    "        >>> model, vis_processors, txt_processors = load_model_and_preprocess(\"blip_caption\")\n",
    "        >>> raw_image = Image.open(\"docs/data/merlion.png\").convert(\"RGB\")\n",
    "        >>> image = vis_processors[\"eval\"](raw_image).unsqueeze(0)\n",
    "        >>> samples = {\"image\": image}\n",
    "        >>> captions = model.generate(samples)\n",
    "        >>> captions\n",
    "        ['a large statue of a person spraying water from a fountain']\n",
    "        >>> captions = model.generate(samples, use_nucleus_sampling=True, num_captions=3)\n",
    "        >>> captions # example output, results may vary due to randomness\n",
    "        ['singapore showing the view of some building',\n",
    "        'the singapore harbor in twilight, as the weather is going down',\n",
    "        'the famous singapore fountain at sunset']\n",
    "        \"\"\"\n",
    "        # 有時後空格會失效，所以手動插入空格 <pad> = 0\n",
    "        def insert_zeros(list):\n",
    "            zeros = [0] * (2 * len(list) - 1)\n",
    "            zeros[::2] = list\n",
    "            return zeros\n",
    "\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "        generated_tokens = []\n",
    "        generated_tokens.append(tokenizer.bos_token_id) #<bos> = 2\n",
    "        text = torch.zeros_like(image).to(device)\n",
    "        lastTurn = False\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length + 1):\n",
    "                # prepare inputs for decoder generation.\n",
    "                encoder_out = self.forward_encoder(image)\n",
    "                if self.VLBridge != None:\n",
    "                    encoder_out = self.VLBridge(encoder_out)\n",
    "                image_embeds = torch.repeat_interleave(encoder_out, num_captions, 0)\n",
    "\n",
    "                # prompt = [self.prompt] * image_embeds.size(0)\n",
    "                # prompt = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "                # prompt.input_ids[:, 0] = self.tokenizer.bos_token_id\n",
    "                # prompt.input_ids = prompt.input_ids[:, :-1]\n",
    "\n",
    "                # get decoded text\n",
    "                decoder_out = self.text_decoder.generate_from_encoder(\n",
    "                    # tokenized_prompt=prompt,\n",
    "                    visual_embeds=image_embeds,\n",
    "                    sep_token_id=self.tokenizer.sep_token_id,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    use_nucleus_sampling=use_nucleus_sampling,\n",
    "                    num_beams=num_beams,\n",
    "                    max_length=max_length,\n",
    "                    min_length=min_length,\n",
    "                    top_p=top_p,\n",
    "                    repetition_penalty=repetition_penalty,\n",
    "                )\n",
    "\n",
    "                feature_fusion = self.feedForwardLinear(decoder_out)\n",
    "                feature_fusion = self.feedForwardLayerNorm(feature_fusion + feature_fusion)\n",
    "                feature_fusion = feature_fusion.squeeze(-1)\n",
    "                feature_fusion = feature_fusion.transpose(0, 1)\n",
    "\n",
    "                # gemma generate\n",
    "                last_hidden_state = self.gemmaGenerate(feature_fusion)\n",
    "                output_text = self.gemmaLm_head(last_hidden_state)\n",
    "\n",
    "                # funny score\n",
    "                output_funny_score = self.FunnyScorelinear1(feature_fusion).squeeze(-1)\n",
    "                output_funny_score = self.FunnyScorelinear2(output_funny_score).squeeze(-1)\n",
    "\n",
    "                if lastTurn: # show final funny score\n",
    "                    return generated_caption, output_funny_score\n",
    "                else:\n",
    "                    next_token_logits = output_text[:, -1, :]\n",
    "                    next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "                    next_token_id = torch.argmax(next_token_probs, dim=-1).item()\n",
    "                    generated_tokens.append(next_token_id)\n",
    "\n",
    "                    generated_caption = insert_zeros(generated_tokens)\n",
    "                    generated_caption = tokenizer.decode(generated_caption, skip_special_tokens=False)\n",
    "                    generated_caption = generated_caption.replace(\"<pad>\", \" \").replace(\"  \", \" \").split()\n",
    "                    generated_caption = [word for word in generated_caption if word[0] != \"<\"]\n",
    "                    generated_caption = \" \".join(generated_caption)\n",
    "\n",
    "                    text = textExtraction([generated_caption]).to(device)\n",
    "                    # text = text.transpose(0, 1)\n",
    "\n",
    "                    if next_token_id in gemmaConfig.eos_token_id or len(generated_caption.split()) > max_length:\n",
    "                        #<eos> = 1; <end_of_turn> = 107\n",
    "                        lastTurn = True\n",
    "                # outputs = self.tokenizer.batch_decode(decoder_out, skip_special_tokens=True)\n",
    "                # captions = [output[len(self.prompt) :] for output in outputs]\n",
    "\n",
    "                # return captions\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg):\n",
    "        # vision encoder\n",
    "        image_encoder = VisionTransformerEncoder.from_config(cfg)\n",
    "        # text encoder + multimodal decoder\n",
    "        text_decoder = XBertLMHeadDecoder.from_config(cfg)\n",
    "\n",
    "        prompt = cfg.get(\"prompt\", None)\n",
    "        max_txt_len = cfg.get(\"max_txt_len\", 40)\n",
    "\n",
    "        model = cls(image_encoder, text_decoder, prompt=prompt, max_txt_len=max_txt_len)\n",
    "        model.load_checkpoint_from_config(cfg)\n",
    "\n",
    "        return model"
   ],
   "id": "c7183fc80fae31d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Discriminator",
   "id": "96d9d533f1afe690"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Generator\n",
    "        self.g_linearFake = nn.Linear(256000, 768)\n",
    "        self.g_con_mlp1 = nn.Linear(768, 2)\n",
    "        self.g_con_mlp2 = nn.Linear(128, 1)\n",
    "        self.g_unc_mlp1 = nn.Linear(768, 1)\n",
    "        self.g_unc_mlp2 = nn.Linear(64, 1)\n",
    "        # Discriminator\n",
    "        self.d_linearFake = nn.Linear(gemmaConfig.vocab_size, 768)\n",
    "        self.d_con_mlp1_r2f = nn.Linear(768, 2)\n",
    "        self.d_con_mlp2_r2f = nn.Linear(256, 1)\n",
    "        self.d_con_mlp1_f2r = nn.Linear(768, 2)\n",
    "        self.d_con_mlp2_f2r = nn.Linear(256, 1)\n",
    "        self.d_con_mlp1_g = nn.Linear(768, 2)\n",
    "        self.d_con_mlp2_g = nn.Linear(128, 1)\n",
    "        self.d_con_mlp1_m = nn.Linear(768, 2)\n",
    "        self.d_con_mlp2_m = nn.Linear(128, 1)\n",
    "        self.d_unc_mlp1_r = nn.Linear(768, 1)\n",
    "        self.d_unc_mlp2_r = nn.Linear(64, 1)\n",
    "        self.d_unc_mlp1_g = nn.Linear(768, 1)\n",
    "        self.d_unc_mlp2_g = nn.Linear(64, 1)\n",
    "        self.d_unc_mlp1_m = nn.Linear(768, 1)\n",
    "        self.d_unc_mlp2_m = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, real_text, fake_text, image):\n",
    "        # real_text = [batch_size, 64, 768]\n",
    "        # fake_text = [batch_size, 256, 256000]\n",
    "        # image = [batch_size, 64, 768]\n",
    "        g_fake_text = self.g_linearFake(fake_text)\n",
    "\n",
    "        d_fake_text = self.d_linearFake(fake_text)\n",
    "        mismatched_text = torch.roll(real_text, 1, 0)\n",
    "\n",
    "        # conditional (contrastive)\n",
    "        C_r = torch.cat((real_text, image), dim=1)\n",
    "        g_C_g = torch.cat((g_fake_text, image), dim=1)\n",
    "        d_C_g = torch.cat((d_fake_text, image), dim=1)\n",
    "        C_m = torch.cat((mismatched_text, image), dim=1)\n",
    "        # contrastive discriminator\n",
    "        d_C_r2f = torch.cat((C_r, d_C_g), dim=1)\n",
    "        d_C_f2r = torch.cat((d_C_g, C_r), dim=1)\n",
    "        ########################## Generator ##########################\n",
    "        g_C_g = self.g_con_mlp1(g_C_g)\n",
    "        g_C_g = self.g_con_mlp2(g_C_g.transpose(1, 2)).squeeze(-1)\n",
    "        ###############################################################\n",
    "\n",
    "        ######################## Discriminator ########################\n",
    "        d_C_r2f = self.d_con_mlp1_r2f(d_C_r2f)\n",
    "        d_C_f2r = self.d_con_mlp1_f2r(d_C_f2r)\n",
    "        d_C_g = self.d_con_mlp1_g(d_C_g)\n",
    "        d_C_m = self.d_con_mlp1_m(C_m)\n",
    "        d_C_r2f = self.d_con_mlp2_r2f(d_C_r2f.transpose(1, 2)).squeeze(-1).unsqueeze(0)\n",
    "        d_C_f2r = self.d_con_mlp2_r2f(d_C_f2r.transpose(1, 2)).squeeze(-1).unsqueeze(0)\n",
    "        d_C_g = self.d_con_mlp2_g(d_C_g.transpose(1, 2)).squeeze(-1).unsqueeze(0)\n",
    "        d_C_m = self.d_con_mlp2_m(d_C_m.transpose(1, 2)).squeeze(-1).unsqueeze(0)\n",
    "        d_con_output = torch.cat((d_C_r2f, d_C_f2r, d_C_g, d_C_m), dim=0)\n",
    "        ###############################################################\n",
    "\n",
    "        #### unconditional ####\n",
    "        ########################## Generator ##########################\n",
    "        g_UC_g = self.g_unc_mlp1(g_fake_text).squeeze(-1)\n",
    "        g_UC_g = self.g_unc_mlp2(g_UC_g).squeeze(-1)\n",
    "        ###############################################################\n",
    "\n",
    "        ######################## Discriminator ########################\n",
    "        d_UC_r = self.d_unc_mlp1_r(real_text).squeeze(-1)\n",
    "        d_UC_g = self.d_unc_mlp1_g(d_fake_text).squeeze(-1)\n",
    "        d_UC_m = self.d_unc_mlp1_m(mismatched_text).squeeze(-1)\n",
    "        d_UC_r = self.d_unc_mlp2_r(d_UC_r).squeeze(-1).unsqueeze(0)\n",
    "        d_UC_g = self.d_unc_mlp2_g(d_UC_g).squeeze(-1).unsqueeze(0)\n",
    "        d_UC_m = self.d_unc_mlp2_m(d_UC_m).squeeze(-1).unsqueeze(0)\n",
    "        d_unc_output = torch.cat((d_UC_r, d_UC_g, d_UC_m), dim=0)\n",
    "        ###############################################################\n",
    "        # torch.Size([3, 32, 1])\n",
    "        return g_C_g, g_UC_g, d_con_output, d_unc_output"
   ],
   "id": "2424d81ad30f5641"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "77398e29241818c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# empty cuda memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "id": "d6a46edd7d44b662"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NetG = BlipCaption().to(device)\n",
    "NetD = Discriminator().to(device)\n",
    "optimizer_G = optim.Adam(NetG.parameters(), lr=0.001)\n",
    "optimizer_D = optim.Adam(NetD.parameters(), lr=0.001)\n",
    "train_losses_FC = []\n",
    "train_losses_G = []\n",
    "train_losses_D = []\n",
    "test_losses_FC = []\n",
    "test_losses_G = []\n",
    "test_losses_D = []\n",
    "save = []\n",
    "present_epoch = 1\n",
    "best_train_loss_FC = 9999\n",
    "best_train_loss_G = 9999\n",
    "best_train_loss_D = 9999\n",
    "best_test_loss_FC = 9999\n",
    "best_test_loss_G = 9999\n",
    "best_test_loss_D = 9999\n",
    "loss_data = pd.DataFrame()\n",
    "\n",
    "checkpoint = False\n",
    "if checkpoint:\n",
    "    checkpoint_G = torch.load('./AdpaterModel/test_save/test_save_2NetG.pth')\n",
    "    checkpoint_D = torch.load('./AdpaterModel/test_save/test_save_2NetD.pth')\n",
    "    NetG.load_state_dict(checkpoint_G['model_state_dict'])\n",
    "    NetD.load_state_dict(checkpoint_D['model_state_dict'])\n",
    "    optimizer_G.load_state_dict(checkpoint_G['optimizer_state_dict'])\n",
    "    optimizer_D.load_state_dict(checkpoint_D['optimizer_state_dict'])\n",
    "    train_losses_FC.append(checkpoint_G['FC_loss'])\n",
    "    train_losses_G.append(checkpoint_G['G_loss'])\n",
    "    train_losses_D.append(checkpoint_G['D_loss'])\n",
    "    present_epoch = checkpoint_G['epoch'] + 1\n",
    "\n",
    "\n",
    "\n",
    "funnyScoreLoss = nn.MSELoss()\n",
    "\n",
    "def generatorLoss(condition_logits, uncondition_logits):\n",
    "    result_fake = (torch.zeros(uncondition_logits.shape[0])).to(device)\n",
    "    con_loss = CrossEntropyLoss()(condition_logits, result_fake.to(torch.long))\n",
    "    unc_loss = BCEWithLogitsLoss()(uncondition_logits, result_fake)\n",
    "    loss = con_loss + unc_loss\n",
    "    return loss\n",
    "\n",
    "def discriminatorLoss(condition_logits, uncondition_logits):\n",
    "    result_true = (torch.ones(uncondition_logits[0].shape[0])).to(device)\n",
    "    result_fake = (torch.zeros(uncondition_logits[0].shape[0])).to(device)\n",
    "\n",
    "    con_r2f = CrossEntropyLoss()(condition_logits[0], result_fake.to(torch.long))\n",
    "    con_f2r = CrossEntropyLoss()(condition_logits[1], result_fake.to(torch.long))\n",
    "    con_f = CrossEntropyLoss()(condition_logits[2], result_fake.to(torch.long))\n",
    "    con_m = CrossEntropyLoss()(condition_logits[3], result_fake.to(torch.long))\n",
    "    unc_r = BCEWithLogitsLoss()(uncondition_logits[0], result_true)\n",
    "    unc_f = BCEWithLogitsLoss()(uncondition_logits[1], result_fake)\n",
    "    unc_m = BCEWithLogitsLoss()(uncondition_logits[2], result_fake)\n",
    "    loss = ((con_r2f + con_f2r)/2) + ((con_f + con_m)/2) + unc_r + ((unc_f + unc_m)/2)\n",
    "    return loss"
   ],
   "id": "1f04ee5e58cb9273"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "save_name = '20241011'\n",
    "if not os.path.exists('./AdpaterModel/'+save_name):\n",
    "    os.makedirs('./AdpaterModel/'+save_name)\n",
    "\n",
    "epochs = 2\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(epochs):\n",
    "    print(\"---------------------------------------- epoch \"+ str(epoch + present_epoch) +\" ---------------------------------------\")\n",
    "    train_loss_FC = 0\n",
    "    train_loss_G = 0\n",
    "    train_loss_D = 0\n",
    "    test_loss_FC = 0\n",
    "    test_loss_G = 0\n",
    "    test_loss_D = 0\n",
    "\n",
    "    ###################################### Train ######################################\n",
    "    with tqdm(train_loader, unit=\"batch\", leave=True) as tepoch:\n",
    "        tepoch.set_postfix({'Now': \" New batch preprocessing\"})\n",
    "        for text, image, funny_score in tepoch:\n",
    "            text = text.squeeze(1)\n",
    "            image = image.squeeze(1)\n",
    "            # print(text.shape, image.shape, funny_score.shape)\n",
    "            # torch.Size([32, 64, 768]) torch.Size([32, 64, 768]) torch.Size([32, 1])\n",
    "            ######################################################\n",
    "            # (1) Generate fake caption\n",
    "            ######################################################\n",
    "            tepoch.set_postfix({'Now': \" Generating fake caption -> Generator\"})\n",
    "            logits, output_funny_score = NetG(text.to(device).to(torch.float32), image.to(device).to(torch.float32))\n",
    "            tepoch.set_postfix({'Now': \" Generating fake caption -> Discriminator\"})\n",
    "            g_con_logits, g_unc_logits, d_con_logits, d_unc_logits = NetD(text.to(device).to(torch.float32), logits.detach().to(torch.float32), image.to(device).to(torch.float32))\n",
    "            ######################################################\n",
    "            # (3) Update Discriminator network\n",
    "            #####################################################\n",
    "            tepoch.set_postfix({'Now': \" Updating Discriminator network\"})\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D = discriminatorLoss(d_con_logits, d_unc_logits)\n",
    "            loss_D.backward(retain_graph=True)\n",
    "            optimizer_D.step()\n",
    "            train_loss_D += loss_D.item()\n",
    "            ######################################################\n",
    "            # (4) Update Generator network\n",
    "            ######################################################\n",
    "            tepoch.set_postfix({'Now': \" Updating Generator network\"})\n",
    "            optimizer_G.zero_grad()\n",
    "            loss_FC = funnyScoreLoss(output_funny_score, funny_score.to(device).to(torch.float32))\n",
    "            loss_FC.backward(retain_graph=True)\n",
    "            train_loss_FC += loss_FC.item()\n",
    "            loss_G = generatorLoss(g_con_logits, g_unc_logits)\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            train_loss_G += loss_G.item()\n",
    "            ######################################################\n",
    "            tepoch.set_postfix({'Now': \" New batch preprocessing\"})\n",
    "            ######################################################\n",
    "    train_loss_FC /= len(train_loader)\n",
    "    train_loss_G /= len(train_loader)\n",
    "    train_loss_D /= len(train_loader)\n",
    "    train_losses_FC.append(train_loss_FC)\n",
    "    train_losses_G.append(train_loss_G)\n",
    "    train_losses_D.append(train_loss_D)\n",
    "    ###################################### Train ######################################\n",
    "\n",
    "\n",
    "    ######################################  Test ######################################\n",
    "    with tqdm(test_loader, unit=\"batch\", leave=True) as tepoch:\n",
    "        tepoch.set_postfix({'Now': \" New batch preprocessing\"})\n",
    "        for text, image, funny_score in tepoch:\n",
    "            text = text.squeeze(1)\n",
    "            image = image.squeeze(1)\n",
    "            # Generator\n",
    "            tepoch.set_postfix({'Now': \" Generating fake caption -> Generator\"})\n",
    "            logits, output_funny_score = NetG(text.to(device).to(torch.float32), image.to(device).to(torch.float32))\n",
    "            # Discriminator\n",
    "            tepoch.set_postfix({'Now': \" Generating fake caption -> Discriminator\"})\n",
    "            g_con_logits, g_unc_logits, d_con_logits, d_unc_logits = NetD(text.to(device).to(torch.float32), logits.detach().to(torch.float32), image.to(device).to(torch.float32))\n",
    "            # loss\n",
    "            tepoch.set_postfix({'Now': \" Computing loss\"})\n",
    "            loss_FC = funnyScoreLoss(output_funny_score, funny_score.to(device).to(torch.float32))\n",
    "            loss_G = generatorLoss(g_con_logits, g_unc_logits)\n",
    "            loss_D = discriminatorLoss(d_con_logits, d_unc_logits)\n",
    "            test_loss_FC += loss_FC.item()\n",
    "            test_loss_G += loss_G.item()\n",
    "            test_loss_D += loss_D.item()\n",
    "    test_loss_FC /= len(test_loader)\n",
    "    test_loss_G /= len(test_loader)\n",
    "    test_loss_D /= len(test_loader)\n",
    "    test_losses_FC.append(test_loss_FC)\n",
    "    test_losses_G.append(test_loss_G)\n",
    "    test_losses_D.append(test_loss_D)\n",
    "    ######################################  Test ######################################\n",
    "\n",
    "    ######################################  Save ######################################\n",
    "    hasSaved = False\n",
    "    # 任一個loss小於最佳loss就存檔\n",
    "    if train_loss_FC < best_train_loss_FC and test_loss_FC < best_test_loss_FC:\n",
    "        best_train_loss_FC = train_loss_FC\n",
    "        best_test_loss_FC = test_loss_FC\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './AdpaterModel/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './AdpaterModel/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    if train_loss_G < best_train_loss_G and test_loss_G < best_test_loss_G:\n",
    "        best_train_loss_G = train_loss_G\n",
    "        best_test_loss_G = test_loss_G\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './AdpaterModel/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './AdpaterModel/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    if train_loss_D < best_train_loss_D and test_loss_D < best_test_loss_D:\n",
    "        best_train_loss_D = train_loss_D\n",
    "        best_test_loss_D = test_loss_D\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './AdpaterModel/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './AdpaterModel/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "\n",
    "    if hasSaved:\n",
    "        save.append(\"V\")\n",
    "    else:\n",
    "        save.append(\" \")\n",
    "\n",
    "    loss_data['train_FC'] = train_losses_FC\n",
    "    loss_data['train_G'] = train_losses_G\n",
    "    loss_data['train_D'] = train_losses_D\n",
    "    loss_data['test_FC'] = test_losses_FC\n",
    "    loss_data['test_G'] = test_losses_G\n",
    "    loss_data['test_D'] = test_losses_D\n",
    "    loss_data['save'] = save\n",
    "    loss_data.to_csv('./AdpaterModel/' + save_name + \"/\" + save_name + '_loss.csv', index=False)\n",
    "    ######################################  Save ######################################\n"
   ],
   "id": "1ca005d3486aced4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# @register_model",
   "id": "5cd5cdf581d8f3ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "list = timm.list_models(\"*swin*\")\n",
    "print(list)"
   ],
   "id": "795c879896aae9ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# save my model on timm\n",
    "timm.model_entrypoint()"
   ],
   "id": "7fd056a274b65042"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
