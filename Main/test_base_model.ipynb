{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T03:26:19.514174Z",
     "start_time": "2024-10-14T03:26:19.510397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from wheel.macosx_libfile import read_data\n",
    "\n",
    "from extractor import addImagePath, textExtraction, imageExtraction, textExtractReverse"
   ],
   "id": "7123375a60e24a10",
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Check the max length of the text data\n",
   "id": "3dae9bb9554dc2e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T05:27:32.060713Z",
     "start_time": "2024-10-09T05:27:31.979491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subUrlList = ['wendys','mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada','sonicdrivein']\n",
    "max_length = 0\n",
    "sum_length = 0\n",
    "for subUrl in subUrlList:\n",
    "    dirPath = '../Data/Instagram/Filter_' + subUrl + '.csv'\n",
    "    data = pd.read_csv(dirPath)\n",
    "    \n",
    "    for i in range(len(data['caption'])):\n",
    "        sum_length += len(str(data['caption'][i]).split())\n",
    "        if len(str(data['caption'][i]).split()) > max_length:\n",
    "            max_length = len(str(data['caption'][i]).split())\n",
    "            word = data['caption'][i]\n",
    "print(max_length)\n",
    "print(sum_length/len(data['caption']))"
   ],
   "id": "32667357b99a51a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373\n",
      "31.870794078061913\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:35:01.489143Z",
     "start_time": "2024-09-23T23:34:44.345758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_length = 0\n",
    "word = ''\n",
    "dirPath = '../Data/Oxford_HIC/oxford_hic_data.csv'\n",
    "data = pd.read_csv(dirPath)\n",
    "max_length = 0\n",
    "sum_length = 0\n",
    "index =0 \n",
    "counter  = 5\n",
    "# find the max word count of the text data['caption']\n",
    "for i in range(len(data['caption'])):\n",
    "    sum_length += len(str(data['caption'][i]).split())\n",
    "    if len(str(data['caption'][i]).split()) > max_length:\n",
    "        max_length = len(str(data['caption'][i]).split())\n",
    "        word = data['caption'][i]\n",
    "        index = i       \n",
    "        #\n",
    "print(max_length, i)\n",
    "print(sum_length/len(data['caption']))\n",
    "data.shape"
   ],
   "id": "b3b8a1b3461c9aed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5988\\3251324240.py:4: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(dirPath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9729 3657846\n",
      "10.514508397972905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3657847, 3)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 193
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:35:22.129734Z",
     "start_time": "2024-09-23T23:35:22.126848Z"
    }
   },
   "cell_type": "code",
   "source": "sum_length",
   "id": "21d6947ca3cd9281",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38460463"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 195
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Check the max word count of the text data",
   "id": "ca059487768954a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T18:33:30.989254Z",
     "start_time": "2024-09-25T18:33:28.914261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subUrlList = ['wendys','mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada','sonicdrivein']\n",
    "wordList = []\n",
    "total = 0\n",
    "for subUrl in subUrlList:\n",
    "    dirPath = '../Data/Instagram/Filter_' + subUrl + '.csv'\n",
    "    data = pd.read_csv(dirPath)\n",
    "    \n",
    "    for i in range(len(data['caption'])):\n",
    "        for word in str(data['caption'][i]).split():\n",
    "            if word not in wordList:\n",
    "                wordList.append(word)\n",
    "                total += 1\n",
    "                \n",
    "print(len(wordList))        \n",
    "print(total)"
   ],
   "id": "b2b206fcf154474e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15056\n",
      "15056\n"
     ]
    }
   ],
   "execution_count": 260
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-25T18:33:33.703355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wordList = []\n",
    "total = 0\n",
    "dirPath = '../Data/Oxford_HIC/oxford_hic_data.csv'\n",
    "data = pd.read_csv(dirPath)\n",
    "\n",
    "for i in range(len(data['caption'])):\n",
    "    for word in str(data['caption'][i]).split():\n",
    "        if word not in wordList:\n",
    "            wordList.append(word)\n",
    "            total += 1\n",
    "\n",
    "print(len(wordList))        \n",
    "print(total)"
   ],
   "id": "a02f3990a97e4a87",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5988\\496614120.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(dirPath)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[261], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(dirPath)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaption\u001B[39m\u001B[38;5;124m'\u001B[39m])):\n\u001B[1;32m----> 5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaption\u001B[39m\u001B[38;5;124m'\u001B[39m][i])\u001B[38;5;241m.\u001B[39msplit():\n\u001B[0;32m      6\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m wordList:\n\u001B[0;32m      7\u001B[0m             wordList\u001B[38;5;241m.\u001B[39mappend(word)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 261
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Load the data and split the data",
   "id": "6f301289c5bb2722"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:47:47.661978Z",
     "start_time": "2024-10-14T00:47:31.505879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if args.img - dir == 'Oxford_HIC':\n",
    "#     dirPath = '../Data/Oxford_HIC/oxford_hic_data.csv'\n",
    "#     imgPath = '../Data/Oxford_HIC/oxford_img/'\n",
    "# else:\n",
    "dirPath = '../Data/Instagram/Filter_' + 'wendys' + '.csv'\n",
    "imgPath = '../Data/Instagram/' + 'wendys' + '_img/'\n",
    "# load data\n",
    "data = pd.read_csv(dirPath)\n",
    "data = addImagePath(data, imgPath)\n",
    "# split data\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_text = textExtraction(train['caption'].tolist())\n",
    "train_image = imageExtraction(train['image_id'])\n",
    "train_funny_score = torch.tensor(train['funny_score'].to_numpy())\n",
    "test_text = textExtraction(test['caption'])\n",
    "test_image = imageExtraction(test['image_id'])\n",
    "test_funny_score = torch.tensor(test['funny_score'].to_numpy())"
   ],
   "id": "8bddfef236149dd6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:47:47.673085Z",
     "start_time": "2024-10-14T00:47:47.668979Z"
    }
   },
   "cell_type": "code",
   "source": "train_text.shape, train_image.shape, train_funny_score.shape",
   "id": "851fa717c816958b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([293, 64, 768]), torch.Size([293, 64, 768]), torch.Size([293]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:47:47.694673Z",
     "start_time": "2024-10-14T00:47:47.691424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "train_dataset = torch.utils.data.TensorDataset(train_text, train_image, train_funny_score)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_text, test_image, test_funny_score)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "127ff8b78bd6db6a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. LLM Test",
   "id": "807b418cfd673b81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T16:21:23.472827Z",
     "start_time": "2024-09-30T16:21:22.108642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 不確定是否為官方的 Gemini ############################################################################\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"describeai/gemini\")\n",
    "gemini = AutoModelForSeq2SeqLM.from_pretrained(\"describeai/gemini\")\n",
    "#######################################################################################################\n",
    "gemini"
   ],
   "id": "9368441165fc2667",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T23:07:28.813666Z",
     "start_time": "2024-10-13T23:07:16.732652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 官方的Gemma #########################################################################################\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\", device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "gemmaConfig =  AutoConfig.from_pretrained('google/gemma-2-2b')\n",
    "########################################################################################################"
   ],
   "id": "2b63e7fd2e5f833e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1405: UserWarning: Current model requires 6656 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f64625db5c04e14b36bb3215cf25d17"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T23:33:49.178647Z",
     "start_time": "2024-10-13T23:33:46.257835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 官方的Gemma #########################################################################################\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "gemmaConfig =  AutoConfig.from_pretrained('google/gemma-2-2b-it')\n",
    "########################################################################################################"
   ],
   "id": "e5f8f2c83fc25118",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88f7725d06f34f56b809770694c1830e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T16:47:42.602569Z",
     "start_time": "2024-09-22T16:47:41.350489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gemma.to(device)\n",
    "vocab_size = 256128  # 词汇表大小\n",
    "embedding_dim = 768  # 嵌入维度，与你的图像嵌入维度相同\n",
    "text_embedding = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "\n",
    "words = \"👻🔥😂😁👍🤦‍♀️🤦‍♂️🤷‍♀️🤷‍♂️✌🤞😉😎🎶😢💖🎉🌹💋👏🐱‍💻🐱‍🐉🐱‍👓✔👀😃✨😆🤔🤢🎁🫢 ha ha\"\n",
    "tokens = tokenizer(words, truncation=True, padding= 'max_length', max_length=100, return_tensors=\"pt\")\n",
    "output = text_embedding(tokens['input_ids'].to(device))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def find_closest_embeddings(x, embedding_matrix, top_k=1):\n",
    "    # Normalize both the input tensor x and the embedding matrix\n",
    "    x = F.normalize(x, dim=1)  # Normalize input tensor along feature dimension\n",
    "    embedding_matrix = F.normalize(embedding_matrix, dim=1)  # Normalize embedding matrix\n",
    "    \n",
    "    # Compute cosine similarity between x and embedding matrix\n",
    "    similarity = torch.matmul(x, embedding_matrix.T)  # Shape: [10, 50265]\n",
    "    \n",
    "    # Find top-k closest embeddings for each tensor in x\n",
    "    top_k_values, top_k_indices = torch.topk(similarity, top_k, dim=1)\n",
    "    \n",
    "    return top_k_indices, top_k_values\n",
    "\n",
    "\n",
    "# print(output.squeeze(0).shape)\n",
    "top_k_indices, top_k_values = find_closest_embeddings(output.squeeze(0), text_embedding.weight)\n",
    "# top_k_indices.shape\n",
    "indices = tokenizer.decode(top_k_indices.squeeze(-1))\n",
    "print(indices)"
   ],
   "id": "b5a1bd08816d845f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 768])\n",
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><bos>👻🔥😂😁👍🤦‍♀️🤦‍♂️🤷‍♀️🤷‍♂️✌🤞😉😎🎶😢💖🎉🌹💋👏🐱‍💻🐱‍🐉🐱‍👓✔👀😃✨😆🤔🤢🎁🫢 ha ha\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T20:36:50.602442Z",
     "start_time": "2024-09-23T20:36:50.584473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = \"👻🔥😂😁👍🤦‍♀️🤦‍♂️🤷‍♀️🤷‍♂️✌🤞😉😎🎶😢💖🎉🌹💋👏🐱‍💻🐱‍🐉🐱‍👓✔👀😃✨😆🤔🤢🎁🫢 ha ha\"\n",
    "tokens = tokenizer(words, truncation=True, padding= 'max_length', max_length=100, return_tensors=\"pt\")\n",
    "tokens"
   ],
   "id": "9a5b2095330cfcd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      2, 242538, 237638, 236471, 238429, 237019, 240525,  67292,\n",
       "         240525,  68399, 239921,  67292, 239921,  68399, 239529, 241807, 238309,\n",
       "         238859, 240438, 240116, 239208, 239548, 240315, 240887, 238499, 242993,\n",
       "         235879, 242482, 242993, 235879, 245092, 242993, 235879, 246943, 237488,\n",
       "         239220, 239938, 236309, 239312, 238918, 241769, 241227, 248165,    661,\n",
       "            661]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T19:03:27.261799Z",
     "start_time": "2024-09-23T19:03:27.257163Z"
    }
   },
   "cell_type": "code",
   "source": "gemma",
   "id": "f26de7efeee68a71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Generator",
   "id": "d8c8fefd0a84950b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T01:03:35.027397Z",
     "start_time": "2024-10-03T01:03:22.299642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\", device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "gemma"
   ],
   "id": "3de2433a8664699a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85ba5a944fab4f8b995d3f83c7f80630"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm()\n",
       "        (post_attention_layernorm): Gemma2RMSNorm()\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T06:03:20.898613Z",
     "start_time": "2024-10-09T06:03:13.883521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "gemma"
   ],
   "id": "c04d83141427f594",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a8ebecbda2641ec9a3e7af1afee356a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm()\n",
       "        (post_attention_layernorm): Gemma2RMSNorm()\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:47:50.836836Z",
     "start_time": "2024-10-14T00:47:47.707285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 官方的Gemma #########################################################################################\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "gemmaConfig =  AutoConfig.from_pretrained('google/gemma-2-2b-it')\n",
    "########################################################################################################"
   ],
   "id": "a2c8fa333911a14e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a64ada164a8422f8dfb355ebe443cee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T03:16:30.194006Z",
     "start_time": "2024-10-14T03:16:30.179857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # self attention\n",
    "        self.selfAttentionMultihead = nn.MultiheadAttention(768, 1)\n",
    "        self.selfAttentionLayerNorm = nn.LayerNorm(768)\n",
    "        self.selfAttentionLinear = nn.Linear(768, 768)\n",
    "        self.selfAttentionLayerNorm2 = nn.LayerNorm(768)\n",
    "        \n",
    "        # multihead attention\n",
    "        self.multiheadAttentionMultihead = nn.MultiheadAttention(768, 8)\n",
    "        self.multiheadAttentionLinear = nn.Linear(768, 768)\n",
    "        self.multiheadAttentionLayerNorm = nn.LayerNorm(768)\n",
    "        \n",
    "        # co-attention text\n",
    "        self.coAttentionTextMultihead = nn.MultiheadAttention(768, 1)\n",
    "        self.coAttentionTextLinear = nn.Linear(768, 768)\n",
    "        self.coAttentionTextLayerNorm = nn.LayerNorm(768)\n",
    "\n",
    "        # co-attention image\n",
    "        self.coAttentionImageMultihead = nn.MultiheadAttention(768, 1)\n",
    "        self.coAttentionImageLinear = nn.Linear(768, 768)\n",
    "        self.coAttentionImageLayerNorm = nn.LayerNorm(768)\n",
    "    \n",
    "        # feed forward\n",
    "        self.feedForwardLinear = nn.Linear(768, 768)\n",
    "        self.feedForwardLayerNorm = nn.LayerNorm(768)\n",
    "        \n",
    "        # gemma\n",
    "        self.gemmaLinearMaxTokens = nn.Linear(64, 16)\n",
    "        self.gemmaLinearBefore = nn.Linear(768, gemmaConfig.vocab_size)\n",
    "        self.gemmaSoftmax = nn.Softmax(dim=2)\n",
    "        self.gemma = nn.Sequential(*list(gemma.children())[:-1])\n",
    "        self.gemmaLm_head = nn.Sequential(*list(gemma.children())[1:])\n",
    "        \n",
    "        # funny score\n",
    "        self.FunnyScorelinear1 = nn.Linear(768, 1)\n",
    "        self.FunnyScorelinear2 = nn.Linear(64, 1)\n",
    "        \n",
    "    def self_multi(self, image, text):\n",
    "        # self attention module\n",
    "        self_out = self.selfAttentionMultihead(image, image, image)[0]\n",
    "        self_out = self.selfAttentionLinear(self_out)\n",
    "        self_out = self.selfAttentionLayerNorm(self_out + image)\n",
    "\n",
    "        # multihead attention module\n",
    "        multi_out = self.multiheadAttentionMultihead(text, text, text)[0]\n",
    "        multi_out = self.multiheadAttentionLinear(multi_out)\n",
    "        multi_out = self.multiheadAttentionLayerNorm(multi_out + text)\n",
    "        \n",
    "        return self_out, multi_out\n",
    "        \n",
    "    def co_attention(self, image, text):\n",
    "        # co-attention image module\n",
    "        visual_attending_textual = self.coAttentionTextMultihead(image, text, text)[0]\n",
    "        visual_attending_textual = self.coAttentionTextLinear(visual_attending_textual)\n",
    "        visual_attending_textual = self.coAttentionTextLayerNorm(visual_attending_textual + image)\n",
    "        \n",
    "        # co-attention text module\n",
    "        textual_attending_visual = self.coAttentionTextMultihead(text, image, image)[0]\n",
    "        textual_attending_visual = self.coAttentionTextLinear(textual_attending_visual)\n",
    "        textual_attending_visual = self.coAttentionTextLayerNorm(textual_attending_visual + text) \n",
    "        \n",
    "        return visual_attending_textual, textual_attending_visual              \n",
    "        \n",
    "    def gemmaGenerate(self, x):\n",
    "        with torch.no_grad():\n",
    "            # maximum 32 tokens\n",
    "            x = self.gemmaLinearMaxTokens(x.transpose(1, 2)).transpose(1, 2)\n",
    "            x = self.gemmaLinearBefore(x)\n",
    "            x = self.gemmaSoftmax(x)\n",
    "            # get max value of each row, total 32*64\n",
    "            top_k_values, top_k_indices = torch.topk(x, 1, dim=2, largest=True)\n",
    "            toGemma = textExtractReverse(top_k_indices).to(device)\n",
    "            # 使用gemma作為model的一部分\n",
    "            output = self.gemma(toGemma)\n",
    "            # output[0] = last_hidden_state\n",
    "            # output[1] = past_key_values\n",
    "            \n",
    "        return output[0]\n",
    "               \n",
    "    \n",
    "    def forward(self, text, image):\n",
    "        # max_seq_len = max(text.shape[1], image.shape[1])\n",
    "        # text = nn.functional.pad(text, (0, 0, 0, max_seq_len - text.shape[1]))\n",
    "        # image = nn.functional.pad(image, (0, 0, 0, max_seq_len - image.shape[1]))\n",
    "        text = text.transpose(0, 1)\n",
    "        image = image.transpose(0, 1)\n",
    "        \n",
    "        ######################### Transformer ######################### \n",
    "            # self attention\n",
    "        self_out, multi_out = self.self_multi(image, text)\n",
    "        # co-attention\n",
    "        visual_attending_textual, textual_attending_visual = self.co_attention(self_out, multi_out)\n",
    "        ###############################################################\n",
    "        \n",
    "        # feature fusion\n",
    "        feature_fusion = visual_attending_textual + textual_attending_visual\n",
    "        feature_fusion = self.feedForwardLinear(feature_fusion)\n",
    "        feature_fusion = self.feedForwardLayerNorm(feature_fusion + feature_fusion)\n",
    "        feature_fusion = feature_fusion.squeeze(-1)\n",
    "        feature_fusion = feature_fusion.transpose(0, 1)\n",
    "        ####################### gemma  generate #######################\n",
    "        last_hidden_state = self.gemmaGenerate(feature_fusion)\n",
    "        output_text = self.gemmaLm_head(last_hidden_state)\n",
    "        ###############################################################\n",
    "        \n",
    "        ######################### funny score #########################\n",
    "        output_funny_score = self.FunnyScorelinear1(feature_fusion).squeeze(-1)\n",
    "        output_funny_score = self.FunnyScorelinear2(output_funny_score).squeeze(-1)\n",
    "        ###############################################################\n",
    "        \n",
    "        return output_text, output_funny_score\n",
    "    \n",
    "    def generate(self, image, max_length = 100):\n",
    "        generated_tokens = []\n",
    "        generated_tokens.append(2) #<bos> = 2\n",
    "        text = torch.zeros_like(image).to(device)\n",
    "        text = text.transpose(0, 1)\n",
    "        image = image.transpose(0, 1)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "        \n",
    "        # 有時後空格會失效，所以手動插入空格 <pad> = 0\n",
    "        def insert_zeros(list):\n",
    "            zeros = [0] * (2 * len(list) - 1)\n",
    "            zeros[::2] = list\n",
    "            return zeros\n",
    "        \n",
    "        lastTurn = False\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length + 1):\n",
    "                # Transformer\n",
    "                # self attention\n",
    "                self_out, multi_out = self.self_multi(image, text)\n",
    "                # co-attention\n",
    "                visual_attending_textual, textual_attending_visual = self.co_attention(self_out, multi_out)\n",
    "                \n",
    "                # feature fusion\n",
    "                feature_fusion = visual_attending_textual + textual_attending_visual\n",
    "                feature_fusion = self.feedForwardLinear(feature_fusion)\n",
    "                feature_fusion = self.feedForwardLayerNorm(feature_fusion + feature_fusion)\n",
    "                feature_fusion = feature_fusion.squeeze(-1)\n",
    "                feature_fusion = feature_fusion.transpose(0, 1)\n",
    "                \n",
    "                # gemma generate\n",
    "                last_hidden_state = self.gemmaGenerate(feature_fusion)\n",
    "                output_text = self.gemmaLm_head(last_hidden_state)\n",
    "                \n",
    "                # funny score\n",
    "                output_funny_score = self.FunnyScorelinear1(feature_fusion).squeeze(-1)\n",
    "                output_funny_score = self.FunnyScorelinear2(output_funny_score).squeeze(-1)\n",
    "                \n",
    "                if lastTurn: # show final funny score\n",
    "                    return generated_caption, output_funny_score\n",
    "                else:\n",
    "                    next_token_logits = output_text[:, -1, :]\n",
    "                    next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "                    next_token_id = torch.argmax(next_token_probs, dim=-1).item()\n",
    "                    generated_tokens.append(next_token_id)\n",
    "                    \n",
    "                    generated_caption = insert_zeros(generated_tokens)\n",
    "                    generated_caption = tokenizer.decode(generated_caption, skip_special_tokens=False)\n",
    "                    generated_caption = generated_caption.replace(\"<pad>\", \" \").replace(\"  \", \" \").split()\n",
    "                    generated_caption = [word for word in generated_caption if word[0] != \"<\"]\n",
    "                    generated_caption = \" \".join(generated_caption)\n",
    "                                               \n",
    "                    text = textExtraction([generated_caption]).to(device)\n",
    "                    text = text.transpose(0, 1)\n",
    "                    \n",
    "                    if next_token_id in gemmaConfig.eos_token_id or len(generated_caption.split()) > max_length: \n",
    "                        #<eos> = 1; <end_of_turn> = 107\n",
    "                        lastTurn = True"
   ],
   "id": "6df9340e53bfbe2a",
   "outputs": [],
   "execution_count": 130
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# vision encoder\n",
    "        self.visual_encoder = VisionTransformerEncoder.from_config(vit_type = vit_type, adapter_type=adapter_type)\n",
    "        # text encoder + multimodal decoder\n",
    "        self.text_decoder = XBertLMHeadDecoder.from_config(med_config_path, False)\n",
    "\n",
    "        self.prompt = prompt\n",
    "        self.prompt_length = len(self.tokenizer(self.prompt).input_ids) - 1\n",
    "\n",
    "        self.max_txt_len = max_txt_len\n",
    "        self.adapter_type = adapter_type\n",
    "        self.bert_adapter = bert_adapter\n",
    "        self.tune_language = tune_language\n",
    "        # if((self.bert_adapter or self.tune_language) and self.adapter_type == None):\n",
    "        if visual_projection == \"linear\":\n",
    "            self.VLBridge = nn.Linear(768, 768)\n",
    "        elif visual_projection == \"ViT_block\":\n",
    "            self.VLBridge = Block(dim=768,num_heads=12,)\n",
    "        else:\n",
    "            self.VLBridge = None"
   ],
   "id": "a6fbc10fcbbbc6d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Discriminator",
   "id": "e431f844eea97c94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T03:50:05.533911Z",
     "start_time": "2024-10-14T03:50:05.523869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Generator\n",
    "        self.g_linearFake = nn.Linear(256000, 768)\n",
    "        self.g_con_mlp1 = nn.Linear(768, 2)\n",
    "        self.g_con_mlp2 = nn.Linear(128, 1)\n",
    "        self.g_unc_mlp1 = nn.Linear(768, 1)\n",
    "        self.g_unc_mlp2 = nn.Linear(64, 1)\n",
    "        # Discriminator\n",
    "        self.d_linearFake = nn.Linear(gemmaConfig.vocab_size, 768)\n",
    "        self.d_con_mlp1_r2f = nn.Linear(768, 2)\n",
    "        self.d_con_mlp2_r2f = nn.Linear(256, 1)\n",
    "        self.d_con_mlp1_f2r = nn.Linear(768, 2)\n",
    "        self.d_con_mlp2_f2r = nn.Linear(256, 1)\n",
    "        self.d_con_mlp1_g = nn.Linear(768, 2)\n",
    "        self.d_con_mlp2_g = nn.Linear(128, 1)\n",
    "        self.d_con_mlp1_m = nn.Linear(768, 2)\n",
    "        self.d_con_mlp2_m = nn.Linear(128, 1)\n",
    "        self.d_unc_mlp1_r = nn.Linear(768, 1)\n",
    "        self.d_unc_mlp2_r = nn.Linear(64, 1)\n",
    "        self.d_unc_mlp1_g = nn.Linear(768, 1)\n",
    "        self.d_unc_mlp2_g = nn.Linear(64, 1)\n",
    "        self.d_unc_mlp1_m = nn.Linear(768, 1)\n",
    "        self.d_unc_mlp2_m = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, real_text, fake_text, image):   \n",
    "        # real_text = [batch_size, 64, 768]\n",
    "        # fake_text = [batch_size, 256, 256000]\n",
    "        # image = [batch_size, 64, 768]\n",
    "        g_fake_text = self.g_linearFake(fake_text)\n",
    "        \n",
    "        d_fake_text = self.d_linearFake(fake_text)\n",
    "        mismatched_text = torch.roll(real_text, 1, 0)\n",
    "        \n",
    "        # conditional (contrastive)\n",
    "        C_r = torch.cat((real_text, image), dim=1)\n",
    "        g_C_g = torch.cat((g_fake_text, image), dim=1)\n",
    "        d_C_g = torch.cat((d_fake_text, image), dim=1)\n",
    "        C_m = torch.cat((mismatched_text, image), dim=1)\n",
    "        # contrastive discriminator\n",
    "        d_C_r2f = torch.cat((C_r, d_C_g), dim=1)\n",
    "        d_C_f2r = torch.cat((d_C_g, C_r), dim=1)\n",
    "        ########################## Generator ##########################\n",
    "        g_C_g = self.g_con_mlp1(g_C_g)\n",
    "        g_C_g = self.g_con_mlp2(g_C_g.transpose(1,2)).squeeze(-1)\n",
    "        ###############################################################\n",
    "        \n",
    "        ######################## Discriminator ########################\n",
    "        d_C_r2f = self.d_con_mlp1_r2f(d_C_r2f)\n",
    "        d_C_f2r = self.d_con_mlp1_f2r(d_C_f2r)\n",
    "        d_C_g = self.d_con_mlp1_g(d_C_g)\n",
    "        d_C_m = self.d_con_mlp1_m(C_m)\n",
    "        d_C_r2f = self.d_con_mlp2_r2f(d_C_r2f.transpose(1,2)).squeeze(-1).unsqueeze(0)\n",
    "        d_C_f2r = self.d_con_mlp2_r2f(d_C_f2r.transpose(1,2)).squeeze(-1).unsqueeze(0)\n",
    "        d_C_g = self.d_con_mlp2_g(d_C_g.transpose(1,2)).squeeze(-1).unsqueeze(0)\n",
    "        d_C_m = self.d_con_mlp2_m(d_C_m.transpose(1,2)).squeeze(-1).unsqueeze(0)\n",
    "        d_con_output = torch.cat((d_C_r2f, d_C_f2r, d_C_g, d_C_m), dim=0)\n",
    "        ###############################################################\n",
    "        \n",
    "        \n",
    "        #### unconditional ####\n",
    "        ########################## Generator ##########################\n",
    "        g_UC_g = self.g_unc_mlp1(g_fake_text).squeeze(-1)\n",
    "        g_UC_g = self.g_unc_mlp2(g_UC_g).squeeze(-1)\n",
    "        ###############################################################\n",
    "        \n",
    "        ######################## Discriminator ########################\n",
    "        d_UC_r  = self.d_unc_mlp1_r(real_text).squeeze(-1)\n",
    "        d_UC_g  = self.d_unc_mlp1_g(d_fake_text).squeeze(-1)\n",
    "        d_UC_m  = self.d_unc_mlp1_m(mismatched_text).squeeze(-1)\n",
    "        d_UC_r = self.d_unc_mlp2_r(d_UC_r).squeeze(-1).unsqueeze(0)\n",
    "        d_UC_g = self.d_unc_mlp2_g(d_UC_g).squeeze(-1).unsqueeze(0)\n",
    "        d_UC_m = self.d_unc_mlp2_m(d_UC_m).squeeze(-1).unsqueeze(0)\n",
    "        d_unc_output = torch.cat((d_UC_r, d_UC_g, d_UC_m), dim=0)\n",
    "        ###############################################################\n",
    "        # torch.Size([3, 32, 1])\n",
    "        return g_C_g, g_UC_g, d_con_output, d_unc_output"
   ],
   "id": "ff75f88ea94b5045",
   "outputs": [],
   "execution_count": 182
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T03:50:06.224767Z",
     "start_time": "2024-10-14T03:50:06.120574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# empty cuda memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "id": "d5c9b5f31e3aa208",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 183
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T03:50:08.466047Z",
     "start_time": "2024-10-14T03:50:06.488200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NetG = Generator().to(device)\n",
    "NetD = Discriminator().to(device)\n",
    "optimizer_G = optim.Adam(NetG.parameters(), lr=0.001)\n",
    "optimizer_D = optim.Adam(NetD.parameters(), lr=0.001)\n",
    "train_losses_FC = []\n",
    "train_losses_G = []\n",
    "train_losses_D = []\n",
    "test_losses_FC = []\n",
    "test_losses_G = []\n",
    "test_losses_D = []\n",
    "save = []\n",
    "present_epoch = 1\n",
    "best_train_loss_FC = 9999\n",
    "best_train_loss_G = 9999\n",
    "best_train_loss_D = 9999\n",
    "best_test_loss_FC = 9999\n",
    "best_test_loss_G = 9999\n",
    "best_test_loss_D = 9999\n",
    "loss_data = pd.DataFrame()\n",
    "\n",
    "checkpoint = False\n",
    "if checkpoint:\n",
    "    checkpoint_G = torch.load('./Model/test_save/test_save_2NetG.pth')\n",
    "    checkpoint_D = torch.load('./Model/test_save/test_save_2NetD.pth')\n",
    "    NetG.load_state_dict(checkpoint_G['model_state_dict'])\n",
    "    NetD.load_state_dict(checkpoint_D['model_state_dict'])\n",
    "    optimizer_G.load_state_dict(checkpoint_G['optimizer_state_dict'])\n",
    "    optimizer_D.load_state_dict(checkpoint_D['optimizer_state_dict'])\n",
    "    train_losses_FC.append(checkpoint_G['FC_loss'])\n",
    "    train_losses_G.append(checkpoint_G['G_loss'])\n",
    "    train_losses_D.append(checkpoint_G['D_loss'])\n",
    "    present_epoch = checkpoint_G['epoch'] + 1\n",
    "\n",
    "    \n",
    "\n",
    "funnyScoreLoss = nn.MSELoss()\n",
    "\n",
    "def generatorLoss(condition_logits, uncondition_logits):\n",
    "    result_fake = (torch.zeros(uncondition_logits.shape[0])).to(device)\n",
    "    con_loss = CrossEntropyLoss()(condition_logits, result_fake.to(torch.long))\n",
    "    unc_loss = BCEWithLogitsLoss()(uncondition_logits, result_fake)\n",
    "    loss = con_loss + unc_loss\n",
    "    return loss\n",
    "\n",
    "def discriminatorLoss(condition_logits, uncondition_logits):\n",
    "    result_true = (torch.ones(uncondition_logits[0].shape[0])).to(device)\n",
    "    result_fake = (torch.zeros(uncondition_logits[0].shape[0])).to(device)\n",
    "    \n",
    "    con_r2f = CrossEntropyLoss()(condition_logits[0], result_fake.to(torch.long))\n",
    "    con_f2r = CrossEntropyLoss()(condition_logits[1], result_fake.to(torch.long))\n",
    "    con_f = CrossEntropyLoss()(condition_logits[2], result_fake.to(torch.long))\n",
    "    con_m = CrossEntropyLoss()(condition_logits[3], result_fake.to(torch.long))\n",
    "    unc_r = BCEWithLogitsLoss()(uncondition_logits[0], result_true)\n",
    "    unc_f = BCEWithLogitsLoss()(uncondition_logits[1], result_fake)\n",
    "    unc_m = BCEWithLogitsLoss()(uncondition_logits[2], result_fake)\n",
    "    loss = ((con_r2f + con_f2r)/2) + ((con_f + con_m)/2) + unc_r + ((unc_f + unc_m)/2)\n",
    "    return loss"
   ],
   "id": "9dc4843819037573",
   "outputs": [],
   "execution_count": 184
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T03:53:12.874294Z",
     "start_time": "2024-10-14T03:50:08.473049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_name = 'test'\n",
    "if not os.path.exists('./Model/'+save_name):\n",
    "    os.makedirs('./Model/'+save_name)\n",
    "    \n",
    "epochs = 2\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(epochs):\n",
    "    print(\"---------------------------------------- epoch \"+ str(epoch + present_epoch) +\" ---------------------------------------\")\n",
    "    train_loss_FC = 0\n",
    "    train_loss_G = 0\n",
    "    train_loss_D = 0\n",
    "    test_loss_FC = 0\n",
    "    test_loss_G = 0\n",
    "    test_loss_D = 0\n",
    "    \n",
    "    ###################################### Train ######################################\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for text, image, funny_score in tepoch:\n",
    "            ######################################################\n",
    "                # (1) Generate fake caption\n",
    "            ######################################################\n",
    "            logits, output_funny_score = NetG(text.to(device).to(torch.float32), image.to(device).to(torch.float32))\n",
    "            g_con_logits, g_unc_logits, d_con_logits, d_unc_logits = NetD(text.to(device).to(torch.float32), logits.detach().to(torch.float32), image.to(device).to(torch.float32))\n",
    "            ######################################################\n",
    "                # (3) Update Discriminator network\n",
    "            #####################################################\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D = discriminatorLoss(d_con_logits, d_unc_logits)\n",
    "            loss_D.backward(retain_graph=True)\n",
    "            optimizer_D.step()\n",
    "            train_loss_D += loss_D.item()\n",
    "            ######################################################\n",
    "                # (4) Update Generator network\n",
    "            ######################################################\n",
    "            optimizer_G.zero_grad()\n",
    "            loss_FC = funnyScoreLoss(output_funny_score, funny_score.to(device).to(torch.float32))\n",
    "            loss_FC.backward(retain_graph=True)\n",
    "            train_loss_FC += loss_FC.item()\n",
    "            loss_G = generatorLoss(g_con_logits, g_unc_logits)\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            train_loss_G += loss_G.item()\n",
    "            ######################################################\n",
    "            tepoch.set_postfix({'FC_loss': loss_FC.item(), 'G_loss': loss_G.item(), 'D_loss': loss_D.item()})\n",
    "            ######################################################\n",
    "    train_loss_FC /= len(train_loader)\n",
    "    train_loss_G /= len(train_loader)\n",
    "    train_loss_D /= len(train_loader)\n",
    "    train_losses_FC.append(train_loss_FC)\n",
    "    train_losses_G.append(train_loss_G)\n",
    "    train_losses_D.append(train_loss_D)\n",
    "    ###################################### Train ######################################\n",
    "    \n",
    "    \n",
    "    ######################################  Test ######################################\n",
    "    with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
    "        for text, image, funny_score in tepoch:\n",
    "            # Generator\n",
    "            logits, output_funny_score = NetG(text.to(device).to(torch.float32), image.to(device).to(torch.float32))\n",
    "            # Discriminator\n",
    "            gen_logits, con_logits, unc_logits = NetD(text.to(device).to(torch.float32), logits.detach().to(torch.float32), image.to(device).to(torch.float32))\n",
    "            # loss\n",
    "            loss_FC = funnyScoreLoss(output_funny_score, funny_score.to(device).to(torch.float32))\n",
    "            loss_G = generatorLoss(gen_logits)\n",
    "            loss_D = discriminatorLoss(unc_logits, con_logits)\n",
    "            test_loss_FC += loss_FC.item()\n",
    "            test_loss_G += loss_G.item()\n",
    "            test_loss_D += loss_D.item()\n",
    "            tepoch.set_postfix({'FC_loss': test_loss_FC, 'G_loss': test_loss_G, 'D_loss': test_loss_D})\n",
    "    test_loss_FC /= len(test_loader)\n",
    "    test_loss_G /= len(test_loader)\n",
    "    test_loss_D /= len(test_loader)\n",
    "    test_losses_FC.append(test_loss_FC)\n",
    "    test_losses_G.append(test_loss_G)\n",
    "    test_losses_D.append(test_loss_D)\n",
    "    ######################################  Test ######################################\n",
    "\n",
    "    ######################################  Save ######################################\n",
    "    hasSaved = False\n",
    "    # 任一個loss小於最佳loss就存檔\n",
    "    if train_loss_FC < best_train_loss_FC and test_loss_FC < best_test_loss_FC:\n",
    "        best_train_loss_FC = train_loss_FC\n",
    "        best_test_loss_FC = test_loss_FC\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    if train_loss_G < best_train_loss_G and test_loss_G < best_test_loss_G:\n",
    "        best_train_loss_G = train_loss_G\n",
    "        best_test_loss_G = test_loss_G\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    if train_loss_D < best_train_loss_D and test_loss_D < best_test_loss_D:\n",
    "        best_train_loss_D = train_loss_D\n",
    "        best_test_loss_D = test_loss_D\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    \n",
    "    if hasSaved:\n",
    "        save.append(\"V\")\n",
    "    else:\n",
    "        save.append(\" \")\n",
    "\n",
    "    loss_data['train_FC'] = train_losses_FC\n",
    "    loss_data['train_G'] = train_losses_G\n",
    "    loss_data['train_D'] = train_losses_D\n",
    "    loss_data['test_FC'] = test_losses_FC\n",
    "    loss_data['test_G'] = test_losses_G\n",
    "    loss_data['test_D'] = test_losses_D\n",
    "    loss_data['save'] = save\n",
    "    loss_data.to_csv('./Model/' + save_name + \"/\" + save_name + '_loss.csv', index=False)\n",
    "    ######################################  Save ######################################"
   ],
   "id": "8418eb4f49f3759b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- epoch 1 ---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2]) torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [03:04<27:39, 184.37s/batch, FC_loss=1.37, G_loss=2.31, D_loss=2.44]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[185], line 29\u001B[0m\n\u001B[0;32m     27\u001B[0m optimizer_D\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     28\u001B[0m loss_D \u001B[38;5;241m=\u001B[39m discriminatorLoss(d_con_logits, d_unc_logits)\n\u001B[1;32m---> 29\u001B[0m \u001B[43mloss_D\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m optimizer_D\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     31\u001B[0m train_loss_D \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss_D\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    520\u001B[0m     )\n\u001B[1;32m--> 521\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 289\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    293\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    294\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    295\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    296\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    297\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    767\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    768\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 769\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    770\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    771\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    772\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    773\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 185
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T03:37:00.969770400Z",
     "start_time": "2024-10-14T03:31:39.296205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = torch.rand(32,2)\n",
    "target = torch.zeros(32).to(torch.long)\n",
    "print(a.shape, target.shape)\n",
    "CrossEntropyLoss()(a, target)"
   ],
   "id": "f4c2670afa9e936a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2]) torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6036)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T00:46:37.838929Z",
     "start_time": "2024-10-03T00:46:37.439345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses_FC, label='train')\n",
    "plt.plot(train_losses_G, label='train')\n",
    "plt.plot(train_losses_D, label='train')\n",
    "plt.plot(test_losses_FC, label='test')\n",
    "plt.plot(test_losses_G, label='test')\n",
    "plt.plot(test_losses_D, label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# save plot\n",
    "plt.savefig('./Model/' + save_name + \"/\" + save_name + '_loss.png')"
   ],
   "id": "65770e7128728d32",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T00:46:43.241584Z",
     "start_time": "2024-10-03T00:46:43.228904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# to csv\n",
    "train_losses = pd.DataFrame({'train_FC': train_losses_FC, 'train_G': train_losses_G, 'train_D': train_losses_D})\n",
    "test_losses = pd.DataFrame({'test_FC': test_losses_FC, 'test_G': test_losses_G, 'test_D': test_losses_D})\n",
    "train_losses.to_csv('./Model/' + save_name + \"/\" + save_name + '_train_losses.csv', index=False)\n",
    "test_losses.to_csv('./Model/' + save_name + \"/\" + save_name + '_test_losses.csv', index=False)"
   ],
   "id": "5cdd08c9e69263de",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate Test",
   "id": "4c44d1e70ced399f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T22:01:28.366816Z",
     "start_time": "2024-10-02T22:01:28.363077Z"
    }
   },
   "cell_type": "code",
   "source": "test_image[0].shape",
   "id": "b4e0d61dc470c2e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:43:27.969750Z",
     "start_time": "2024-10-02T23:43:21.089352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#load model\n",
    "NetG = Generator().to(device)\n",
    "NetD = Discriminator().to(device)\n",
    "NetG.load_state_dict(torch.load('./Model/test_batch32/test_batch32_5NetG.pth'))\n",
    "NetD.load_state_dict(torch.load('./Model/test_batch32/test_batch32_5NetD.pth'))\n",
    "# train with load model\n",
    "NetG.train()\n",
    "NetD.train()\n"
   ],
   "id": "c3f72adc27a5f410",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15460\\3508553010.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  NetG.load_state_dict(torch.load('./Model/test_batch32/test_batch32_5NetG.pth'))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15460\\3508553010.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  NetD.load_state_dict(torch.load('./Model/test_batch32/test_batch32_5NetD.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:47:40.015314Z",
     "start_time": "2024-10-02T23:43:27.978751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generate\n",
    "NetG.eval()\n",
    "NetD.eval()\n",
    "image = test_image[0].unsqueeze(0).to(device)\n",
    "output = NetG.generate(image, 100)\n",
    "output"
   ],
   "id": "aa601343f448bd19",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 991.56it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 998.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.2329,  0.4325,  0.4199,  ..., -0.4702, -0.1142, -0.0098]],\n",
       " \n",
       "         [[ 0.1089,  0.3625,  0.8224,  ...,  0.2763,  0.0975, -0.0043]],\n",
       " \n",
       "         [[-0.1282, -0.1454, -0.1592,  ...,  0.1126,  0.7090,  0.6611]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.4177,  0.1868,  0.0531,  ..., -0.5458,  0.0338, -1.4983]],\n",
       " \n",
       "         [[-0.1912,  0.1032,  0.4763,  ...,  0.7547,  0.7066, -0.5460]],\n",
       " \n",
       "         [[-0.1003, -0.3331, -0.0245,  ..., -0.5132,  0.0633,  0.8948]]],\n",
       "        device='cuda:0'),\n",
       " [2,\n",
       "  540,\n",
       "  235248,\n",
       "  236193,\n",
       "  235248,\n",
       "  235248,\n",
       "  235250,\n",
       "  235274,\n",
       "  35351,\n",
       "  235254,\n",
       "  605,\n",
       "  6935,\n",
       "  235276,\n",
       "  235248,\n",
       "  235248,\n",
       "  235248,\n",
       "  132385,\n",
       "  235248,\n",
       "  235265,\n",
       "  235248,\n",
       "  2173,\n",
       "  235274,\n",
       "  235248,\n",
       "  2465,\n",
       "  3682,\n",
       "  236193,\n",
       "  18824,\n",
       "  235274,\n",
       "  235248,\n",
       "  11200,\n",
       "  235276,\n",
       "  235276,\n",
       "  616,\n",
       "  235248,\n",
       "  235248,\n",
       "  235248,\n",
       "  235274,\n",
       "  235248,\n",
       "  235248,\n",
       "  2012,\n",
       "  235276,\n",
       "  2012,\n",
       "  236193,\n",
       "  235248,\n",
       "  235248,\n",
       "  235248,\n",
       "  235265,\n",
       "  618,\n",
       "  235276,\n",
       "  669,\n",
       "  235248,\n",
       "  235248,\n",
       "  24255,\n",
       "  618,\n",
       "  14383,\n",
       "  236193,\n",
       "  235248,\n",
       "  235362,\n",
       "  236193,\n",
       "  5862,\n",
       "  1420,\n",
       "  236193,\n",
       "  235248,\n",
       "  235248,\n",
       "  46816,\n",
       "  235248,\n",
       "  235248,\n",
       "  1820,\n",
       "  235248,\n",
       "  235276,\n",
       "  235265,\n",
       "  690,\n",
       "  235256,\n",
       "  235274,\n",
       "  236193,\n",
       "  235256,\n",
       "  235248,\n",
       "  235248,\n",
       "  235248,\n",
       "  236193,\n",
       "  819,\n",
       "  235248,\n",
       "  235248,\n",
       "  235254,\n",
       "  235265,\n",
       "  84521,\n",
       "  3550,\n",
       "  235276,\n",
       "  42113,\n",
       "  235265,\n",
       "  235345,\n",
       "  509,\n",
       "  235248,\n",
       "  235276,\n",
       "  4943,\n",
       "  235248,\n",
       "  235248,\n",
       "  236193,\n",
       "  53152,\n",
       "  235269,\n",
       "  235265],\n",
       " tensor([-0.4983], device='cuda:0'))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. Adapter",
   "id": "2c0340b7c2e49bfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NetG = Generator().to(device)\n",
    "NetD = Discriminator().to(device)\n",
    "optimizer_G = optim.Adam(NetG.parameters(), lr=0.001)\n",
    "optimizer_D = optim.Adam(NetD.parameters(), lr=0.001)\n",
    "train_losses_FC = []\n",
    "train_losses_G = []\n",
    "train_losses_D = []\n",
    "test_losses_FC = []\n",
    "test_losses_G = []\n",
    "test_losses_D = []\n",
    "save = []\n",
    "present_epoch = 1\n",
    "best_train_loss_FC = 9999\n",
    "best_train_loss_G = 9999\n",
    "best_train_loss_D = 9999\n",
    "best_test_loss_FC = 9999\n",
    "best_test_loss_G = 9999\n",
    "best_test_loss_D = 9999\n",
    "loss_data = pd.DataFrame()\n",
    "\n",
    "checkpoint = False\n",
    "if checkpoint:\n",
    "    checkpoint_G = torch.load('./Model/test_save/test_save_2NetG.pth')\n",
    "    checkpoint_D = torch.load('./Model/test_save/test_save_2NetD.pth')\n",
    "    NetG.load_state_dict(checkpoint_G['model_state_dict'])\n",
    "    NetD.load_state_dict(checkpoint_D['model_state_dict'])\n",
    "    optimizer_G.load_state_dict(checkpoint_G['optimizer_state_dict'])\n",
    "    optimizer_D.load_state_dict(checkpoint_D['optimizer_state_dict'])\n",
    "    train_losses_FC.append(checkpoint_G['FC_loss'])\n",
    "    train_losses_G.append(checkpoint_G['G_loss'])\n",
    "    train_losses_D.append(checkpoint_G['D_loss'])\n",
    "    present_epoch = checkpoint_G['epoch'] + 1\n",
    "\n",
    "    \n",
    "\n",
    "funnyScoreLoss = nn.MSELoss()\n",
    "\n",
    "def generatorLoss(generator_logits):\n",
    "    result_fake = (torch.zeros(generator_logits[1].shape[0])).to(device)\n",
    "    unc_loss = BCEWithLogitsLoss()(generator_logits[1], result_fake)\n",
    "    con_loss = BCEWithLogitsLoss()(generator_logits[0], result_fake)\n",
    "    loss = con_loss + unc_loss\n",
    "    return loss\n",
    "\n",
    "def discriminatorLoss(uncondition_logits, condition_logits):\n",
    "    result_true = (torch.ones(condition_logits[0].shape[0])).to(device)\n",
    "    result_fake = (torch.zeros(condition_logits[0].shape[0])).to(device)\n",
    "    unc_r = BCEWithLogitsLoss()(condition_logits[0], result_true)\n",
    "    unc_f = BCEWithLogitsLoss()(condition_logits[1], result_fake)\n",
    "    unc_m = BCEWithLogitsLoss()(condition_logits[2], result_fake)\n",
    "    con_r = BCEWithLogitsLoss()(uncondition_logits[0], result_true)\n",
    "    con_f = BCEWithLogitsLoss()(uncondition_logits[1], result_fake)\n",
    "    con_m = BCEWithLogitsLoss()(uncondition_logits[2], result_fake)\n",
    "    loss = unc_r + ((unc_f + unc_m)/2) + con_r + ((con_f + con_m)/2)\n",
    "    return loss"
   ],
   "id": "8dd879c4c76846d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "save_name = '20241011'\n",
    "if not os.path.exists('./Model/'+save_name):\n",
    "    os.makedirs('./Model/'+save_name)\n",
    "    \n",
    "epochs = 10\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(epochs):\n",
    "    print(\"---------------------------------------- epoch \"+ str(epoch + present_epoch) +\" ---------------------------------------\")\n",
    "    train_loss_FC = 0\n",
    "    train_loss_G = 0\n",
    "    train_loss_D = 0\n",
    "    test_loss_FC = 0\n",
    "    test_loss_G = 0\n",
    "    test_loss_D = 0\n",
    "    \n",
    "    ###################################### Train ######################################\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for text_b, image_b, funny_score_b in tepoch:\n",
    "            print(text_b, image_b, funny_score_b)\n",
    "            text, image, funny_score = OxfordDataset.tokenize_batch(text_b, image_b, funny_score_b)\n",
    "            print(text.shape, image.shape, funny_score.shape)\n",
    "            ######################################################\n",
    "                # (1) Generate fake caption\n",
    "            ######################################################\n",
    "            logits, output_funny_score = NetG(text.to(device).to(torch.float32), image.to(device).to(torch.float32))\n",
    "            gen_logits, con_logits, unc_logits = NetD(text.to(device).to(torch.float32), logits.detach().to(torch.float32), image.to(device).to(torch.float32))\n",
    "            ######################################################\n",
    "                # (3) Update Discriminator network\n",
    "            #####################################################\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D = discriminatorLoss(unc_logits, con_logits)\n",
    "            loss_D.backward(retain_graph=True)\n",
    "            optimizer_D.step()\n",
    "            train_loss_D += loss_D.item()\n",
    "            ######################################################\n",
    "                # (4) Update Generator network\n",
    "            ######################################################\n",
    "            optimizer_G.zero_grad()\n",
    "            loss_FC = funnyScoreLoss(output_funny_score, funny_score.to(device).to(torch.float32))\n",
    "            loss_FC.backward(retain_graph=True)\n",
    "            train_loss_FC += loss_FC.item()\n",
    "            loss_G = generatorLoss(gen_logits)\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            train_loss_G += loss_G.item()\n",
    "            ######################################################\n",
    "            tepoch.set_postfix({'FC_loss': loss_FC.item(), 'G_loss': loss_G.item(), 'D_loss': loss_D.item()})\n",
    "            ######################################################\n",
    "    train_loss_FC /= len(train_loader)\n",
    "    train_loss_G /= len(train_loader)\n",
    "    train_loss_D /= len(train_loader)\n",
    "    train_losses_FC.append(train_loss_FC)\n",
    "    train_losses_G.append(train_loss_G)\n",
    "    train_losses_D.append(train_loss_D)\n",
    "    ###################################### Train ######################################\n",
    "    \n",
    "    \n",
    "    ######################################  Test ######################################\n",
    "    with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
    "        for text, image, funny_score in tepoch:\n",
    "            # Generator\n",
    "            logits, output_funny_score = NetG(text.to(device).to(torch.float32), image.to(device).to(torch.float32))\n",
    "            # Discriminator\n",
    "            gen_logits, con_logits, unc_logits = NetD(text.to(device).to(torch.float32), logits.detach().to(torch.float32), image.to(device).to(torch.float32))\n",
    "            # loss\n",
    "            loss_FC = funnyScoreLoss(output_funny_score, funny_score.to(device).to(torch.float32))\n",
    "            loss_G = generatorLoss(gen_logits)\n",
    "            loss_D = discriminatorLoss(unc_logits, con_logits)\n",
    "            test_loss_FC += loss_FC.item()\n",
    "            test_loss_G += loss_G.item()\n",
    "            test_loss_D += loss_D.item()\n",
    "            tepoch.set_postfix({'FC_loss': test_loss_FC, 'G_loss': test_loss_G, 'D_loss': test_loss_D})\n",
    "    test_loss_FC /= len(test_loader)\n",
    "    test_loss_G /= len(test_loader)\n",
    "    test_loss_D /= len(test_loader)\n",
    "    test_losses_FC.append(test_loss_FC)\n",
    "    test_losses_G.append(test_loss_G)\n",
    "    test_losses_D.append(test_loss_D)\n",
    "    ######################################  Test ######################################\n",
    "    \n",
    "    ######################################  Save ######################################\n",
    "    hasSaved = False\n",
    "    # 任一個loss小於最佳loss就存檔\n",
    "    if train_loss_FC < best_train_loss_FC and test_loss_FC < best_test_loss_FC:\n",
    "        best_train_loss_FC = train_loss_FC\n",
    "        best_test_loss_FC = test_loss_FC\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    if train_loss_G < best_train_loss_G and test_loss_G < best_test_loss_G:\n",
    "        best_train_loss_G = train_loss_G\n",
    "        best_test_loss_G = test_loss_G\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    if train_loss_D < best_train_loss_D and test_loss_D < best_test_loss_D:\n",
    "        best_train_loss_D = train_loss_D\n",
    "        best_test_loss_D = test_loss_D\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    \n",
    "    if hasSaved:\n",
    "        save.append(\"V\")\n",
    "    else:\n",
    "        save.append(\" \")\n",
    "\n",
    "    loss_data['train_FC'] = train_losses_FC\n",
    "    loss_data['train_G'] = train_losses_G\n",
    "    loss_data['train_D'] = train_losses_D\n",
    "    loss_data['test_FC'] = test_losses_FC\n",
    "    loss_data['test_G'] = test_losses_G\n",
    "    loss_data['test_D'] = test_losses_D\n",
    "    loss_data['save'] = save\n",
    "    loss_data.to_csv('./Model/' + save_name + \"/\" + save_name + '_loss.csv', index=False)\n",
    "    ######################################  Save ######################################"
   ],
   "id": "d105d60cc85cc3c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4c7c985efb0f9a32"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T01:09:18.962671Z",
     "start_time": "2024-10-15T01:09:16.930380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoImageProcessor, Swinv2Model\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoConfig\n",
    "\n",
    "def addImagePath(data, imgPath):\n",
    "    data['image_id'] = data['image_id'].apply(lambda x: imgPath + str(x) + '.jpg')\n",
    "    return data\n",
    "\n",
    "def textExtraction(text_data):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "    gemmaConfig = AutoConfig.from_pretrained('google/gemma-2-2b-it')\n",
    "    vocab_size = gemmaConfig.vocab_size  # 詞彙表大小\n",
    "\n",
    "    embedding_dim = 768  # 嵌入维度，與你的圖片嵌入维度相同\n",
    "    text_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    all_features = []\n",
    "    for text in (text_data):\n",
    "        tokens = tokenizer(text, padding='longest', return_tensors='pt')\n",
    "        output = text_embedding(tokens['input_ids'])\n",
    "        linear = torch.nn.Linear(output.shape[1], 64)\n",
    "        projected_output = linear(output.transpose(1, 2)).transpose(1, 2)\n",
    "        all_features.append(projected_output)\n",
    "    return torch.cat(all_features)\n",
    "\n",
    "def textExtractReverse(data):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "\n",
    "    # 有時後空格會失效，所以手動插入空格 <pad> = 0\n",
    "    def insert_zeros(tensor):\n",
    "        zeros = torch.zeros(tensor.shape[0], tensor.shape[1] * 2 - 1)\n",
    "        zeros[:, ::2] = tensor\n",
    "        zeros = zeros.to(int)\n",
    "        return zeros\n",
    "\n",
    "    reverse_data = insert_zeros(data.squeeze(-1))\n",
    "    # reverse the token\n",
    "    reverse = tokenizer.batch_decode(reverse_data, skip_special_tokens=False)\n",
    "    # tokenize with gemma-2b\n",
    "    tokenizer_gemma = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "    prompt = \"Write a humor memetic post for Instagram with the following elements: \"\n",
    "    # all_features = []\n",
    "    for i, text in enumerate(reverse):\n",
    "        text = text.replace(\"<pad>\", \" \").replace(\"  \", \" \")\n",
    "        text = text.split()\n",
    "        text = ', '.join(text)\n",
    "        reverse[i] = prompt + text + \".\"\n",
    "    tokens = tokenizer_gemma(reverse, padding='max_length', max_length=64, return_tensors='pt')\n",
    "    # all_features.append(tokens['input_ids'])\n",
    "    return tokens['input_ids']\n",
    "\n",
    "# 定義批量處理和提取特徵的函數\n",
    "def imageExtraction(image_data):\n",
    "    # 加載 Swinv2 模型和處理器\n",
    "    image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "    swin = Swinv2Model.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "\n",
    "    # 將模型設置為評估模式\n",
    "    swin.eval()\n",
    "\n",
    "    # 如果有 GPU，可以將模型移動到 GPU 上\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    swin.to(device)\n",
    "\n",
    "    all_features = []\n",
    "    for image_path in (image_data):\n",
    "        with torch.no_grad():\n",
    "            # 加載並預處理圖像\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = np.array(image)\n",
    "            image = image[:, :, :3]\n",
    "            # 使用 image_processor 將 batch 圖片處理成適合模型的格式\n",
    "            inputs = image_processor(image, return_tensors=\"pt\")\n",
    "            # 將 inputs 放到 GPU 上（如果可用）\n",
    "            inputs.to(device)\n",
    "            # 獲取 Swinv2 模型的輸出\n",
    "            outputs = swin(**inputs)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "            # 儲存特徵\n",
    "            last_hidden_states = last_hidden_states.cpu()\n",
    "            all_features.append(last_hidden_states)\n",
    "    return torch.cat(all_features)\n",
    "\n"
   ],
   "id": "52943676af162956",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T01:31:46.494891Z",
     "start_time": "2024-10-15T01:09:18.962671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "dirPath = '../Data/Oxford_HIC/Filtered_oxford_hic_data.csv'\n",
    "data = pd.read_csv(dirPath)\n",
    "data = addImagePath(data, '../Data/Oxford_HIC/oxford_img/')\n",
    "with tqdm.tqdm(total=data.shape[0]) as pbar:\n",
    "    for i in range(data.shape[0]):\n",
    "        image_path = data['image_id'][i]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except:\n",
    "            print(image_path)\n",
    "        pbar.update(1)"
   ],
   "id": "2cd70712875151d7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8975/3398160 [00:03<22:26, 2517.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9829/3398160 [00:03<21:28, 2628.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12347/3398160 [00:04<20:46, 2715.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 24906/3398160 [00:09<21:39, 2596.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_2495.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_2495.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_2495.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 31555/3398160 [00:12<20:52, 2687.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_3049.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 78074/3398160 [00:29<21:03, 2627.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_6997.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_6997.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_6997.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 85056/3398160 [00:32<20:06, 2746.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 98382/3398160 [00:37<22:22, 2458.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_8176.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_8176.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_8176.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_8176.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_8176.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_8176.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_8176.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_8176.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 147720/3398160 [00:59<26:00, 2083.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_12501.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 302766/3398160 [02:27<36:12, 1424.97it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 303413/3398160 [02:28<33:43, 1529.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 306121/3398160 [02:29<34:53, 1477.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 871760/3398160 [06:27<17:09, 2454.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_67874.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_67874.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_67874.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_67874.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_67874.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_67874.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 879298/3398160 [06:31<16:58, 2472.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_68584.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_68584.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_68584.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_68584.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_68584.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_68584.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 894582/3398160 [06:37<17:12, 2424.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 912074/3398160 [06:44<17:42, 2339.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_70929.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 952215/3398160 [07:01<16:54, 2409.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_70929.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 968082/3398160 [07:08<18:30, 2189.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_12501.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_12501.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_12501.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_12501.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3398160/3398160 [22:24<00:00, 2526.94it/s]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load Data",
   "id": "821d6044bf065fc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T07:35:51.373749Z",
     "start_time": "2024-10-17T07:35:48.940849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoImageProcessor, Swinv2Model\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoConfig\n",
    "from extractor import textExtraction, textExtractReverse, imageExtraction, addImagePath\n",
    "import tqdm\n",
    "import gc"
   ],
   "id": "85cc88a5b37af13a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T07:35:54.413619Z",
     "start_time": "2024-10-17T07:35:52.047235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dirPath = '../Data/Oxford_HIC/Filtered_oxford_hic_data.csv'\n",
    "imgPath = '../Data/Oxford_HIC/oxford_img/'\n",
    "\n",
    "data = pd.read_csv(dirPath)"
   ],
   "id": "91dc51ac29931383",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T07:35:55.375338Z",
     "start_time": "2024-10-17T07:35:55.006751Z"
    }
   },
   "cell_type": "code",
   "source": "data['image_path'] = data['image_id'].apply(lambda x: imgPath + str(x) + '.jpg')",
   "id": "eaaf1a4aa5644cfe",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T07:35:55.408941Z",
     "start_time": "2024-10-17T07:35:55.403213Z"
    }
   },
   "cell_type": "code",
   "source": "data.head()",
   "id": "1ef9b213d4e1e40b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   image_id                                            caption  funny_score  \\\n",
       "0  bokete_0                          My driver's license photo          0.0   \n",
       "1  bokete_1                                    Refugee relief.          0.0   \n",
       "2  bokete_2  Now! I think I stepped on a cat! What? Really?...          0.0   \n",
       "3  bokete_3      You wouldn't know I was reading a comic book.          0.0   \n",
       "4  bokete_4                  Oh no! I forgot my ・・・・ clothes!\"          0.0   \n",
       "\n",
       "                                   image_path  \n",
       "0  ../Data/Oxford_HIC/oxford_img/bokete_0.jpg  \n",
       "1  ../Data/Oxford_HIC/oxford_img/bokete_1.jpg  \n",
       "2  ../Data/Oxford_HIC/oxford_img/bokete_2.jpg  \n",
       "3  ../Data/Oxford_HIC/oxford_img/bokete_3.jpg  \n",
       "4  ../Data/Oxford_HIC/oxford_img/bokete_4.jpg  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>caption</th>\n",
       "      <th>funny_score</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bokete_0</td>\n",
       "      <td>My driver's license photo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>../Data/Oxford_HIC/oxford_img/bokete_0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bokete_1</td>\n",
       "      <td>Refugee relief.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>../Data/Oxford_HIC/oxford_img/bokete_1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bokete_2</td>\n",
       "      <td>Now! I think I stepped on a cat! What? Really?...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>../Data/Oxford_HIC/oxford_img/bokete_2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bokete_3</td>\n",
       "      <td>You wouldn't know I was reading a comic book.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>../Data/Oxford_HIC/oxford_img/bokete_3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bokete_4</td>\n",
       "      <td>Oh no! I forgot my ・・・・ clothes!\"</td>\n",
       "      <td>0.0</td>\n",
       "      <td>../Data/Oxford_HIC/oxford_img/bokete_4.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T07:35:56.602661Z",
     "start_time": "2024-10-17T07:35:56.598604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "columns = ['image_id', 'caption', 'funny_score']\n",
    "new_data = pd.DataFrame(columns=columns)\n",
    "# new_data['image_id'] = data['image_id'].astype(object)\n",
    "# new_data['caption'] = new_data['caption'].astype(object)\n",
    "# new_data['funny_score'] = data['funny_score'].astype(float)"
   ],
   "id": "2f43a0d5e0ca746d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-17T07:35:57.738710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 50000 一個round直到跑完\n",
    "start_index = 0\n",
    "end_index = 50000\n",
    "filename = 0\n",
    "while start_index < data.shape[0]+1:\n",
    "    if start_index == 0 or start_index % 34*50000:\n",
    "        columns = ['image_id', 'caption', 'funny_score']\n",
    "        new_data = pd.DataFrame(columns=columns)\n",
    "        filename += 1\n",
    "    else:\n",
    "        new_data = pd.read_csv('../Data/Oxford_HIC/Caption'+filename+'_oxford_hic_data.csv')\n",
    "\n",
    "    a = textExtraction(data['caption'][start_index:end_index].tolist())\n",
    "    for i in range(len(a)):\n",
    "        new_data = new_data._append({'image_id': data['image_id'][i], 'caption': a[i].tolist(), 'funny_score': \n",
    "            data['funny_score'][i]}, ignore_index=True)\n",
    "    new_data.to_csv('../Data/Oxford_HIC/Caption'+filename+'_oxford_hic_data.csv', index=False)\n",
    "    print(start_index, end_index)\n",
    "    # clear memory\n",
    "    del a\n",
    "    del new_data\n",
    "    gc.collect()\n",
    "    start_index = end_index\n",
    "    if end_index == data.shape[0]:\n",
    "        break\n",
    "    if end_index + 50000 > data.shape[0]:\n",
    "        end_index = data.shape[0]\n",
    "    else:\n",
    "        end_index += 50000\n",
    "        "
   ],
   "id": "8f623a0d8be2ce67",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:07<00:00, 6276.13it/s]\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15600\\3889116747.py:15: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  new_data = new_data._append({'image_id': data['image_id'][i], 'caption': a[i].tolist(), 'funny_score':\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:11:14.687009Z",
     "start_time": "2024-10-17T06:10:52.714116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data1 = pd.read_csv('../Data/Oxford_HIC/Caption_oxford_hic_data.csv')\n",
    "data2 = pd.read_csv('../Data/Oxford_HIC/Caption2_oxford_hic_data.csv')\n",
    "data = pd.concat([data1, data2], ignore_index=True)\n",
    "data.to_csv('../Data/Oxford_HIC/Caption_Done_oxford_hic_data.csv', index=False)"
   ],
   "id": "942ee44c7ff6a1e2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:12:08.448376Z",
     "start_time": "2024-10-17T06:12:08.441936Z"
    }
   },
   "cell_type": "code",
   "source": "data.shape",
   "id": "4532981ecccc049",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3398081, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:37:41.545588Z",
     "start_time": "2024-10-17T06:37:41.534704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定義批量處理和提取特徵的函數\n",
    "def imageExtractiona(image_data):\n",
    "    # 加載 Swinv2 模型和處理器\n",
    "    image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "    swin = Swinv2Model.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "\n",
    "    # 將模型設置為評估模式\n",
    "    swin.eval()\n",
    "\n",
    "    # 如果有 GPU，可以將模型移動到 GPU 上\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    swin.to(device)\n",
    "\n",
    "    # all_features = []\n",
    "    # for image_path in (image_data):\n",
    "    with torch.no_grad():\n",
    "        # 加載並預處理圖像\n",
    "        image = Image.open(image_data).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        image = image[:, :, :3]\n",
    "        # 使用 image_processor 將 batch 圖片處理成適合模型的格式\n",
    "        inputs = image_processor(image, return_tensors=\"pt\")\n",
    "        # 將 inputs 放到 GPU 上（如果可用）\n",
    "        inputs.to(device)\n",
    "        # 獲取 Swinv2 模型的輸出\n",
    "        outputs = swin(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        # 儲存特徵\n",
    "        last_hidden_states = last_hidden_states.cpu()\n",
    "        # all_features.append(last_hidden_states)\n",
    "    return last_hidden_states.squeeze(0).detach().numpy()"
   ],
   "id": "18e43980acc44bd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:14:27.324807Z",
     "start_time": "2024-10-17T06:14:27.235999Z"
    }
   },
   "cell_type": "code",
   "source": "img_names = data['image_id'].unique()",
   "id": "80c2c91176fb7b2d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:17:43.570774Z",
     "start_time": "2024-10-17T06:17:35.898318Z"
    }
   },
   "cell_type": "code",
   "source": [
    "done = -1\n",
    "counter = 0\n",
    "imgPath = '../Data/Oxford_HIC/oxford_img/'\n",
    "with tqdm.tqdm(total=3398081) as pbar:\n",
    "    for i in range(len(img_names)):\n",
    "        if i <= done:\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        new_data = pd.read_csv('../Data/Oxford_HIC/Image_oxford_hic_data.csv')\n",
    "        image_path = imgPath + img_names[i] + '.jpg'\n",
    "        image = imageExtractiona(image_path)\n",
    "        updateCount = new_data['image_id'].value_counts().get(img_names[i])\n",
    "        new_data = new_data.replace({'image_id': img_names[i]}, value=image, regex=True)\n",
    "        del image\n",
    "        gc.collect()\n",
    "        pbar.update(updateCount)\n",
    "        counter += updateCount \n",
    "        print(\"present: \"+done+\", counter: \"+counter)\n",
    "        if counter / 50000 > 0:\n",
    "            new_data.to_csv('../Data/Oxford_HIC/Image_oxford_hic_data.csv', index=False)\n",
    "            done = i\n",
    "            counter = 0\n",
    "            print(\"save: \"+done)\n",
    "            del new_data\n",
    "            gc.collect()\n",
    "new_data.to_csv('../Data/Oxford_HIC/Image_oxford_hic_data.csv', index=False)"
   ],
   "id": "e658aed7b67f6ef9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3398081 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "value argument must be scalar, dict, or Series",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 13\u001B[0m\n\u001B[0;32m     11\u001B[0m image \u001B[38;5;241m=\u001B[39m imageExtractiona(image_path)\n\u001B[0;32m     12\u001B[0m updateCount \u001B[38;5;241m=\u001B[39m new_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalue_counts()\u001B[38;5;241m.\u001B[39mget(img_names[i])\n\u001B[1;32m---> 13\u001B[0m new_data \u001B[38;5;241m=\u001B[39m \u001B[43mnew_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplace\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mimg_names\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mregex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m image\n\u001B[0;32m     15\u001B[0m gc\u001B[38;5;241m.\u001B[39mcollect()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:8085\u001B[0m, in \u001B[0;36mNDFrame.replace\u001B[1;34m(self, to_replace, value, inplace, limit, regex, method)\u001B[0m\n\u001B[0;32m   8083\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_replace_columnwise(mapping, inplace, regex)\n\u001B[0;32m   8084\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 8085\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue argument must be scalar, dict, or Series\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   8087\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_list_like(to_replace):\n\u001B[0;32m   8088\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_list_like(value):\n\u001B[0;32m   8089\u001B[0m         \u001B[38;5;66;03m# e.g. to_replace = [NA, ''] and value is 0,\u001B[39;00m\n\u001B[0;32m   8090\u001B[0m         \u001B[38;5;66;03m#  so we replace NA with 0 and then replace '' with 0\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: value argument must be scalar, dict, or Series"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T07:29:26.460678Z",
     "start_time": "2024-10-17T07:29:26.368802Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "malformed node or string on line 1: <ast.Call object at 0x000001F7A03A1DE0>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[78], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m new_data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../Data/Oxford_HIC/Caption2_oxford_hic_data.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m new_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaption\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mnew_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcaption\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mast\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mliteral_eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[0;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4800\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m   4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m-> 4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[0;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[1;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[0;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[0;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[0;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[0;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[1;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[0;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[1;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[1;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[0;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[0;32m   1747\u001B[0m     )\n",
      "File \u001B[1;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Cell \u001B[1;32mIn[78], line 2\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m      1\u001B[0m new_data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../Data/Oxford_HIC/Caption2_oxford_hic_data.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m new_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaption\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m new_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaption\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[43mast\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mliteral_eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:110\u001B[0m, in \u001B[0;36mliteral_eval\u001B[1;34m(node_or_string)\u001B[0m\n\u001B[0;32m    108\u001B[0m                 \u001B[38;5;28;01mreturn\u001B[39;00m left \u001B[38;5;241m-\u001B[39m right\n\u001B[0;32m    109\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _convert_signed_num(node)\n\u001B[1;32m--> 110\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_convert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnode_or_string\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:109\u001B[0m, in \u001B[0;36mliteral_eval.<locals>._convert\u001B[1;34m(node)\u001B[0m\n\u001B[0;32m    107\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    108\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m left \u001B[38;5;241m-\u001B[39m right\n\u001B[1;32m--> 109\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_convert_signed_num\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnode\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:83\u001B[0m, in \u001B[0;36mliteral_eval.<locals>._convert_signed_num\u001B[1;34m(node)\u001B[0m\n\u001B[0;32m     81\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     82\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m-\u001B[39m operand\n\u001B[1;32m---> 83\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_convert_num\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnode\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:74\u001B[0m, in \u001B[0;36mliteral_eval.<locals>._convert_num\u001B[1;34m(node)\u001B[0m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_convert_num\u001B[39m(node):\n\u001B[0;32m     73\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(node, Constant) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(node\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mcomplex\u001B[39m):\n\u001B[1;32m---> 74\u001B[0m         \u001B[43m_raise_malformed_node\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m node\u001B[38;5;241m.\u001B[39mvalue\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:71\u001B[0m, in \u001B[0;36mliteral_eval.<locals>._raise_malformed_node\u001B[1;34m(node)\u001B[0m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m lno \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(node, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlineno\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     70\u001B[0m     msg \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m on line \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlno\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m---> 71\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg \u001B[38;5;241m+\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnode\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: malformed node or string on line 1: <ast.Call object at 0x000001F7A03A1DE0>"
     ]
    }
   ],
   "execution_count": 78,
   "source": [
    "new_data = pd.read_csv('../Data/Oxford_HIC/Caption2_oxford_hic_data.csv')\n",
    "new_data['caption'] = new_data['caption'].apply(lambda x: torch.tensor(ast.literal_eval(x)))"
   ],
   "id": "5b64bfd1bf5d2305"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T07:29:01.830845Z",
     "start_time": "2024-10-17T07:29:01.752233Z"
    }
   },
   "cell_type": "code",
   "source": "new_data.to_csv('../Data/Oxford_HIC/Caption2_oxford_hic_data.csv', index=False)",
   "id": "ec29f6e714ae363e",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# convert numpy array to tensor\n",
    "def from_np_array(array):\n",
    "    return torch.from_numpy(array)"
   ],
   "id": "a3a71f22ce3a1f9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T07:16:40.643152Z",
     "start_time": "2024-10-17T07:16:40.627275Z"
    }
   },
   "cell_type": "code",
   "source": "np.vstack(new_data['caption'][0][:768]).astype(np.float32)",
   "id": "e77ff1a15fbbae2",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '['",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[57], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcaption\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m768\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mValueError\u001B[0m: could not convert string to float: '['"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:51:12.711125Z",
     "start_time": "2024-10-17T06:51:12.380050Z"
    }
   },
   "cell_type": "code",
   "source": "torch.tensor(new_data['caption'].values)",
   "id": "ab3af6b279cdfa9a",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mto_np_array\u001B[39m(array):\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray2string(array, separator\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 9\u001B[0m new_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaption\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mnew_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcaption\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mto_np_array\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[0;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4800\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m   4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m-> 4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[0;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[1;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[0;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[0;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[0;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[0;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[1;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[0;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[1;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[1;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[0;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[0;32m   1747\u001B[0m     )\n",
      "File \u001B[1;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Cell \u001B[1;32mIn[34], line 7\u001B[0m, in \u001B[0;36mto_np_array\u001B[1;34m(array)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mto_np_array\u001B[39m(array):\n\u001B[1;32m----> 7\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray2string\u001B[49m\u001B[43m(\u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseparator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m, \u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\numpy\\core\\arrayprint.py:733\u001B[0m, in \u001B[0;36marray2string\u001B[1;34m(a, max_line_width, precision, suppress_small, separator, prefix, style, formatter, threshold, edgeitems, sign, floatmode, suffix, legacy)\u001B[0m\n\u001B[0;32m    730\u001B[0m     options[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlinewidth\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(suffix)\n\u001B[0;32m    732\u001B[0m \u001B[38;5;66;03m# treat as a null array if any of shape elements == 0\u001B[39;00m\n\u001B[1;32m--> 733\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43ma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[]\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    736\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _array2string(a, options, separator, prefix)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'str' object has no attribute 'size'"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T07:05:07.965425Z",
     "start_time": "2024-10-17T07:05:07.954677Z"
    }
   },
   "cell_type": "code",
   "source": "new_data['caption'] = new_data['caption'].apply(lambda x: np.asarray(x))",
   "id": "b4dd95bf1e32d202",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T07:08:07.164848Z",
     "start_time": "2024-10-17T07:08:07.152765Z"
    }
   },
   "cell_type": "code",
   "source": "type(new_data['caption'].to_numeric().to_numpy()[0])",
   "id": "af7c9dbc71742cfe",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'to_numeric'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[54], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28mtype\u001B[39m(\u001B[43mnew_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcaption\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_numeric\u001B[49m()\u001B[38;5;241m.\u001B[39mto_numpy()[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:6299\u001B[0m, in \u001B[0;36mNDFrame.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   6292\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   6293\u001B[0m     name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_internal_names_set\n\u001B[0;32m   6294\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_metadata\n\u001B[0;32m   6295\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_accessors\n\u001B[0;32m   6296\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info_axis\u001B[38;5;241m.\u001B[39m_can_hold_identifiers_and_holds_name(name)\n\u001B[0;32m   6297\u001B[0m ):\n\u001B[0;32m   6298\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m[name]\n\u001B[1;32m-> 6299\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mobject\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getattribute__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Series' object has no attribute 'to_numeric'"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T07:05:13.875106Z",
     "start_time": "2024-10-17T07:05:13.862775Z"
    }
   },
   "cell_type": "code",
   "source": "a = torch.tensor(new_data['caption'].values)",
   "id": "7e1b381aa04f0368",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[48], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m a \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcaption\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:44:32.204011Z",
     "start_time": "2024-10-17T06:44:32.197962Z"
    }
   },
   "cell_type": "code",
   "source": "from_np_array(a)",
   "id": "ff7acd85728359e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1,,0.2],,[0.3,,0.4]]\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001B[1;36m(most recent call last)\u001B[0m:\n",
      "\u001B[0m  File \u001B[0;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001B[0m in \u001B[0;35mrun_code\u001B[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001B[0m\n",
      "\u001B[0m  Cell \u001B[0;32mIn[20], line 1\u001B[0m\n    from_np_array(a)\u001B[0m\n",
      "\u001B[0m  Cell \u001B[0;32mIn[16], line 6\u001B[0m in \u001B[0;35mfrom_np_array\u001B[0m\n    return np.array(ast.literal_eval(array_string))\u001B[0m\n",
      "\u001B[0m  File \u001B[0;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:64\u001B[0m in \u001B[0;35mliteral_eval\u001B[0m\n    node_or_string = parse(node_or_string.lstrip(\" \\t\"), mode='eval')\u001B[0m\n",
      "\u001B[1;36m  File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ast.py:50\u001B[1;36m in \u001B[1;35mparse\u001B[1;36m\n\u001B[1;33m    return compile(source, filename, mode, flags,\u001B[1;36m\n",
      "\u001B[1;36m  File \u001B[1;32m<unknown>:1\u001B[1;36m\u001B[0m\n\u001B[1;33m    [[0.1,,0.2],,[0.3,,0.4]]\u001B[0m\n\u001B[1;37m          ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:42:52.989828Z",
     "start_time": "2024-10-17T06:42:52.980386Z"
    }
   },
   "cell_type": "code",
   "source": "new_data['caption'][0]",
   "id": "944128c9edb2354c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[[ 1.4988748  -0.2620451  -0.1156857  ... -1.47795     0.34309986\\n    0.5048515 ]\\n  [ 0.10183249 -0.91149616  1.3678972  ...  0.7048202   1.3715851\\n   -1.2201723 ]\\n  [-0.57482886  1.0277464   0.4090946  ... -0.7973502  -0.05981994\\n    1.1919205 ]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:42:53.384425Z",
     "start_time": "2024-10-17T06:42:53.363038Z"
    }
   },
   "cell_type": "code",
   "source": "from_np_array(new_data['caption'][0])",
   "id": "ab061b07939a0c55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.4988748,-0.2620451,-0.1156857,...,-1.47795,0.34309986,0.5048515,],[0.10183249,-0.91149616,1.3678972,...,0.7048202,1.3715851,-1.2201723,],[-0.57482886,1.0277464,0.4090946,...,-0.7973502,-0.05981994,1.1919205,],...,[0.,0.,0.,...,0.,0.,0.,],[0.,0.,0.,...,0.,0.,0.,],[0.,0.,0.,...,0.,0.,0.,]]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (1, 7) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mfrom_np_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_data\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcaption\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[16], line 6\u001B[0m, in \u001B[0;36mfrom_np_array\u001B[1;34m(array_string)\u001B[0m\n\u001B[0;32m      4\u001B[0m array_string \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(array_string\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[ \u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39msplit())\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(array_string)\n\u001B[1;32m----> 6\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mast\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mliteral_eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43marray_string\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mValueError\u001B[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (1, 7) + inhomogeneous part."
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:37:49.146629Z",
     "start_time": "2024-10-17T06:37:48.792196Z"
    }
   },
   "cell_type": "code",
   "source": "new_data = pd.read_csv(('../Data/Oxford_HIC/Caption2_oxford_hic_data.csv'), converters={'caption': from_np_array})",
   "id": "7d9b58694f16b73a",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (1, 7) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m new_data \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m../Data/Oxford_HIC/Caption2_oxford_hic_data.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconverters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcaption\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_np_array\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m   1013\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m   1014\u001B[0m     dialect,\n\u001B[0;32m   1015\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m   1023\u001B[0m )\n\u001B[0;32m   1024\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m-> 1026\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    623\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n\u001B[0;32m    625\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m parser:\n\u001B[1;32m--> 626\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mparser\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001B[0m, in \u001B[0;36mTextFileReader.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m   1916\u001B[0m nrows \u001B[38;5;241m=\u001B[39m validate_integer(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnrows\u001B[39m\u001B[38;5;124m\"\u001B[39m, nrows)\n\u001B[0;32m   1917\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1918\u001B[0m     \u001B[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001B[39;00m\n\u001B[0;32m   1919\u001B[0m     (\n\u001B[0;32m   1920\u001B[0m         index,\n\u001B[0;32m   1921\u001B[0m         columns,\n\u001B[0;32m   1922\u001B[0m         col_dict,\n\u001B[1;32m-> 1923\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[0;32m   1924\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnrows\u001B[49m\n\u001B[0;32m   1925\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1926\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[0;32m   1927\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001B[0m, in \u001B[0;36mCParserWrapper.read\u001B[1;34m(self, nrows)\u001B[0m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlow_memory:\n\u001B[1;32m--> 234\u001B[0m         chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_low_memory\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnrows\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    235\u001B[0m         \u001B[38;5;66;03m# destructive to chunks\u001B[39;00m\n\u001B[0;32m    236\u001B[0m         data \u001B[38;5;241m=\u001B[39m _concatenate_chunks(chunks)\n",
      "File \u001B[1;32mparsers.pyx:838\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:921\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._read_rows\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:1045\u001B[0m, in \u001B[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mparsers.pyx:2116\u001B[0m, in \u001B[0;36mpandas._libs.parsers._apply_converter\u001B[1;34m()\u001B[0m\n",
      "Cell \u001B[1;32mIn[3], line 5\u001B[0m, in \u001B[0;36mfrom_np_array\u001B[1;34m(array_string)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_np_array\u001B[39m(array_string):\n\u001B[0;32m      4\u001B[0m     array_string \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(array_string\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[ \u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39msplit())\n\u001B[1;32m----> 5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mast\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mliteral_eval\u001B[49m\u001B[43m(\u001B[49m\u001B[43marray_string\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mValueError\u001B[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (1, 7) + inhomogeneous part."
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:30:00.469814Z",
     "start_time": "2024-10-17T06:30:00.465999Z"
    }
   },
   "cell_type": "code",
   "source": "print(np.zeros((2,32)))",
   "id": "975f22a2c084433c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "new_data['caption'][0]",
   "id": "5974348ccaf91cb9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:29:04.539941Z",
     "start_time": "2024-10-17T06:29:04.537362Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "1b3f1a587f8c523c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array('[[[-1.2073393   1.7021252   0.1255643  ...  0.61603266  1.0901691\\n   -0.6272133 ]\\n  [-0.19677335 -1.6854944   2.637267   ...  0.0995123   0.05772842\\n    0.460544  ]\\n  [-2.6708777  -2.1325426  -1.3437752  ...  0.50450504 -0.25246727\\n   -0.8824037 ]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]',\n",
       "      dtype='<U480')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:22:40.150176Z",
     "start_time": "2024-10-17T06:22:40.144800Z"
    }
   },
   "cell_type": "code",
   "source": "new_data['caption'][0]",
   "id": "fb91b7b8171c1944",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[[-1.2073393   1.7021252   0.1255643  ...  0.61603266  1.0901691\\n   -0.6272133 ]\\n  [-0.19677335 -1.6854944   2.637267   ...  0.0995123   0.05772842\\n    0.460544  ]\\n  [-2.6708777  -2.1325426  -1.3437752  ...  0.50450504 -0.25246727\\n   -0.8824037 ]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:21:51.358484Z",
     "start_time": "2024-10-17T06:21:51.349826Z"
    }
   },
   "cell_type": "code",
   "source": "str(image)",
   "id": "fe9d2f0dcf456b8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[-1.8035113   0.5202071   0.44079134 ...  0.08648883 -0.49302292\\n  -2.6763113 ]\\n [ 0.2727667   1.3701634   0.02853544 ...  0.40519142 -0.20689118\\n   0.19367419]\\n [-0.30716035  2.7028956   2.3378856  ... -0.48075885 -0.6480472\\n  -0.2691131 ]\\n ...\\n [-2.5182683  -1.1495117   1.1463233  ... -0.09283329 -0.44629905\\n  -0.27841654]\\n [-2.0778356  -0.60657835 -0.2197553  ...  0.00846398  0.5642824\\n  -0.07053319]\\n [-0.702207   -0.23446804 -0.14710543 ... -0.41947365  0.8525086\\n   0.22255601]]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:21:46.050431Z",
     "start_time": "2024-10-17T06:21:46.042746Z"
    }
   },
   "cell_type": "code",
   "source": "type(str(image))",
   "id": "cd791194352ed2d5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:19:28.532595Z",
     "start_time": "2024-10-17T06:19:28.520851Z"
    }
   },
   "cell_type": "code",
   "source": "img_names[1]",
   "id": "64ecce93c608f515",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bokete_1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:20:16.023187Z",
     "start_time": "2024-10-17T06:20:15.924963Z"
    }
   },
   "cell_type": "code",
   "source": "new_data[new_data['image_id'] == img_names[1]]",
   "id": "1624e6c5dd3a9ec7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         image_id  \\\n",
       "1        bokete_1   \n",
       "71       bokete_1   \n",
       "83       bokete_1   \n",
       "14058    bokete_1   \n",
       "14061    bokete_1   \n",
       "...           ...   \n",
       "3364193  bokete_1   \n",
       "3364226  bokete_1   \n",
       "3364229  bokete_1   \n",
       "3364388  bokete_1   \n",
       "3364394  bokete_1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              caption  \\\n",
       "1         [[[-1.2073393   1.7021252   0.1255643  ...  0.61603266  1.0901691\\n   -0.6272133 ]\\n  [ 0.8378471  -0.6280587   1.6113715  ...  1.2343345  -1.0705055\\n    0.68104696]\\n  [-1.3428543   1.3530686  -1.3197852  ...  2.0909517  -0.22560208\\n   -0.23841415]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]   \n",
       "71       [[[-1.2073393   1.7021252   0.1255643  ...  0.61603266  1.0901691\\n   -0.6272133 ]\\n  [ 0.46632877  0.82260597 -0.7356484  ... -0.6014576   0.58393437\\n   -1.5014281 ]\\n  [ 0.6138676   0.24137755 -0.34282905 ... -0.33515948 -0.08146442\\n   -0.49719766]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]   \n",
       "83         [[[-1.2073393   1.7021252   0.1255643  ...  0.61603266  1.0901691\\n   -0.6272133 ]\\n  [ 2.3104925  -1.2284186   0.4108604  ... -1.3703358  -0.55332714\\n   -2.115347  ]\\n  [ 0.08541805 -0.9338966   0.35552385 ... -2.3272226  -0.437683\\n    0.37508503]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]   \n",
       "14058     [[[-1.2073393   1.7021252   0.1255643  ...  0.61603266  1.0901691\\n   -0.6272133 ]\\n  [ 0.28800568 -0.44470543  0.38858706 ... -0.6444891   0.68217653\\n    0.0292403 ]\\n  [-0.20284268  0.7190651   0.45093298 ...  0.14716679  1.6177548\\n   -0.8183027 ]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]   \n",
       "14061     [[[-1.2073393   1.7021252   0.1255643  ...  0.61603266  1.0901691\\n   -0.6272133 ]\\n  [ 1.0361224   0.6835399   0.35565475 ...  0.6691101  -0.84878945\\n   -2.251736  ]\\n  [ 0.06058354  0.6858816   0.36848387 ... -1.1212515   1.3296813\\n   -0.8366386 ]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ...   \n",
       "3364193  [[[ 0.5354878  -1.6377859   0.28486285 ... -1.0308985  -0.3299835\\n    0.06350164]\\n  [ 0.31435645  0.15635382  1.1615888  ...  0.37976146  0.89481306\\n    2.0685327 ]\\n  [ 1.8686832   0.08056349  1.3834219  ... -0.4654484  -0.57117975\\n   -0.11110411]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]   \n",
       "3364226    [[[ 0.5354878  -1.6377859   0.28486285 ... -1.0308985  -0.3299835\\n    0.06350164]\\n  [ 0.09726033  1.1991981   0.88470876 ... -0.15244927 -2.1828835\\n   -0.79935503]\\n  [-0.11938557  0.23644252 -0.19435006 ...  1.8796645  -0.0731431\\n    0.1618904 ]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]   \n",
       "3364229    [[[ 0.5354878  -1.6377859   0.28486285 ... -1.0308985  -0.3299835\\n    0.06350164]\\n  [-0.02113086 -0.13309428 -0.04981306 ... -1.3503776  -0.0695071\\n    0.25330848]\\n  [ 1.1683044   0.4262135   0.28816196 ... -2.0449386   0.5471454\\n   -1.561776  ]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]   \n",
       "3364388   [[[ 0.5354878  -1.6377859   0.28486285 ... -1.0308985  -0.3299835\\n    0.06350164]\\n  [-0.45088387  0.65365636  1.5499189  ... -0.69708693 -0.45897043\\n   -0.4077254 ]\\n  [-0.97835624 -1.2394557   0.48008233 ...  1.8498646  -1.1807806\\n   -0.8074078 ]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]   \n",
       "3364394    [[[ 0.5354878  -1.6377859   0.28486285 ... -1.0308985  -0.3299835\\n    0.06350164]\\n  [-0.0079591   1.788373   -1.2496855  ... -0.20522931 -1.8884162\\n    0.2368834 ]\\n  [-0.75013995 -0.18466908 -0.6167592  ...  0.8609214   0.5037136\\n    1.2767462 ]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]   \n",
       "\n",
       "         funny_score  \n",
       "1            0.00000  \n",
       "71           0.00000  \n",
       "83           0.00000  \n",
       "14058        0.00000  \n",
       "14061        0.00000  \n",
       "...              ...  \n",
       "3364193      0.00000  \n",
       "3364226      0.00002  \n",
       "3364229      0.00000  \n",
       "3364388      0.00000  \n",
       "3364394      0.00000  \n",
       "\n",
       "[1292 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>caption</th>\n",
       "      <th>funny_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bokete_1</td>\n",
       "      <td>[[[-1.2073393   1.7021252   0.1255643  ...  0.61603266  1.0901691\\n   -0.6272133 ]\\n  [ 0.8378471  -0.6280587   1.6113715  ...  1.2343345  -1.0705055\\n    0.68104696]\\n  [-1.3428543   1.3530686  -1.3197852  ...  2.0909517  -0.22560208\\n   -0.23841415]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>bokete_1</td>\n",
       "      <td>[[[-1.2073393   1.7021252   0.1255643  ...  0.61603266  1.0901691\\n   -0.6272133 ]\\n  [ 0.46632877  0.82260597 -0.7356484  ... -0.6014576   0.58393437\\n   -1.5014281 ]\\n  [ 0.6138676   0.24137755 -0.34282905 ... -0.33515948 -0.08146442\\n   -0.49719766]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>bokete_1</td>\n",
       "      <td>[[[-1.2073393   1.7021252   0.1255643  ...  0.61603266  1.0901691\\n   -0.6272133 ]\\n  [ 2.3104925  -1.2284186   0.4108604  ... -1.3703358  -0.55332714\\n   -2.115347  ]\\n  [ 0.08541805 -0.9338966   0.35552385 ... -2.3272226  -0.437683\\n    0.37508503]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14058</th>\n",
       "      <td>bokete_1</td>\n",
       "      <td>[[[-1.2073393   1.7021252   0.1255643  ...  0.61603266  1.0901691\\n   -0.6272133 ]\\n  [ 0.28800568 -0.44470543  0.38858706 ... -0.6444891   0.68217653\\n    0.0292403 ]\\n  [-0.20284268  0.7190651   0.45093298 ...  0.14716679  1.6177548\\n   -0.8183027 ]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14061</th>\n",
       "      <td>bokete_1</td>\n",
       "      <td>[[[-1.2073393   1.7021252   0.1255643  ...  0.61603266  1.0901691\\n   -0.6272133 ]\\n  [ 1.0361224   0.6835399   0.35565475 ...  0.6691101  -0.84878945\\n   -2.251736  ]\\n  [ 0.06058354  0.6858816   0.36848387 ... -1.1212515   1.3296813\\n   -0.8366386 ]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364193</th>\n",
       "      <td>bokete_1</td>\n",
       "      <td>[[[ 0.5354878  -1.6377859   0.28486285 ... -1.0308985  -0.3299835\\n    0.06350164]\\n  [ 0.31435645  0.15635382  1.1615888  ...  0.37976146  0.89481306\\n    2.0685327 ]\\n  [ 1.8686832   0.08056349  1.3834219  ... -0.4654484  -0.57117975\\n   -0.11110411]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364226</th>\n",
       "      <td>bokete_1</td>\n",
       "      <td>[[[ 0.5354878  -1.6377859   0.28486285 ... -1.0308985  -0.3299835\\n    0.06350164]\\n  [ 0.09726033  1.1991981   0.88470876 ... -0.15244927 -2.1828835\\n   -0.79935503]\\n  [-0.11938557  0.23644252 -0.19435006 ...  1.8796645  -0.0731431\\n    0.1618904 ]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]</td>\n",
       "      <td>0.00002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364229</th>\n",
       "      <td>bokete_1</td>\n",
       "      <td>[[[ 0.5354878  -1.6377859   0.28486285 ... -1.0308985  -0.3299835\\n    0.06350164]\\n  [-0.02113086 -0.13309428 -0.04981306 ... -1.3503776  -0.0695071\\n    0.25330848]\\n  [ 1.1683044   0.4262135   0.28816196 ... -2.0449386   0.5471454\\n   -1.561776  ]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364388</th>\n",
       "      <td>bokete_1</td>\n",
       "      <td>[[[ 0.5354878  -1.6377859   0.28486285 ... -1.0308985  -0.3299835\\n    0.06350164]\\n  [-0.45088387  0.65365636  1.5499189  ... -0.69708693 -0.45897043\\n   -0.4077254 ]\\n  [-0.97835624 -1.2394557   0.48008233 ...  1.8498646  -1.1807806\\n   -0.8074078 ]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364394</th>\n",
       "      <td>bokete_1</td>\n",
       "      <td>[[[ 0.5354878  -1.6377859   0.28486285 ... -1.0308985  -0.3299835\\n    0.06350164]\\n  [-0.0079591   1.788373   -1.2496855  ... -0.20522931 -1.8884162\\n    0.2368834 ]\\n  [-0.75013995 -0.18466908 -0.6167592  ...  0.8609214   0.5037136\\n    1.2767462 ]\\n  ...\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]\\n  [ 0.          0.          0.         ...  0.          0.\\n    0.        ]]]</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1292 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:19:42.924914Z",
     "start_time": "2024-10-17T06:19:42.833750Z"
    }
   },
   "cell_type": "code",
   "source": "new_data['image_id'].value_counts().get(img_names[1])",
   "id": "c2b9d5645be4b79",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1292"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:20:25.970295Z",
     "start_time": "2024-10-17T06:20:25.937222Z"
    }
   },
   "cell_type": "code",
   "source": "new_data.replace({'image_id': img_names[1]}, value=image, regex=True)",
   "id": "10a77559ca31cbd7",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "value argument must be scalar, dict, or Series",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mnew_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreplace\u001B[49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage_id\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mimg_names\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mregex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:8085\u001B[0m, in \u001B[0;36mNDFrame.replace\u001B[1;34m(self, to_replace, value, inplace, limit, regex, method)\u001B[0m\n\u001B[0;32m   8083\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_replace_columnwise(mapping, inplace, regex)\n\u001B[0;32m   8084\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 8085\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalue argument must be scalar, dict, or Series\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   8087\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_list_like(to_replace):\n\u001B[0;32m   8088\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_list_like(value):\n\u001B[0;32m   8089\u001B[0m         \u001B[38;5;66;03m# e.g. to_replace = [NA, ''] and value is 0,\u001B[39;00m\n\u001B[0;32m   8090\u001B[0m         \u001B[38;5;66;03m#  so we replace NA with 0 and then replace '' with 0\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: value argument must be scalar, dict, or Series"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d7bf18de2059f079"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
