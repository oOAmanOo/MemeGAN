{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T03:26:19.514174Z",
     "start_time": "2024-10-14T03:26:19.510397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from wheel.macosx_libfile import read_data\n",
    "\n",
    "from extractor import addImagePath, textExtraction, imageExtraction, textExtractReverse"
   ],
   "id": "7123375a60e24a10",
   "outputs": [],
   "execution_count": 156
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Check the max length of the text data\n",
   "id": "3dae9bb9554dc2e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T05:27:32.060713Z",
     "start_time": "2024-10-09T05:27:31.979491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subUrlList = ['wendys','mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada','sonicdrivein']\n",
    "max_length = 0\n",
    "sum_length = 0\n",
    "for subUrl in subUrlList:\n",
    "    dirPath = '../Data/Instagram/Filter_' + subUrl + '.csv'\n",
    "    data = pd.read_csv(dirPath)\n",
    "    \n",
    "    for i in range(len(data['caption'])):\n",
    "        sum_length += len(str(data['caption'][i]).split())\n",
    "        if len(str(data['caption'][i]).split()) > max_length:\n",
    "            max_length = len(str(data['caption'][i]).split())\n",
    "            word = data['caption'][i]\n",
    "print(max_length)\n",
    "print(sum_length/len(data['caption']))"
   ],
   "id": "32667357b99a51a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373\n",
      "31.870794078061913\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:35:01.489143Z",
     "start_time": "2024-09-23T23:34:44.345758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_length = 0\n",
    "word = ''\n",
    "dirPath = '../Data/Oxford_HIC/oxford_hic_data.csv'\n",
    "data = pd.read_csv(dirPath)\n",
    "max_length = 0\n",
    "sum_length = 0\n",
    "index =0 \n",
    "counter  = 5\n",
    "# find the max word count of the text data['caption']\n",
    "for i in range(len(data['caption'])):\n",
    "    sum_length += len(str(data['caption'][i]).split())\n",
    "    if len(str(data['caption'][i]).split()) > max_length:\n",
    "        max_length = len(str(data['caption'][i]).split())\n",
    "        word = data['caption'][i]\n",
    "        index = i       \n",
    "        #\n",
    "print(max_length, i)\n",
    "print(sum_length/len(data['caption']))\n",
    "data.shape"
   ],
   "id": "b3b8a1b3461c9aed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5988\\3251324240.py:4: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(dirPath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9729 3657846\n",
      "10.514508397972905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3657847, 3)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 193
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:35:22.129734Z",
     "start_time": "2024-09-23T23:35:22.126848Z"
    }
   },
   "cell_type": "code",
   "source": "sum_length",
   "id": "21d6947ca3cd9281",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38460463"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 195
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Check the max word count of the text data",
   "id": "ca059487768954a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T18:33:30.989254Z",
     "start_time": "2024-09-25T18:33:28.914261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subUrlList = ['wendys','mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada','sonicdrivein']\n",
    "wordList = []\n",
    "total = 0\n",
    "for subUrl in subUrlList:\n",
    "    dirPath = '../Data/Instagram/Filter_' + subUrl + '.csv'\n",
    "    data = pd.read_csv(dirPath)\n",
    "    \n",
    "    for i in range(len(data['caption'])):\n",
    "        for word in str(data['caption'][i]).split():\n",
    "            if word not in wordList:\n",
    "                wordList.append(word)\n",
    "                total += 1\n",
    "                \n",
    "print(len(wordList))        \n",
    "print(total)"
   ],
   "id": "b2b206fcf154474e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15056\n",
      "15056\n"
     ]
    }
   ],
   "execution_count": 260
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-25T18:33:33.703355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wordList = []\n",
    "total = 0\n",
    "dirPath = '../Data/Oxford_HIC/oxford_hic_data.csv'\n",
    "data = pd.read_csv(dirPath)\n",
    "\n",
    "for i in range(len(data['caption'])):\n",
    "    for word in str(data['caption'][i]).split():\n",
    "        if word not in wordList:\n",
    "            wordList.append(word)\n",
    "            total += 1\n",
    "\n",
    "print(len(wordList))        \n",
    "print(total)"
   ],
   "id": "a02f3990a97e4a87",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5988\\496614120.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(dirPath)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[261], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(dirPath)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaption\u001B[39m\u001B[38;5;124m'\u001B[39m])):\n\u001B[1;32m----> 5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaption\u001B[39m\u001B[38;5;124m'\u001B[39m][i])\u001B[38;5;241m.\u001B[39msplit():\n\u001B[0;32m      6\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m wordList:\n\u001B[0;32m      7\u001B[0m             wordList\u001B[38;5;241m.\u001B[39mappend(word)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 261
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Load the data and split the data",
   "id": "6f301289c5bb2722"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:47:47.661978Z",
     "start_time": "2024-10-14T00:47:31.505879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if args.img - dir == 'Oxford_HIC':\n",
    "#     dirPath = '../Data/Oxford_HIC/oxford_hic_data.csv'\n",
    "#     imgPath = '../Data/Oxford_HIC/oxford_img/'\n",
    "# else:\n",
    "dirPath = '../Data/Instagram/Filter_' + 'wendys' + '.csv'\n",
    "imgPath = '../Data/Instagram/' + 'wendys' + '_img/'\n",
    "# load data\n",
    "data = pd.read_csv(dirPath)\n",
    "data = addImagePath(data, imgPath)\n",
    "# split data\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_text = textExtraction(train['caption'].tolist())\n",
    "train_image = imageExtraction(train['image_id'])\n",
    "train_funny_score = torch.tensor(train['funny_score'].to_numpy())\n",
    "test_text = textExtraction(test['caption'])\n",
    "test_image = imageExtraction(test['image_id'])\n",
    "test_funny_score = torch.tensor(test['funny_score'].to_numpy())"
   ],
   "id": "8bddfef236149dd6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:47:47.673085Z",
     "start_time": "2024-10-14T00:47:47.668979Z"
    }
   },
   "cell_type": "code",
   "source": "train_text.shape, train_image.shape, train_funny_score.shape",
   "id": "851fa717c816958b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([293, 64, 768]), torch.Size([293, 64, 768]), torch.Size([293]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:47:47.694673Z",
     "start_time": "2024-10-14T00:47:47.691424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "train_dataset = torch.utils.data.TensorDataset(train_text, train_image, train_funny_score)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_text, test_image, test_funny_score)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "127ff8b78bd6db6a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. LLM Test",
   "id": "807b418cfd673b81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T16:21:23.472827Z",
     "start_time": "2024-09-30T16:21:22.108642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 不確定是否為官方的 Gemini ############################################################################\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"describeai/gemini\")\n",
    "gemini = AutoModelForSeq2SeqLM.from_pretrained(\"describeai/gemini\")\n",
    "#######################################################################################################\n",
    "gemini"
   ],
   "id": "9368441165fc2667",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T23:07:28.813666Z",
     "start_time": "2024-10-13T23:07:16.732652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 官方的Gemma #########################################################################################\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\", device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "gemmaConfig =  AutoConfig.from_pretrained('google/gemma-2-2b')\n",
    "########################################################################################################"
   ],
   "id": "2b63e7fd2e5f833e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1405: UserWarning: Current model requires 6656 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f64625db5c04e14b36bb3215cf25d17"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T23:33:49.178647Z",
     "start_time": "2024-10-13T23:33:46.257835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 官方的Gemma #########################################################################################\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "gemmaConfig =  AutoConfig.from_pretrained('google/gemma-2-2b-it')\n",
    "########################################################################################################"
   ],
   "id": "e5f8f2c83fc25118",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88f7725d06f34f56b809770694c1830e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T16:47:42.602569Z",
     "start_time": "2024-09-22T16:47:41.350489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gemma.to(device)\n",
    "vocab_size = 256128  # 词汇表大小\n",
    "embedding_dim = 768  # 嵌入维度，与你的图像嵌入维度相同\n",
    "text_embedding = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "\n",
    "words = \"👻🔥😂😁👍🤦‍♀️🤦‍♂️🤷‍♀️🤷‍♂️✌🤞😉😎🎶😢💖🎉🌹💋👏🐱‍💻🐱‍🐉🐱‍👓✔👀😃✨😆🤔🤢🎁🫢 ha ha\"\n",
    "tokens = tokenizer(words, truncation=True, padding= 'max_length', max_length=100, return_tensors=\"pt\")\n",
    "output = text_embedding(tokens['input_ids'].to(device))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def find_closest_embeddings(x, embedding_matrix, top_k=1):\n",
    "    # Normalize both the input tensor x and the embedding matrix\n",
    "    x = F.normalize(x, dim=1)  # Normalize input tensor along feature dimension\n",
    "    embedding_matrix = F.normalize(embedding_matrix, dim=1)  # Normalize embedding matrix\n",
    "    \n",
    "    # Compute cosine similarity between x and embedding matrix\n",
    "    similarity = torch.matmul(x, embedding_matrix.T)  # Shape: [10, 50265]\n",
    "    \n",
    "    # Find top-k closest embeddings for each tensor in x\n",
    "    top_k_values, top_k_indices = torch.topk(similarity, top_k, dim=1)\n",
    "    \n",
    "    return top_k_indices, top_k_values\n",
    "\n",
    "\n",
    "# print(output.squeeze(0).shape)\n",
    "top_k_indices, top_k_values = find_closest_embeddings(output.squeeze(0), text_embedding.weight)\n",
    "# top_k_indices.shape\n",
    "indices = tokenizer.decode(top_k_indices.squeeze(-1))\n",
    "print(indices)"
   ],
   "id": "b5a1bd08816d845f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 768])\n",
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><bos>👻🔥😂😁👍🤦‍♀️🤦‍♂️🤷‍♀️🤷‍♂️✌🤞😉😎🎶😢💖🎉🌹💋👏🐱‍💻🐱‍🐉🐱‍👓✔👀😃✨😆🤔🤢🎁🫢 ha ha\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T20:36:50.602442Z",
     "start_time": "2024-09-23T20:36:50.584473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = \"👻🔥😂😁👍🤦‍♀️🤦‍♂️🤷‍♀️🤷‍♂️✌🤞😉😎🎶😢💖🎉🌹💋👏🐱‍💻🐱‍🐉🐱‍👓✔👀😃✨😆🤔🤢🎁🫢 ha ha\"\n",
    "tokens = tokenizer(words, truncation=True, padding= 'max_length', max_length=100, return_tensors=\"pt\")\n",
    "tokens"
   ],
   "id": "9a5b2095330cfcd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      2, 242538, 237638, 236471, 238429, 237019, 240525,  67292,\n",
       "         240525,  68399, 239921,  67292, 239921,  68399, 239529, 241807, 238309,\n",
       "         238859, 240438, 240116, 239208, 239548, 240315, 240887, 238499, 242993,\n",
       "         235879, 242482, 242993, 235879, 245092, 242993, 235879, 246943, 237488,\n",
       "         239220, 239938, 236309, 239312, 238918, 241769, 241227, 248165,    661,\n",
       "            661]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T19:03:27.261799Z",
     "start_time": "2024-09-23T19:03:27.257163Z"
    }
   },
   "cell_type": "code",
   "source": "gemma",
   "id": "f26de7efeee68a71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Generator",
   "id": "d8c8fefd0a84950b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T01:03:35.027397Z",
     "start_time": "2024-10-03T01:03:22.299642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\", device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "gemma"
   ],
   "id": "3de2433a8664699a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85ba5a944fab4f8b995d3f83c7f80630"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm()\n",
       "        (post_attention_layernorm): Gemma2RMSNorm()\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T06:03:20.898613Z",
     "start_time": "2024-10-09T06:03:13.883521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "gemma"
   ],
   "id": "c04d83141427f594",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a8ebecbda2641ec9a3e7af1afee356a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm()\n",
       "        (post_attention_layernorm): Gemma2RMSNorm()\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:47:50.836836Z",
     "start_time": "2024-10-14T00:47:47.707285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 官方的Gemma #########################################################################################\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "gemmaConfig =  AutoConfig.from_pretrained('google/gemma-2-2b-it')\n",
    "########################################################################################################"
   ],
   "id": "a2c8fa333911a14e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a64ada164a8422f8dfb355ebe443cee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T03:16:30.194006Z",
     "start_time": "2024-10-14T03:16:30.179857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # self attention\n",
    "        self.selfAttentionMultihead = nn.MultiheadAttention(768, 1)\n",
    "        self.selfAttentionLayerNorm = nn.LayerNorm(768)\n",
    "        self.selfAttentionLinear = nn.Linear(768, 768)\n",
    "        self.selfAttentionLayerNorm2 = nn.LayerNorm(768)\n",
    "        \n",
    "        # multihead attention\n",
    "        self.multiheadAttentionMultihead = nn.MultiheadAttention(768, 8)\n",
    "        self.multiheadAttentionLinear = nn.Linear(768, 768)\n",
    "        self.multiheadAttentionLayerNorm = nn.LayerNorm(768)\n",
    "        \n",
    "        # co-attention text\n",
    "        self.coAttentionTextMultihead = nn.MultiheadAttention(768, 1)\n",
    "        self.coAttentionTextLinear = nn.Linear(768, 768)\n",
    "        self.coAttentionTextLayerNorm = nn.LayerNorm(768)\n",
    "\n",
    "        # co-attention image\n",
    "        self.coAttentionImageMultihead = nn.MultiheadAttention(768, 1)\n",
    "        self.coAttentionImageLinear = nn.Linear(768, 768)\n",
    "        self.coAttentionImageLayerNorm = nn.LayerNorm(768)\n",
    "    \n",
    "        # feed forward\n",
    "        self.feedForwardLinear = nn.Linear(768, 768)\n",
    "        self.feedForwardLayerNorm = nn.LayerNorm(768)\n",
    "        \n",
    "        # gemma\n",
    "        self.gemmaLinearMaxTokens = nn.Linear(64, 16)\n",
    "        self.gemmaLinearBefore = nn.Linear(768, gemmaConfig.vocab_size)\n",
    "        self.gemmaSoftmax = nn.Softmax(dim=2)\n",
    "        self.gemma = nn.Sequential(*list(gemma.children())[:-1])\n",
    "        self.gemmaLm_head = nn.Sequential(*list(gemma.children())[1:])\n",
    "        \n",
    "        # funny score\n",
    "        self.FunnyScorelinear1 = nn.Linear(768, 1)\n",
    "        self.FunnyScorelinear2 = nn.Linear(64, 1)\n",
    "        \n",
    "    def self_multi(self, image, text):\n",
    "        # self attention module\n",
    "        self_out = self.selfAttentionMultihead(image, image, image)[0]\n",
    "        self_out = self.selfAttentionLinear(self_out)\n",
    "        self_out = self.selfAttentionLayerNorm(self_out + image)\n",
    "\n",
    "        # multihead attention module\n",
    "        multi_out = self.multiheadAttentionMultihead(text, text, text)[0]\n",
    "        multi_out = self.multiheadAttentionLinear(multi_out)\n",
    "        multi_out = self.multiheadAttentionLayerNorm(multi_out + text)\n",
    "        \n",
    "        return self_out, multi_out\n",
    "        \n",
    "    def co_attention(self, image, text):\n",
    "        # co-attention image module\n",
    "        visual_attending_textual = self.coAttentionTextMultihead(image, text, text)[0]\n",
    "        visual_attending_textual = self.coAttentionTextLinear(visual_attending_textual)\n",
    "        visual_attending_textual = self.coAttentionTextLayerNorm(visual_attending_textual + image)\n",
    "        \n",
    "        # co-attention text module\n",
    "        textual_attending_visual = self.coAttentionTextMultihead(text, image, image)[0]\n",
    "        textual_attending_visual = self.coAttentionTextLinear(textual_attending_visual)\n",
    "        textual_attending_visual = self.coAttentionTextLayerNorm(textual_attending_visual + text) \n",
    "        \n",
    "        return visual_attending_textual, textual_attending_visual              \n",
    "        \n",
    "    def gemmaGenerate(self, x):\n",
    "        with torch.no_grad():\n",
    "            # maximum 32 tokens\n",
    "            x = self.gemmaLinearMaxTokens(x.transpose(1, 2)).transpose(1, 2)\n",
    "            x = self.gemmaLinearBefore(x)\n",
    "            x = self.gemmaSoftmax(x)\n",
    "            # get max value of each row, total 32*64\n",
    "            top_k_values, top_k_indices = torch.topk(x, 1, dim=2, largest=True)\n",
    "            toGemma = textExtractReverse(top_k_indices).to(device)\n",
    "            # 使用gemma作為model的一部分\n",
    "            output = self.gemma(toGemma)\n",
    "            # output[0] = last_hidden_state\n",
    "            # output[1] = past_key_values\n",
    "            \n",
    "        return output[0]\n",
    "               \n",
    "    \n",
    "    def forward(self, text, image):\n",
    "        # max_seq_len = max(text.shape[1], image.shape[1])\n",
    "        # text = nn.functional.pad(text, (0, 0, 0, max_seq_len - text.shape[1]))\n",
    "        # image = nn.functional.pad(image, (0, 0, 0, max_seq_len - image.shape[1]))\n",
    "        text = text.transpose(0, 1)\n",
    "        image = image.transpose(0, 1)\n",
    "        \n",
    "        ######################### Transformer ######################### \n",
    "            # self attention\n",
    "        self_out, multi_out = self.self_multi(image, text)\n",
    "        # co-attention\n",
    "        visual_attending_textual, textual_attending_visual = self.co_attention(self_out, multi_out)\n",
    "        ###############################################################\n",
    "        \n",
    "        # feature fusion\n",
    "        feature_fusion = visual_attending_textual + textual_attending_visual\n",
    "        feature_fusion = self.feedForwardLinear(feature_fusion)\n",
    "        feature_fusion = self.feedForwardLayerNorm(feature_fusion + feature_fusion)\n",
    "        feature_fusion = feature_fusion.squeeze(-1)\n",
    "        feature_fusion = feature_fusion.transpose(0, 1)\n",
    "        ####################### gemma  generate #######################\n",
    "        last_hidden_state = self.gemmaGenerate(feature_fusion)\n",
    "        output_text = self.gemmaLm_head(last_hidden_state)\n",
    "        ###############################################################\n",
    "        \n",
    "        ######################### funny score #########################\n",
    "        output_funny_score = self.FunnyScorelinear1(feature_fusion).squeeze(-1)\n",
    "        output_funny_score = self.FunnyScorelinear2(output_funny_score).squeeze(-1)\n",
    "        ###############################################################\n",
    "        \n",
    "        return output_text, output_funny_score\n",
    "    \n",
    "    def generate(self, image, max_length = 100):\n",
    "        generated_tokens = []\n",
    "        generated_tokens.append(2) #<bos> = 2\n",
    "        text = torch.zeros_like(image).to(device)\n",
    "        text = text.transpose(0, 1)\n",
    "        image = image.transpose(0, 1)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "        \n",
    "        # 有時後空格會失效，所以手動插入空格 <pad> = 0\n",
    "        def insert_zeros(list):\n",
    "            zeros = [0] * (2 * len(list) - 1)\n",
    "            zeros[::2] = list\n",
    "            return zeros\n",
    "        \n",
    "        lastTurn = False\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length + 1):\n",
    "                # Transformer\n",
    "                # self attention\n",
    "                self_out, multi_out = self.self_multi(image, text)\n",
    "                # co-attention\n",
    "                visual_attending_textual, textual_attending_visual = self.co_attention(self_out, multi_out)\n",
    "                \n",
    "                # feature fusion\n",
    "                feature_fusion = visual_attending_textual + textual_attending_visual\n",
    "                feature_fusion = self.feedForwardLinear(feature_fusion)\n",
    "                feature_fusion = self.feedForwardLayerNorm(feature_fusion + feature_fusion)\n",
    "                feature_fusion = feature_fusion.squeeze(-1)\n",
    "                feature_fusion = feature_fusion.transpose(0, 1)\n",
    "                \n",
    "                # gemma generate\n",
    "                last_hidden_state = self.gemmaGenerate(feature_fusion)\n",
    "                output_text = self.gemmaLm_head(last_hidden_state)\n",
    "                \n",
    "                # funny score\n",
    "                output_funny_score = self.FunnyScorelinear1(feature_fusion).squeeze(-1)\n",
    "                output_funny_score = self.FunnyScorelinear2(output_funny_score).squeeze(-1)\n",
    "                \n",
    "                if lastTurn: # show final funny score\n",
    "                    return generated_caption, output_funny_score\n",
    "                else:\n",
    "                    next_token_logits = output_text[:, -1, :]\n",
    "                    next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "                    next_token_id = torch.argmax(next_token_probs, dim=-1).item()\n",
    "                    generated_tokens.append(next_token_id)\n",
    "                    \n",
    "                    generated_caption = insert_zeros(generated_tokens)\n",
    "                    generated_caption = tokenizer.decode(generated_caption, skip_special_tokens=False)\n",
    "                    generated_caption = generated_caption.replace(\"<pad>\", \" \").replace(\"  \", \" \").split()\n",
    "                    generated_caption = [word for word in generated_caption if word[0] != \"<\"]\n",
    "                    generated_caption = \" \".join(generated_caption)\n",
    "                                               \n",
    "                    text = textExtraction([generated_caption]).to(device)\n",
    "                    text = text.transpose(0, 1)\n",
    "                    \n",
    "                    if next_token_id in gemmaConfig.eos_token_id or len(generated_caption.split()) > max_length: \n",
    "                        #<eos> = 1; <end_of_turn> = 107\n",
    "                        lastTurn = True"
   ],
   "id": "6df9340e53bfbe2a",
   "outputs": [],
   "execution_count": 130
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# vision encoder\n",
    "        self.visual_encoder = VisionTransformerEncoder.from_config(vit_type = vit_type, adapter_type=adapter_type)\n",
    "        # text encoder + multimodal decoder\n",
    "        self.text_decoder = XBertLMHeadDecoder.from_config(med_config_path, False)\n",
    "\n",
    "        self.prompt = prompt\n",
    "        self.prompt_length = len(self.tokenizer(self.prompt).input_ids) - 1\n",
    "\n",
    "        self.max_txt_len = max_txt_len\n",
    "        self.adapter_type = adapter_type\n",
    "        self.bert_adapter = bert_adapter\n",
    "        self.tune_language = tune_language\n",
    "        # if((self.bert_adapter or self.tune_language) and self.adapter_type == None):\n",
    "        if visual_projection == \"linear\":\n",
    "            self.VLBridge = nn.Linear(768, 768)\n",
    "        elif visual_projection == \"ViT_block\":\n",
    "            self.VLBridge = Block(dim=768,num_heads=12,)\n",
    "        else:\n",
    "            self.VLBridge = None"
   ],
   "id": "a6fbc10fcbbbc6d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Discriminator",
   "id": "e431f844eea97c94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T03:50:05.533911Z",
     "start_time": "2024-10-14T03:50:05.523869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Generator\n",
    "        self.g_linearFake = nn.Linear(256000, 768)\n",
    "        self.g_con_mlp1 = nn.Linear(768, 2)\n",
    "        self.g_con_mlp2 = nn.Linear(128, 1)\n",
    "        self.g_unc_mlp1 = nn.Linear(768, 1)\n",
    "        self.g_unc_mlp2 = nn.Linear(64, 1)\n",
    "        # Discriminator\n",
    "        self.d_linearFake = nn.Linear(gemmaConfig.vocab_size, 768)\n",
    "        self.d_con_mlp1_r2f = nn.Linear(768, 2)\n",
    "        self.d_con_mlp2_r2f = nn.Linear(256, 1)\n",
    "        self.d_con_mlp1_f2r = nn.Linear(768, 2)\n",
    "        self.d_con_mlp2_f2r = nn.Linear(256, 1)\n",
    "        self.d_con_mlp1_g = nn.Linear(768, 2)\n",
    "        self.d_con_mlp2_g = nn.Linear(128, 1)\n",
    "        self.d_con_mlp1_m = nn.Linear(768, 2)\n",
    "        self.d_con_mlp2_m = nn.Linear(128, 1)\n",
    "        self.d_unc_mlp1_r = nn.Linear(768, 1)\n",
    "        self.d_unc_mlp2_r = nn.Linear(64, 1)\n",
    "        self.d_unc_mlp1_g = nn.Linear(768, 1)\n",
    "        self.d_unc_mlp2_g = nn.Linear(64, 1)\n",
    "        self.d_unc_mlp1_m = nn.Linear(768, 1)\n",
    "        self.d_unc_mlp2_m = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, real_text, fake_text, image):   \n",
    "        # real_text = [batch_size, 64, 768]\n",
    "        # fake_text = [batch_size, 256, 256000]\n",
    "        # image = [batch_size, 64, 768]\n",
    "        g_fake_text = self.g_linearFake(fake_text)\n",
    "        \n",
    "        d_fake_text = self.d_linearFake(fake_text)\n",
    "        mismatched_text = torch.roll(real_text, 1, 0)\n",
    "        \n",
    "        # conditional (contrastive)\n",
    "        C_r = torch.cat((real_text, image), dim=1)\n",
    "        g_C_g = torch.cat((g_fake_text, image), dim=1)\n",
    "        d_C_g = torch.cat((d_fake_text, image), dim=1)\n",
    "        C_m = torch.cat((mismatched_text, image), dim=1)\n",
    "        # contrastive discriminator\n",
    "        d_C_r2f = torch.cat((C_r, d_C_g), dim=1)\n",
    "        d_C_f2r = torch.cat((d_C_g, C_r), dim=1)\n",
    "        ########################## Generator ##########################\n",
    "        g_C_g = self.g_con_mlp1(g_C_g)\n",
    "        g_C_g = self.g_con_mlp2(g_C_g.transpose(1,2)).squeeze(-1)\n",
    "        ###############################################################\n",
    "        \n",
    "        ######################## Discriminator ########################\n",
    "        d_C_r2f = self.d_con_mlp1_r2f(d_C_r2f)\n",
    "        d_C_f2r = self.d_con_mlp1_f2r(d_C_f2r)\n",
    "        d_C_g = self.d_con_mlp1_g(d_C_g)\n",
    "        d_C_m = self.d_con_mlp1_m(C_m)\n",
    "        d_C_r2f = self.d_con_mlp2_r2f(d_C_r2f.transpose(1,2)).squeeze(-1).unsqueeze(0)\n",
    "        d_C_f2r = self.d_con_mlp2_r2f(d_C_f2r.transpose(1,2)).squeeze(-1).unsqueeze(0)\n",
    "        d_C_g = self.d_con_mlp2_g(d_C_g.transpose(1,2)).squeeze(-1).unsqueeze(0)\n",
    "        d_C_m = self.d_con_mlp2_m(d_C_m.transpose(1,2)).squeeze(-1).unsqueeze(0)\n",
    "        d_con_output = torch.cat((d_C_r2f, d_C_f2r, d_C_g, d_C_m), dim=0)\n",
    "        ###############################################################\n",
    "        \n",
    "        \n",
    "        #### unconditional ####\n",
    "        ########################## Generator ##########################\n",
    "        g_UC_g = self.g_unc_mlp1(g_fake_text).squeeze(-1)\n",
    "        g_UC_g = self.g_unc_mlp2(g_UC_g).squeeze(-1)\n",
    "        ###############################################################\n",
    "        \n",
    "        ######################## Discriminator ########################\n",
    "        d_UC_r  = self.d_unc_mlp1_r(real_text).squeeze(-1)\n",
    "        d_UC_g  = self.d_unc_mlp1_g(d_fake_text).squeeze(-1)\n",
    "        d_UC_m  = self.d_unc_mlp1_m(mismatched_text).squeeze(-1)\n",
    "        d_UC_r = self.d_unc_mlp2_r(d_UC_r).squeeze(-1).unsqueeze(0)\n",
    "        d_UC_g = self.d_unc_mlp2_g(d_UC_g).squeeze(-1).unsqueeze(0)\n",
    "        d_UC_m = self.d_unc_mlp2_m(d_UC_m).squeeze(-1).unsqueeze(0)\n",
    "        d_unc_output = torch.cat((d_UC_r, d_UC_g, d_UC_m), dim=0)\n",
    "        ###############################################################\n",
    "        # torch.Size([3, 32, 1])\n",
    "        return g_C_g, g_UC_g, d_con_output, d_unc_output"
   ],
   "id": "ff75f88ea94b5045",
   "outputs": [],
   "execution_count": 182
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T03:50:06.224767Z",
     "start_time": "2024-10-14T03:50:06.120574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# empty cuda memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "id": "d5c9b5f31e3aa208",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "303"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 183
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T03:50:08.466047Z",
     "start_time": "2024-10-14T03:50:06.488200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NetG = Generator().to(device)\n",
    "NetD = Discriminator().to(device)\n",
    "optimizer_G = optim.Adam(NetG.parameters(), lr=0.001)\n",
    "optimizer_D = optim.Adam(NetD.parameters(), lr=0.001)\n",
    "train_losses_FC = []\n",
    "train_losses_G = []\n",
    "train_losses_D = []\n",
    "test_losses_FC = []\n",
    "test_losses_G = []\n",
    "test_losses_D = []\n",
    "save = []\n",
    "present_epoch = 1\n",
    "best_train_loss_FC = 9999\n",
    "best_train_loss_G = 9999\n",
    "best_train_loss_D = 9999\n",
    "best_test_loss_FC = 9999\n",
    "best_test_loss_G = 9999\n",
    "best_test_loss_D = 9999\n",
    "loss_data = pd.DataFrame()\n",
    "\n",
    "checkpoint = False\n",
    "if checkpoint:\n",
    "    checkpoint_G = torch.load('./Model/test_save/test_save_2NetG.pth')\n",
    "    checkpoint_D = torch.load('./Model/test_save/test_save_2NetD.pth')\n",
    "    NetG.load_state_dict(checkpoint_G['model_state_dict'])\n",
    "    NetD.load_state_dict(checkpoint_D['model_state_dict'])\n",
    "    optimizer_G.load_state_dict(checkpoint_G['optimizer_state_dict'])\n",
    "    optimizer_D.load_state_dict(checkpoint_D['optimizer_state_dict'])\n",
    "    train_losses_FC.append(checkpoint_G['FC_loss'])\n",
    "    train_losses_G.append(checkpoint_G['G_loss'])\n",
    "    train_losses_D.append(checkpoint_G['D_loss'])\n",
    "    present_epoch = checkpoint_G['epoch'] + 1\n",
    "\n",
    "    \n",
    "\n",
    "funnyScoreLoss = nn.MSELoss()\n",
    "\n",
    "def generatorLoss(condition_logits, uncondition_logits):\n",
    "    result_fake = (torch.zeros(uncondition_logits.shape[0])).to(device)\n",
    "    con_loss = CrossEntropyLoss()(condition_logits, result_fake.to(torch.long))\n",
    "    unc_loss = BCEWithLogitsLoss()(uncondition_logits, result_fake)\n",
    "    loss = con_loss + unc_loss\n",
    "    return loss\n",
    "\n",
    "def discriminatorLoss(condition_logits, uncondition_logits):\n",
    "    result_true = (torch.ones(uncondition_logits[0].shape[0])).to(device)\n",
    "    result_fake = (torch.zeros(uncondition_logits[0].shape[0])).to(device)\n",
    "    \n",
    "    con_r2f = CrossEntropyLoss()(condition_logits[0], result_fake.to(torch.long))\n",
    "    con_f2r = CrossEntropyLoss()(condition_logits[1], result_fake.to(torch.long))\n",
    "    con_f = CrossEntropyLoss()(condition_logits[2], result_fake.to(torch.long))\n",
    "    con_m = CrossEntropyLoss()(condition_logits[3], result_fake.to(torch.long))\n",
    "    unc_r = BCEWithLogitsLoss()(uncondition_logits[0], result_true)\n",
    "    unc_f = BCEWithLogitsLoss()(uncondition_logits[1], result_fake)\n",
    "    unc_m = BCEWithLogitsLoss()(uncondition_logits[2], result_fake)\n",
    "    loss = ((con_r2f + con_f2r)/2) + ((con_f + con_m)/2) + unc_r + ((unc_f + unc_m)/2)\n",
    "    return loss"
   ],
   "id": "9dc4843819037573",
   "outputs": [],
   "execution_count": 184
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T03:53:12.874294Z",
     "start_time": "2024-10-14T03:50:08.473049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_name = 'test'\n",
    "if not os.path.exists('./Model/'+save_name):\n",
    "    os.makedirs('./Model/'+save_name)\n",
    "    \n",
    "epochs = 2\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(epochs):\n",
    "    print(\"---------------------------------------- epoch \"+ str(epoch + present_epoch) +\" ---------------------------------------\")\n",
    "    train_loss_FC = 0\n",
    "    train_loss_G = 0\n",
    "    train_loss_D = 0\n",
    "    test_loss_FC = 0\n",
    "    test_loss_G = 0\n",
    "    test_loss_D = 0\n",
    "    \n",
    "    ###################################### Train ######################################\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for text, image, funny_score in tepoch:\n",
    "            ######################################################\n",
    "                # (1) Generate fake caption\n",
    "            ######################################################\n",
    "            logits, output_funny_score = NetG(text.to(device).to(torch.float32), image.to(device).to(torch.float32))\n",
    "            g_con_logits, g_unc_logits, d_con_logits, d_unc_logits = NetD(text.to(device).to(torch.float32), logits.detach().to(torch.float32), image.to(device).to(torch.float32))\n",
    "            ######################################################\n",
    "                # (3) Update Discriminator network\n",
    "            #####################################################\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D = discriminatorLoss(d_con_logits, d_unc_logits)\n",
    "            loss_D.backward(retain_graph=True)\n",
    "            optimizer_D.step()\n",
    "            train_loss_D += loss_D.item()\n",
    "            ######################################################\n",
    "                # (4) Update Generator network\n",
    "            ######################################################\n",
    "            optimizer_G.zero_grad()\n",
    "            loss_FC = funnyScoreLoss(output_funny_score, funny_score.to(device).to(torch.float32))\n",
    "            loss_FC.backward(retain_graph=True)\n",
    "            train_loss_FC += loss_FC.item()\n",
    "            loss_G = generatorLoss(g_con_logits, g_unc_logits)\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            train_loss_G += loss_G.item()\n",
    "            ######################################################\n",
    "            tepoch.set_postfix({'FC_loss': loss_FC.item(), 'G_loss': loss_G.item(), 'D_loss': loss_D.item()})\n",
    "            ######################################################\n",
    "    train_loss_FC /= len(train_loader)\n",
    "    train_loss_G /= len(train_loader)\n",
    "    train_loss_D /= len(train_loader)\n",
    "    train_losses_FC.append(train_loss_FC)\n",
    "    train_losses_G.append(train_loss_G)\n",
    "    train_losses_D.append(train_loss_D)\n",
    "    ###################################### Train ######################################\n",
    "    \n",
    "    \n",
    "    ######################################  Test ######################################\n",
    "    with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
    "        for text, image, funny_score in tepoch:\n",
    "            # Generator\n",
    "            logits, output_funny_score = NetG(text.to(device).to(torch.float32), image.to(device).to(torch.float32))\n",
    "            # Discriminator\n",
    "            gen_logits, con_logits, unc_logits = NetD(text.to(device).to(torch.float32), logits.detach().to(torch.float32), image.to(device).to(torch.float32))\n",
    "            # loss\n",
    "            loss_FC = funnyScoreLoss(output_funny_score, funny_score.to(device).to(torch.float32))\n",
    "            loss_G = generatorLoss(gen_logits)\n",
    "            loss_D = discriminatorLoss(unc_logits, con_logits)\n",
    "            test_loss_FC += loss_FC.item()\n",
    "            test_loss_G += loss_G.item()\n",
    "            test_loss_D += loss_D.item()\n",
    "            tepoch.set_postfix({'FC_loss': test_loss_FC, 'G_loss': test_loss_G, 'D_loss': test_loss_D})\n",
    "    test_loss_FC /= len(test_loader)\n",
    "    test_loss_G /= len(test_loader)\n",
    "    test_loss_D /= len(test_loader)\n",
    "    test_losses_FC.append(test_loss_FC)\n",
    "    test_losses_G.append(test_loss_G)\n",
    "    test_losses_D.append(test_loss_D)\n",
    "    ######################################  Test ######################################\n",
    "\n",
    "    ######################################  Save ######################################\n",
    "    hasSaved = False\n",
    "    # 任一個loss小於最佳loss就存檔\n",
    "    if train_loss_FC < best_train_loss_FC and test_loss_FC < best_test_loss_FC:\n",
    "        best_train_loss_FC = train_loss_FC\n",
    "        best_test_loss_FC = test_loss_FC\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    if train_loss_G < best_train_loss_G and test_loss_G < best_test_loss_G:\n",
    "        best_train_loss_G = train_loss_G\n",
    "        best_test_loss_G = test_loss_G\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    if train_loss_D < best_train_loss_D and test_loss_D < best_test_loss_D:\n",
    "        best_train_loss_D = train_loss_D\n",
    "        best_test_loss_D = test_loss_D\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    \n",
    "    if hasSaved:\n",
    "        save.append(\"V\")\n",
    "    else:\n",
    "        save.append(\" \")\n",
    "\n",
    "    loss_data['train_FC'] = train_losses_FC\n",
    "    loss_data['train_G'] = train_losses_G\n",
    "    loss_data['train_D'] = train_losses_D\n",
    "    loss_data['test_FC'] = test_losses_FC\n",
    "    loss_data['test_G'] = test_losses_G\n",
    "    loss_data['test_D'] = test_losses_D\n",
    "    loss_data['save'] = save\n",
    "    loss_data.to_csv('./Model/' + save_name + \"/\" + save_name + '_loss.csv', index=False)\n",
    "    ######################################  Save ######################################"
   ],
   "id": "8418eb4f49f3759b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- epoch 1 ---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2]) torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [03:04<27:39, 184.37s/batch, FC_loss=1.37, G_loss=2.31, D_loss=2.44]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[185], line 29\u001B[0m\n\u001B[0;32m     27\u001B[0m optimizer_D\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     28\u001B[0m loss_D \u001B[38;5;241m=\u001B[39m discriminatorLoss(d_con_logits, d_unc_logits)\n\u001B[1;32m---> 29\u001B[0m \u001B[43mloss_D\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     30\u001B[0m optimizer_D\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     31\u001B[0m train_loss_D \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss_D\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    520\u001B[0m     )\n\u001B[1;32m--> 521\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 289\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    293\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    294\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    295\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    296\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    297\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    767\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    768\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 769\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    770\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    771\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    772\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    773\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 185
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T03:37:00.969770400Z",
     "start_time": "2024-10-14T03:31:39.296205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = torch.rand(32,2)\n",
    "target = torch.zeros(32).to(torch.long)\n",
    "print(a.shape, target.shape)\n",
    "CrossEntropyLoss()(a, target)"
   ],
   "id": "f4c2670afa9e936a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2]) torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6036)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T00:46:37.838929Z",
     "start_time": "2024-10-03T00:46:37.439345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses_FC, label='train')\n",
    "plt.plot(train_losses_G, label='train')\n",
    "plt.plot(train_losses_D, label='train')\n",
    "plt.plot(test_losses_FC, label='test')\n",
    "plt.plot(test_losses_G, label='test')\n",
    "plt.plot(test_losses_D, label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# save plot\n",
    "plt.savefig('./Model/' + save_name + \"/\" + save_name + '_loss.png')"
   ],
   "id": "65770e7128728d32",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA4m0lEQVR4nO3de3xU9Z3/8ffJZUJCmIRAyGWJJMi9RJCLGNCKggTwx8rFRZFlCVXo1uAu0lSlWhFQUEotyFp9rFaxv9W61YK11qKIXCpiRAp4AUPFpOiPJBApGYKQ6/f3B2TIJJMwk9ucgdfzwTyYOed7vudzvjM55z1nbpYxxggAAMBGQgJdAAAAQH0EFAAAYDsEFAAAYDsEFAAAYDsEFAAAYDsEFAAAYDsEFAAAYDsEFAAAYDthgS6gOWpqanTkyBF16tRJlmUFuhwAAOADY4xOnjyp5ORkhYQ0fY4kKAPKkSNHlJKSEugyAABAM3z99dfq3r17k22CMqB06tRJ0tkNdDqdAa4GAAD4wuVyKSUlxX0cb0pQBpTal3WcTicBBQCAIOPL2zN4kywAALAdAgoAALAdAgoAALCdoHwPCoCWMcaoqqpK1dXVgS4lKIWHhys0NDTQZQAXNQIKcImpqKhQYWGhvvvuu0CXErQsy1L37t0VHR0d6FKAi5ZfAWXFihVav369vvjiC0VGRmrkyJF6/PHH1bdvX3eb0aNHa9u2bR7L/fCHP9Qzzzzjvn348GH96Ec/0pYtWxQdHa3Zs2drxYoVCgsjLwFtqaamRvn5+QoNDVVycrIcDgdfdugnY4yOHTumb775Rr179+ZMCtBG/EoE27ZtU3Z2toYPH66qqir99Kc/1bhx47R//3517NjR3W7u3LlaunSp+3ZUVJT7enV1tW666SYlJibqgw8+UGFhof7t3/5N4eHhWr58eStsEoDGVFRUqKamRikpKR5/l/BPfHy8CgoKVFlZSUAB2ohfAWXjxo0et9etW6du3bpp9+7d+v73v++eHhUVpcTERK99vPPOO9q/f7/effddJSQkaPDgwVq2bJnuu+8+Pfzww3I4HM3YDAD+uNBXTKNpnHUC2l6L9lKlpaWSpLi4OI/pL730krp27aqBAwdq0aJFHq9179y5U+np6UpISHBPy8zMlMvl0ueff+51PeXl5XK5XB4XAABw8Wr2mz5qamq0YMECjRo1SgMHDnRPv/3229WjRw8lJyfrk08+0X333ae8vDytX79eklRUVOQRTiS5bxcVFXld14oVK7RkyZLmlgoAAIJMswNKdna2PvvsM73//vse0+fNm+e+np6erqSkJI0ZM0aHDh3S5Zdf3qx1LVq0SAsXLnTfrv0ufwBojtTUVC1YsEALFiwIdCkAGtGsgDJ//ny9+eab2r59+wV/jXDEiBGSpC+//FKXX365EhMT9dFHH3m0KS4ulqRG37cSERGhiIiI5pQK4CIxevRoDR48WKtXr25xX7t27fJ4Yz8A+/EroBhjdPfdd2vDhg3aunWr0tLSLrjM3r17JUlJSUmSpIyMDD366KM6evSounXrJknatGmTnE6nBgwY4Gf5rev/fbFfeR/+RZJk6dyb4M69Ge7sf/Wneb5Rzn27dr7XtnX787jSYD3n+6v9r4n1t3VN7uW9jUu9mr1N83E7vNZbd0WW1WDahbbDUp2amrEdDdp6raneNrZ3TfXftGlZXh8LlVXVqq6qUmVFhUIb3C8eW9c0q9EbTU9tMNGH9Z1rYoyRMTWqaeTL5Ywxqq6u9vy6AqvBFUlSly5dJJ19qbrxkhqvzRhz4boBtIhl/PhLu+uuu/Tyyy/rD3/4g8d3n8TExCgyMlKHDh3Syy+/rIkTJ6pLly765JNPdM8996h79+7u70aprq7W4MGDlZycrJUrV6qoqEizZs3SnXfe6fPHjF0ul2JiYlRaWtqqv2a8b9Of9e5zT7Vaf4DdRMV11ZAZc5ScmKDw0FAZY3SmKjAH2w5hDUNUY/7z3vv0u/UbPKatfvwxLbjvfv3Pr5/V40+s1hcHD+qVF55XclKSHl6+Qrv37tV3p0+r9+U99dOcH+v7o0a5lx1+3fWamzVb8+ZkSZKSevXRqkcf0btbt2rrX95XUkKCFi+6X5ljx3itp7K6WkeKivXX376g746XnJ/RIGz6GFbrzrB8DKuNtG0sSPv7RKD529GwbavX5M8TgdrZ/jwRqN2OC7RtznY0/cQyMPe3x/rrjG3KgHQN+P4Nak3+HL/9OoPy9NNPSzp7qrWuF154QVlZWXI4HHr33Xe1evVqnTp1SikpKZo2bZoefPBBd9vQ0FC9+eab+tGPfqSMjAx17NhRs2fP9vjelEDpltZTI6bcKunsDtud3YyRexdu6s1zTzYe8yVT56qpnVI7q14f5nyzBtPqLaM6NZ0vymvb1tgOU3ebfNiOxmtqfD3+1WTqNvWvpgZtmz+2Hn3UHSsft6N+29a4v32pKaJTJ1khIQoJDVVIaKhOV9bo+nV5CoStc3orMtxqcN95s+xnD+qr/AL17dNb9y74T0lS3t/+Jkla/vNVeuj++9UjJUUxMU4dKSzUDaOv0/0/vkcOh0Ovbnhds+f9u/6y6W11T05udB1PrP0vPXjfvXrovvv06//7f5X94xzt2rZFnWNjfd8oL49HzrUgWIWGhbV6QPGH3y/xNCUlJaXBt8h606NHD7311lv+rLpdJPXqq6RefS/cEAhSZ86cUX5+vrr8U4o6dOig7yqqJAUmoHRL66koh2+7oARj1NHpVJeERKWPyJAkfXu6XJL06IrHdPPNN6s2CvQz0g0T/4972auvH6NNW7dp595PlD3qWklnd7ydunRVt7Se7nZzfvAD/fDu/5AkDRx+lX794m+UX3RUfQcP8ajFyOjMmTNyVVbrtmU/V0R4+Nnp3gKpj09OPKdfKITXne/7kxP3VJ+fZPm/HXWfZPnyROBC29GwDy9PaNqtJu+hv3lP/Eyj9TZnO+o/eWn6Cc2Fn1DVXu+Wev7vIxD4bnngEhYZHqr9SzMDtm5fWXVORVv1TksPHz783PWzt8tOlenhhx/Wn/70JxUWFqqqqkqnT5/W119/7fEFdZZlKSTkfA2DBg9WyLlvhe3kdMrpdKrk22/d0+oKDQ1TSEiIIqM7qUOHDv5tOACfEFCAS5hlWT6fxbCr+p/GycnJ0aZNm7Rq1Sr16tVLkZGRuuWWW1RRUdFkP+HnzoTUsixLNTU1rV4vAN8E954JwCXD4XCoupFP8NS1Y8cOZWVlacqUKZKksrIyFRQUtHF1AFobP8gBICikpqYqNzdXBQUFKikpafTsRu/evbV+/Xrt3btX+/bt0+23386ZECAIEVAABIWcnByFhoZqwIABio+P1+HDh722e+KJJ9S5c2eNHDlSkyZNUmZmpoYMGeK1LQD78ut7UOyirb4HBbjY1X6KJy0tjTd3tgDjCDSPP8dvzqAAAADbIaAAAADbIaAAAADbIaAAAADbIaAAAADbIaAAAADbIaAAAADbIaAAAADbIaAAuOSkpqZq9erVgS4DQBP4sUAAQWH06NEaPHhwqwSLXbt2NfgVZAD2QkABcFEwxqi6ulphYRfercXHx7dDRQBagpd4ANheVlaWtm3bpjVr1siyLFmWpXXr1smyLP35z3/W0KFDFRERoffff1+HDh3SzTffrISEBEVHR2v48OF69913Pfqr/xKPZVl67rnnNGXKFEVFRal3795644032nkrAdRFQAEuZcZIFacCc/Hjd0rXrFmjjIwMzZ07V4WFhSosLFRKSook6f7779djjz2mAwcO6IorrlBZWZkmTpyozZs3a8+ePRo/frwmTZrU6K8f11qyZImmT5+uTz75RBMnTtTMmTN1/PjxFg0vgObjJR7gUlb5nbQ8OTDr/ukRyeHb+0BiYmLkcDgUFRWlxMRESdIXX3whSVq6dKluvPFGd9u4uDgNGjTIfXvZsmXasGGD3njjDc2fP7/RdWRlZWnGjBmSpOXLl+vJJ5/URx99pPHjx/u9aQBajjMoAILasGHDPG6XlZUpJydH/fv3V2xsrKKjo3XgwIELnkG54oor3Nc7duwop9Opo0ePtknNAC6MMyjApSw86uyZjECtuxXU/zROTk6ONm3apFWrVqlXr16KjIzULbfcooqKiqbLCQ/3uG1ZlmpqalqlRgD+I6AAlzLL8vlllkBzOByqrq6+YLsdO3YoKytLU6ZMkXT2jEpBQUEbVwegtfESD4CgkJqaqtzcXBUUFKikpKTRsxu9e/fW+vXrtXfvXu3bt0+33347Z0KAIERAARAUcnJyFBoaqgEDBig+Pr7R95Q88cQT6ty5s0aOHKlJkyYpMzNTQ4YMaedqAbSUZYwfn/WzCZfLpZiYGJWWlsrpdAa6HCBonDlzRvn5+UpLS1OHDh0CXU7QYhyB5vHn+M0ZFAAAYDsEFAAAYDsEFAAAYDsEFAAAYDsEFAAAYDsEFAAAYDsEFAAAYDsEFAAAYDsEFAAAYDsEFACXnNTUVK1evTrQZQBoAr9mDCAojB49WoMHD26VYLFr1y517Bgcv+IMXKoIKAAuCsYYVVdXKyzswru1+Pj4dqgIQEvwEg8A28vKytK2bdu0Zs0aWZYly7K0bt06WZalP//5zxo6dKgiIiL0/vvv69ChQ7r55puVkJCg6OhoDR8+XO+++65Hf/Vf4rEsS88995ymTJmiqKgo9e7dW2+88UY7byWAujiDAlzCjDE6XXU6IOuODIuUZVk+tV2zZo0OHjyogQMHaunSpZKkzz//XJJ0//33a9WqVerZs6c6d+6sr7/+WhMnTtSjjz6qiIgI/eY3v9GkSZOUl5enyy67rNF1LFmyRCtXrtTPf/5zrV27VjNnztTf//53xcXFtXxjAfiNgAJcwk5XndaIl0cEZN25t+cqKjzKp7YxMTFyOByKiopSYmKiJOmLL76QJC1dulQ33niju21cXJwGDRrkvr1s2TJt2LBBb7zxhubPn9/oOrKysjRjxgxJ0vLly/Xkk0/qo48+0vjx4/3eNgAtx0s8AILasGHDPG6XlZUpJydH/fv3V2xsrKKjo3XgwAEdPny4yX6uuOIK9/WOHTvK6XTq6NGjbVIzgAvjDApwCYsMi1Tu7bkBW3drqP9pnJycHG3atEmrVq1Sr169FBkZqVtuuUUVFRVN9hMeHu5x27Is1dTUtEqNAPxHQAEuYZZl+fwyS6A5HA5VV1dfsN2OHTuUlZWlKVOmSDp7RqWgoKCNqwPQ2niJB0BQSE1NVW5urgoKClRSUtLo2Y3evXtr/fr12rt3r/bt26fbb7+dMyFAECKgAAgKOTk5Cg0N1YABAxQfH9/oe0qeeOIJde7cWSNHjtSkSZOUmZmpIUOGtHO1AFrKMsaYQBfhL5fLpZiYGJWWlsrpdAa6HCBonDlzRvn5+UpLS1OHDh0CXU7QYhyB5vHn+M0ZFAAAYDsEFAAAYDsEFAAAYDsEFAAAYDsEFAAAYDsEFAAAYDsEFAAAYDsEFAAAYDsEFAAAYDsEFAAAYDt+BZQVK1Zo+PDh6tSpk7p166bJkycrLy/Po82ZM2eUnZ2tLl26KDo6WtOmTVNxcbFHm8OHD+umm25SVFSUunXrpp/85Ceqqqpq+dYAuGiNHj1aCxYsaLX+srKyNHny5FbrD0Dr8iugbNu2TdnZ2frwww+1adMmVVZWaty4cTp16pS7zT333KM//vGPevXVV7Vt2zYdOXJEU6dOdc+vrq7WTTfdpIqKCn3wwQd68cUXtW7dOj300EOtt1UAACC4mRY4evSokWS2bdtmjDHmxIkTJjw83Lz66qvuNgcOHDCSzM6dO40xxrz11lsmJCTEFBUVuds8/fTTxul0mvLycp/WW1paaiSZ0tLSlpQPXHJOnz5t9u/fb06fPh3oUvwye/ZsI8njkp+fbz799FMzfvx407FjR9OtWzfzr//6r+bYsWPu5V599VUzcOBA06FDBxMXF2fGjBljysrKzOLFixv0t2XLFp/rCdZxBALNn+N3i96DUlpaKkmKi4uTJO3evVuVlZUaO3asu02/fv102WWXaefOnZKknTt3Kj09XQkJCe42mZmZcrlc+vzzz72up7y8XC6Xy+MCoOWMMar57ruAXIwfP6S+Zs0aZWRkaO7cuSosLFRhYaE6deqkG264QVdeeaU+/vhjbdy4UcXFxZo+fbokqbCwUDNmzNAPfvADHThwQFu3btXUqVNljFFOTo6mT5+u8ePHu/sbOXJkWw0zgGYIa+6CNTU1WrBggUaNGqWBAwdKkoqKiuRwOBQbG+vRNiEhQUVFRe42dcNJ7fzaed6sWLFCS5YsaW6pABphTp9W3pChAVl337/ulhUV5VPbmJgYORwORUVFKTExUZL0yCOP6Morr9Ty5cvd7Z5//nmlpKTo4MGDKisrU1VVlaZOnaoePXpIktLT091tIyMjVV5e7u4PgL00+wxKdna2PvvsM73yyiutWY9XixYtUmlpqfvy9ddft/k6Adjbvn37tGXLFkVHR7sv/fr1kyQdOnRIgwYN0pgxY5Senq5/+Zd/0bPPPqt//OMfAa4agK+adQZl/vz5evPNN7V9+3Z1797dPT0xMVEVFRU6ceKEx1mU4uJi97OUxMREffTRRx791X7Kp7FnMhEREYqIiGhOqQCaYEVGqu9fdwds3S1RVlamSZMm6fHHH28wLykpSaGhodq0aZM++OADvfPOO1q7dq0eeOAB5ebmKi0trUXrBtD2/DqDYozR/PnztWHDBr333nsN/siHDh2q8PBwbd682T0tLy9Phw8fVkZGhiQpIyNDn376qY4ePepus2nTJjmdTg0YMKAl2wLAT5ZlKSQqKiAXy7L8qtXhcKi6utp9e8iQIfr888+VmpqqXr16eVw6duzo3r5Ro0ZpyZIl2rNnjxwOhzZs2OC1PwD24ldAyc7O1v/8z//o5ZdfVqdOnVRUVKSioiKdPn1a0tnXie+44w4tXLhQW7Zs0e7duzVnzhxlZGTo6quvliSNGzdOAwYM0KxZs7Rv3z69/fbbevDBB5Wdnc1ZEgCNSk1NVW5urgoKClRSUqLs7GwdP35cM2bM0K5du3To0CG9/fbbmjNnjqqrq5Wbm6vly5fr448/1uHDh7V+/XodO3ZM/fv3d/f3ySefKC8vTyUlJaqsrAzwFgLw4M/Hg1TvY3m1lxdeeMHd5vTp0+auu+4ynTt3NlFRUWbKlCmmsLDQo5+CggIzYcIEExkZabp27Wp+/OMfm8rKSp/r4GPGQPME88dj8/LyzNVXX20iIyPdHzM+ePCgmTJliomNjTWRkZGmX79+ZsGCBaampsbs37/fZGZmmvj4eBMREWH69Olj1q5d6+7v6NGj5sYbbzTR0dF8zBhoJ/4cvy1j/Pisn024XC7FxMSotLRUTqcz0OUAQePMmTPKz89XWlqaOnToEOhyghbjCDSPP8dvfosHAADYDgEFAADYDgEFAADYDgEFAADYDgEFAADYDgEFAADYDgEFAADYDgEFAADYDgEFAADYDgEFAADYDgEFQFAYPXq0FixY0Gr9ZWVlafLkya3WH4DWRUABAAC2Q0ABYHtZWVnatm2b1qxZI8uyZFmWCgoK9Nlnn2nChAmKjo5WQkKCZs2apZKSEvdyr732mtLT0xUZGakuXbpo7NixOnXqlB5++GG9+OKL+sMf/uDub+vWrYHbQAANhAW6AACBY4xRVUVNQNYd5giRZVk+tV2zZo0OHjyogQMHaunSpZKk8PBwXXXVVbrzzjv1y1/+UqdPn9Z9992n6dOn67333lNhYaFmzJihlStXasqUKTp58qT+8pe/yBijnJwcHThwQC6XSy+88IIkKS4urs22FYD/CCjAJayqokb//Z/bArLueWuuU3hEqE9tY2Ji5HA4FBUVpcTEREnSI488oiuvvFLLly93t3v++eeVkpKigwcPqqysTFVVVZo6dap69OghSUpPT3e3jYyMVHl5ubs/APZCQAEQlPbt26ctW7YoOjq6wbxDhw5p3LhxGjNmjNLT05WZmalx48bplltuUefOnQNQLQB/EVCAS1iYI0Tz1lwXsHW3RFlZmSZNmqTHH3+8wbykpCSFhoZq06ZN+uCDD/TOO+9o7dq1euCBB5Sbm6u0tLQWrRtA2yOgAJcwy7J8fpkl0BwOh6qrq923hwwZot///vdKTU1VWJj3XZllWRo1apRGjRqlhx56SD169NCGDRu0cOHCBv0BsBc+xQMgKKSmpio3N1cFBQUqKSlRdna2jh8/rhkzZmjXrl06dOiQ3n77bc2ZM0fV1dXKzc3V8uXL9fHHH+vw4cNav369jh07pv79+7v7++STT5SXl6eSkhJVVlYGeAsB1EVAARAUcnJyFBoaqgEDBig+Pl4VFRXasWOHqqurNW7cOKWnp2vBggWKjY1VSEiInE6ntm/frokTJ6pPnz568MEH9Ytf/EITJkyQJM2dO1d9+/bVsGHDFB8frx07dgR4CwHUZRljTKCL8JfL5VJMTIxKS0vldDoDXQ4QNM6cOaP8/HylpaWpQ4cOgS4naDGOQPP4c/zmDAoAALAdAgoAALAdAgoAALAdAgoAALAdAgoAALAdAgoAALAdAgoAALAdAgoAALAdAgoAALAdAgoAALAdAgqAoDB69GgtWLCg1frLysrS5MmTW60/AK2LgAIAAGyHgALA9rKysrRt2zatWbNGlmXJsiwVFBTos88+04QJExQdHa2EhATNmjVLJSUl7uVee+01paenKzIyUl26dNHYsWN16tQpPfzww3rxxRf1hz/8wd3f1q1bA7eBABoIC3QBAALHGKOq8vKArDssIkKWZfnUds2aNTp48KAGDhyopUuXSpLCw8N11VVX6c4779Qvf/lLnT59Wvfdd5+mT5+u9957T4WFhZoxY4ZWrlypKVOm6OTJk/rLX/4iY4xycnJ04MABuVwuvfDCC5KkuLi4NttWAP4joACXsKrycj05+5aArPs/XnxN4R06+NQ2JiZGDodDUVFRSkxMlCQ98sgjuvLKK7V8+XJ3u+eff14pKSk6ePCgysrKVFVVpalTp6pHjx6SpPT0dHfbyMhIlZeXu/sDYC8EFABBad++fdqyZYuio6MbzDt06JDGjRunMWPGKD09XZmZmRo3bpxuueUWde7cOQDVAvAXAQW4hIVFROg/XnwtYOtuibKyMk2aNEmPP/54g3lJSUkKDQ3Vpk2b9MEHH+idd97R2rVr9cADDyg3N1dpaWktWjeAtkdAAS5hlmX5/DJLoDkcDlVXV7tvDxkyRL///e+VmpqqsDDvuzLLsjRq1CiNGjVKDz30kHr06KENGzZo4cKFDfoDYC98igdAUEhNTVVubq4KCgpUUlKi7OxsHT9+XDNmzNCuXbt06NAhvf3225ozZ46qq6uVm5ur5cuX6+OPP9bhw4e1fv16HTt2TP3793f398knnygvL08lJSWqrKwM8BYCqIuAAiAo5OTkKDQ0VAMGDFB8fLwqKiq0Y8cOVVdXa9y4cUpPT9eCBQsUGxurkJAQOZ1Obd++XRMnTlSfPn304IMP6he/+IUmTJggSZo7d6769u2rYcOGKT4+Xjt27AjwFgKoyzLGmEAX4S+Xy6WYmBiVlpbK6XQGuhwgaJw5c0b5+flKS0tThyB5aceOGEegefw5fnMGBQAA2A4BBQAA2A4BBQAA2A4BBQAA2A4BBQAA2A4BBbgE1dTUBLqEoBaEH34Egg7fJAtcQhwOh0JCQnTkyBHFx8fL4XD4/IvCOMsYo2PHjp39Ft7w8ECXA1y0CCjAJSQkJERpaWkqLCzUkSNHAl1O0LIsS927d1doaGigSwEuWgQU4BLjcDh02WWXqaqqit+iaabw8HDCCdDGCCjAJaj25QleogBgV7xJFgAA2A4BBQAA2A4BBQAA2A4BBQAA2A4BBQAA2I7fAWX79u2aNGmSkpOTZVmWXn/9dY/5WVlZsizL4zJ+/HiPNsePH9fMmTPldDoVGxurO+64Q2VlZS3aEAAAcPHwO6CcOnVKgwYN0lNPPdVom/Hjx6uwsNB9+e1vf+sxf+bMmfr888+1adMmvfnmm9q+fbvmzZvnf/UAAOCi5Pf3oEyYMEETJkxosk1ERIQSExO9zjtw4IA2btyoXbt2adiwYZKktWvXauLEiVq1apWSk5P9LQkAAFxk2uQ9KFu3blW3bt3Ut29f/ehHP9K3337rnrdz507Fxsa6w4kkjR07ViEhIcrNzfXaX3l5uVwul8cFAABcvFo9oIwfP16/+c1vtHnzZj3++OPatm2bJkyY4P5K7aKiInXr1s1jmbCwMMXFxamoqMhrnytWrFBMTIz7kpKS0tplAwAAG2n1r7q/7bbb3NfT09N1xRVX6PLLL9fWrVs1ZsyYZvW5aNEiLVy40H3b5XIRUgAAuIi1+ceMe/bsqa5du+rLL7+UJCUmJuro0aMebaqqqnT8+PFG37cSEREhp9PpcQEAABevNg8o33zzjb799lslJSVJkjIyMnTixAnt3r3b3ea9995TTU2NRowY0dblAACAIOD3SzxlZWXusyGSlJ+fr7179youLk5xcXFasmSJpk2bpsTERB06dEj33nuvevXqpczMTElS//79NX78eM2dO1fPPPOMKisrNX/+fN122218ggcAAEiSLGOM8WeBrVu36vrrr28wffbs2Xr66ac1efJk7dmzRydOnFBycrLGjRunZcuWKSEhwd32+PHjmj9/vv74xz8qJCRE06ZN05NPPqno6GifanC5XIqJiVFpaSkv9wAAECT8OX77HVDsgIACAEDw8ef4zW/xAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2yGgAAAA2/E7oGzfvl2TJk1ScnKyLMvS66+/7jHfGKOHHnpISUlJioyM1NixY/W3v/3No83x48c1c+ZMOZ1OxcbG6o477lBZWVmLNgQAAFw8wvxd4NSpUxo0aJB+8IMfaOrUqQ3mr1y5Uk8++aRefPFFpaWl6Wc/+5kyMzO1f/9+dejQQZI0c+ZMFRYWatOmTaqsrNScOXM0b948vfzyyy3fohYoqyjTP8784/wEq+7V8zcsy/I+vZE2dfnUT0uW9aWfRpb1dR2+9OVLfY2ObwvGCABwcbCMMabZC1uWNmzYoMmTJ0s6e/YkOTlZP/7xj5WTkyNJKi0tVUJCgtatW6fbbrtNBw4c0IABA7Rr1y4NGzZMkrRx40ZNnDhR33zzjZKTky+4XpfLpZiYGJWWlsrpdDa3/AZePfiqlu5c2mr9of3ZISS1Wg1tHYTbsYYm+2pJEG5BIG/rGlrrceZTDW38WG/R30k7PCmx2+M9YI/1Vq5hUPwgZaZmem3TXP4cv/0+g9KU/Px8FRUVaezYse5pMTExGjFihHbu3KnbbrtNO3fuVGxsrDucSNLYsWMVEhKi3NxcTZkypTVL8kuYFaaO4R0lnQ1btYy8Z7jG2jQ6vW4/Hlf96weNa2z8mlgAAODFmT5nWj2g+KNVA0pRUZEkKSEhwWN6QkKCe15RUZG6devmWURYmOLi4txt6isvL1d5ebn7tsvlas2y3ab0nqIpvQMXkJqjtUJSU+18WUej/fhQR6P9+LlsawXJVquhBSHUp35aMrbtWEOL67Dx/eRTDW38hMUWj5UA7Qt8XYcvfbX546wFNbTaY8XP+ym9a7rX9u2lVQNKW1mxYoWWLFkS6DJsyZdTsgAABJtW/ZhxYmKiJKm4uNhjenFxsXteYmKijh496jG/qqpKx48fd7epb9GiRSotLXVfvv7669YsGwAA2EyrBpS0tDQlJiZq8+bN7mkul0u5ubnKyMiQJGVkZOjEiRPavXu3u817772nmpoajRgxwmu/ERERcjqdHhcAAHDx8vslnrKyMn355Zfu2/n5+dq7d6/i4uJ02WWXacGCBXrkkUfUu3dv98eMk5OT3Z/06d+/v8aPH6+5c+fqmWeeUWVlpebPn6/bbrvNp0/wAACAi5/fAeXjjz/W9ddf7769cOFCSdLs2bO1bt063XvvvTp16pTmzZunEydO6JprrtHGjRvd34EiSS+99JLmz5+vMWPGKCQkRNOmTdOTTz7ZCpsDAAAuBi36HpRAaavvQQEAAG3Hn+M3v8UDAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsh4ACAABsp9UDysMPPyzLsjwu/fr1c88/c+aMsrOz1aVLF0VHR2vatGkqLi5u7TIAAEAQa5MzKN/73vdUWFjovrz//vvueffcc4/++Mc/6tVXX9W2bdt05MgRTZ06tS3KAAAAQSqsTToNC1NiYmKD6aWlpfr1r3+tl19+WTfccIMk6YUXXlD//v314Ycf6uqrr26LcgAAQJBpkzMof/vb35ScnKyePXtq5syZOnz4sCRp9+7dqqys1NixY91t+/Xrp8suu0w7d+5stL/y8nK5XC6PCwAAuHi1ekAZMWKE1q1bp40bN+rpp59Wfn6+rr32Wp08eVJFRUVyOByKjY31WCYhIUFFRUWN9rlixQrFxMS4LykpKa1dNgAAsJFWf4lnwoQJ7utXXHGFRowYoR49euh3v/udIiMjm9XnokWLtHDhQvdtl8tFSAEA4CLW5h8zjo2NVZ8+ffTll18qMTFRFRUVOnHihEeb4uJir+9ZqRURESGn0+lxAQAAF682DyhlZWU6dOiQkpKSNHToUIWHh2vz5s3u+Xl5eTp8+LAyMjLauhQAABAkWv0lnpycHE2aNEk9evTQkSNHtHjxYoWGhmrGjBmKiYnRHXfcoYULFyouLk5Op1N33323MjIy+AQPAABwa/WA8s0332jGjBn69ttvFR8fr2uuuUYffvih4uPjJUm//OUvFRISomnTpqm8vFyZmZn61a9+1dplAACAIGYZY0ygi/CXy+VSTEyMSktLeT8KAABBwp/jN7/FAwAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbIeAAgAAbCcs0AUAAPxnjJHO/jt/3RgZI/d1b9NM7TxJpqbudePu19ScX4fx6MuzX299mLMLnmtbr72Xae7aZDz7ulB79/Xz61L9PmrO9ttoey/bV7cPI0nn+mhsvd76qN0Wz/G8QO3G1Bnv83V466P+ej3u89r7wajhfeIxHk08bs4t139ksq6Z3rt5D9BWQEABmqHRHabO7ehrDxx1dvqN7RR82RHX9uH1QNPogaDODrZubTrX3ofa5F7Oc8dV20djO0nfD2R+HDhqp9XWfK6PC7b3VmfteHocOJpef4MDk5c+vI9JEwcmNb6uJgPHubED2lJVZXVA109AqaP8dJVOuyouvPNXYzvb2oTq7WDRxEGt3kHIds8avBwIvO5UaxN/nYPa+Z1/U7W0/7MGbweOun2cHYPa7TbnxuP89gJBzZIsSZZ19oplWbIsua97m3b2unVuOckKsc525W7vbVnr3PS612uX8+zD67q8LVu/D5/a15kmSSHW2eXkQ3tv81Wv9nNvlqi//Q3G5txy56426MOS9231uG+89OF5X9Ybp0buy/N91R1Pzz4ckYGNCASUOv62q1jbXs4LdBm4yF14h1hnZyLPnV/ddt535vV2XGf3eI324fuO0PsO9IIHNenswaB2uxvsTBvpQ2d3nO4DontHLp8OjHV3vpJVbzlv7b2Na50d+LmjUu3BoMkDuEd/TdyXdWrz/aDWsA93bU3dl17GBrA7AkodYeEhiog6OyRNPyuot0OSbzvQBjuw2p1+IztTz4NB3et1Dmpe+rC87PTUYP11apMlnTv4NLXjbLKWejvOxnb+jab82tprr1+ovbf7pnYMG9wnvt2XTR/Umj6QNDmuludBDQBwYQSUOvplJKlfRlKgywAA4JLHx4wBAIDtEFAAAIDtEFAAAIDtEFAAAIDtEFAAAIDt8Cmeus64pNPH5f62L0nub+UyXr6dyz2tfhvjQ5smlvOljdfl5EObtly/PzUGav3Bet/6U2Nbrj/I79uArT9Y/2693Q70fetLmwDct748/tt0/d7a1Lvtb9+DbpNuXKJAIaDU9dlr0pv3BLoKAAACr9wV0NUHNKA89dRT+vnPf66ioiINGjRIa9eu1VVXXRW4gkLCpPCoczfqfKGW+8u1rHq3606rf9tbm3r9NNm3v+uv30YXbtOm6/dhjNp9/U317e12a/Xd3G3zpQ2PrYbTArn+pvr2KK6V+27pttVfpr3Xf7Hct4Fafxv93UbFKZACFlD+93//VwsXLtQzzzyjESNGaPXq1crMzFReXp66desWkJoKYsfoYPrvJElWnTvMsiRjnb8Lrbozzt2y6j4G6v1B1U6zvD6WQhq2ldzfYOoxzcuDyf0tpvUa1/3GUo+lzn7dqefyddp7LFWvXve8kJC6reot7zke3h73577ntWE7y2q4vNfazjeoX5t1rraGf4te+vZWW53xsTzmNV2bPLbt7O0GtVkNa7PqbXfdlTYYd6vOb4nU2Tav41mntvPTzl4JCfHo4dw07+Nm1d1Gj6K98Ge6H20badlm62usLd8CDLQvyxhvL+61vREjRmj48OH6r//6L0lSTU2NUlJSdPfdd+v+++9vclmXy6WYmBiVlpbK6XS2Wk0bH3taPdY92Wr9Abg01TQWq7xMNo1HMK+M16DkvQ/jdXJjbX2vozlt6y/hdbsb6bbRMfInNHpp26Bfb2cZLjDJeDydrTO9qdoazPJ3++o/e/Gubg11m5p6y1vu6Z4tK0eP1cgVP2t6JX7y5/gdkDMoFRUV2r17txYtWuSeFhISorFjx2rnzp0N2peXl6u8vNx92+Vqm9fFOjmjdbJjTL03O0leJpy9C31od35WI314W76RbiyZBrOsRlfpfYblRx61zv0CsC99N/p30sj6vLZvtG1r9NEIn+6Xs0L4GWP4qNHHio9/T03iYYh2cvD/HQ3o+gMSUEpKSlRdXa2EhASP6QkJCfriiy8atF+xYoWWLFnS5nWNumuWdNesNl8PLn71T0zW3jY156ebuvOMZ/g0RuemmfO3PWaen+atjbvvmpp666pzxdvytX171G0aLl93Gxt8YMA0rK3Gs1+Pvr19QKOmfh3yMka1bbyMkUy9Ojz79axXDeqoX3edTaszRvXvB2+1yaOOurXVH7fatt769Kzb85MWDT6o4eVx01Rt3j7pYRqp42zp9ddb71MhtZNMI4+beht+fowaPhDqP44kSTXeazOmpsHyMudqrl9b3Sl111H/8V9vnZ61Gc//z023jJfa6vTtUVsjf1uN/Y17/Xup17a2a8vL32GD9bgnePl7k9SzV3cFUlB8imfRokVauHCh+7bL5VJKSkoAKwKaVv/9Cu7bfPMQAPgkIAGla9euCg0NVXFxscf04uJiJSYmNmgfERGhiIiI9ioPAAAEWECezzkcDg0dOlSbN292T6upqdHmzZuVkZERiJIAAICNBOwlnoULF2r27NkaNmyYrrrqKq1evVqnTp3SnDlzAlUSAACwiYAFlFtvvVXHjh3TQw89pKKiIg0ePFgbN25s8MZZAABw6QnY96C0RFt9DwoAAGg7/hy/+UwBAACwHQIKAACwHQIKAACwHQIKAACwHQIKAACwHQIKAACwHQIKAACwHQIKAACwnaD4NeP6ar9bzuVyBbgSAADgq9rjti/fERuUAeXkyZOSpJSUlABXAgAA/HXy5EnFxMQ02SYov+q+pqZGR44cUadOnWRZVqv27XK5lJKSoq+//pqv0W9DjHP7YJzbB+PcPhjn9tNWY22M0cmTJ5WcnKyQkKbfZRKUZ1BCQkLUvXv3Nl2H0+nkD6AdMM7tg3FuH4xz+2Cc209bjPWFzpzU4k2yAADAdggoAADAdggo9URERGjx4sWKiIgIdCkXNca5fTDO7YNxbh+Mc/uxw1gH5ZtkAQDAxY0zKAAAwHYIKAAAwHYIKAAAwHYIKAAAwHYuyYDy1FNPKTU1VR06dNCIESP00UcfNdn+1VdfVb9+/dShQwelp6frrbfeaqdKg5s/4/zss8/q2muvVefOndW5c2eNHTv2gvcLzvL38VzrlVdekWVZmjx5ctsWeJHwd5xPnDih7OxsJSUlKSIiQn369GHf4QN/x3n16tXq27evIiMjlZKSonvuuUdnzpxpp2qD0/bt2zVp0iQlJyfLsiy9/vrrF1xm69atGjJkiCIiItSrVy+tW7euzeuUucS88sorxuFwmOeff958/vnnZu7cuSY2NtYUFxd7bb9jxw4TGhpqVq5cafbv328efPBBEx4ebj799NN2rjy4+DvOt99+u3nqqafMnj17zIEDB0xWVpaJiYkx33zzTTtXHlz8Heda+fn55p/+6Z/Mtddea26++eb2KTaI+TvO5eXlZtiwYWbixInm/fffN/n5+Wbr1q1m79697Vx5cPF3nF966SUTERFhXnrpJZOfn2/efvttk5SUZO655552rjy4vPXWW+aBBx4w69evN5LMhg0bmmz/1VdfmaioKLNw4UKzf/9+s3btWhMaGmo2btzYpnVecgHlqquuMtnZ2e7b1dXVJjk52axYscJr++nTp5ubbrrJY9qIESPMD3/4wzatM9j5O871VVVVmU6dOpkXX3yxrUq8KDRnnKuqqszIkSPNc889Z2bPnk1A8YG/4/z000+bnj17moqKivYq8aLg7zhnZ2ebG264wWPawoULzahRo9q0zouJLwHl3nvvNd/73vc8pt16660mMzOzDSsz5pJ6iaeiokK7d+/W2LFj3dNCQkI0duxY7dy50+syO3fu9GgvSZmZmY22R/PGub7vvvtOlZWViouLa6syg15zx3np0qXq1q2b7rjjjvYoM+g1Z5zfeOMNZWRkKDs7WwkJCRo4cKCWL1+u6urq9io76DRnnEeOHKndu3e7Xwb66quv9NZbb2nixIntUvOlIlDHwaD8scDmKikpUXV1tRISEjymJyQk6IsvvvC6TFFRkdf2RUVFbVZnsGvOONd33333KTk5ucEfBc5rzji///77+vWvf629e/e2Q4UXh+aM81dffaX33ntPM2fO1FtvvaUvv/xSd911lyorK7V48eL2KDvoNGecb7/9dpWUlOiaa66RMUZVVVX693//d/30pz9tj5IvGY0dB10ul06fPq3IyMg2We8ldQYFweGxxx7TK6+8og0bNqhDhw6BLueicfLkSc2aNUvPPvusunbtGuhyLmo1NTXq1q2b/vu//1tDhw7VrbfeqgceeEDPPPNMoEu7qGzdulXLly/Xr371K/31r3/V+vXr9ac//UnLli0LdGloBZfUGZSuXbsqNDRUxcXFHtOLi4uVmJjodZnExES/2qN541xr1apVeuyxx/Tuu+/qiiuuaMsyg56/43zo0CEVFBRo0qRJ7mk1NTWSpLCwMOXl5enyyy9v26KDUHMez0lJSQoPD1doaKh7Wv/+/VVUVKSKigo5HI42rTkYNWecf/azn2nWrFm68847JUnp6ek6deqU5s2bpwceeEAhITwHbw2NHQedTmebnT2RLrEzKA6HQ0OHDtXmzZvd02pqarR582ZlZGR4XSYjI8OjvSRt2rSp0fZo3jhL0sqVK7Vs2TJt3LhRw4YNa49Sg5q/49yvXz99+umn2rt3r/vyz//8z7r++uu1d+9epaSktGf5QaM5j+dRo0bpyy+/dAdASTp48KCSkpIIJ41ozjh/9913DUJIbSg0/MxcqwnYcbBN34JrQ6+88oqJiIgw69atM/v37zfz5s0zsbGxpqioyBhjzKxZs8z999/vbr9jxw4TFhZmVq1aZQ4cOGAWL17Mx4x94O84P/bYY8bhcJjXXnvNFBYWui8nT54M1CYEBX/HuT4+xeMbf8f58OHDplOnTmb+/PkmLy/PvPnmm6Zbt27mkUceCdQmBAV/x3nx4sWmU6dO5re//a356quvzDvvvGMuv/xyM3369EBtQlA4efKk2bNnj9mzZ4+RZJ544gmzZ88e8/e//90YY8z9999vZs2a5W5f+zHjn/zkJ+bAgQPmqaee4mPGbWXt2rXmsssuMw6Hw1x11VXmww8/dM+77rrrzOzZsz3a/+53vzN9+vQxDofDfO973zN/+tOf2rni4OTPOPfo0cNIanBZvHhx+xceZPx9PNdFQPGdv+P8wQcfmBEjRpiIiAjTs2dP8+ijj5qqqqp2rjr4+DPOlZWV5uGHHzaXX3656dChg0lJSTF33XWX+cc//tH+hQeRLVu2eN3f1o7t7NmzzXXXXddgmcGDBxuHw2F69uxpXnjhhTav0zKG82AAAMBeLqn3oAAAgOBAQAEAALZDQAEAALZDQAEAALZDQAEAALZDQAEAALZDQAEAALZDQAEAALZDQAEAALZDQAEAALZDQAEAALZDQAEAALbz/wH4l1SwPJlj5wAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T00:46:43.241584Z",
     "start_time": "2024-10-03T00:46:43.228904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# to csv\n",
    "train_losses = pd.DataFrame({'train_FC': train_losses_FC, 'train_G': train_losses_G, 'train_D': train_losses_D})\n",
    "test_losses = pd.DataFrame({'test_FC': test_losses_FC, 'test_G': test_losses_G, 'test_D': test_losses_D})\n",
    "train_losses.to_csv('./Model/' + save_name + \"/\" + save_name + '_train_losses.csv', index=False)\n",
    "test_losses.to_csv('./Model/' + save_name + \"/\" + save_name + '_test_losses.csv', index=False)"
   ],
   "id": "5cdd08c9e69263de",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate Test",
   "id": "4c44d1e70ced399f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T22:01:28.366816Z",
     "start_time": "2024-10-02T22:01:28.363077Z"
    }
   },
   "cell_type": "code",
   "source": "test_image[0].shape",
   "id": "b4e0d61dc470c2e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 768])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:43:27.969750Z",
     "start_time": "2024-10-02T23:43:21.089352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#load model\n",
    "NetG = Generator().to(device)\n",
    "NetD = Discriminator().to(device)\n",
    "NetG.load_state_dict(torch.load('./Model/test_batch32/test_batch32_5NetG.pth'))\n",
    "NetD.load_state_dict(torch.load('./Model/test_batch32/test_batch32_5NetD.pth'))\n",
    "# train with load model\n",
    "NetG.train()\n",
    "NetD.train()\n"
   ],
   "id": "c3f72adc27a5f410",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15460\\3508553010.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  NetG.load_state_dict(torch.load('./Model/test_batch32/test_batch32_5NetG.pth'))\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_15460\\3508553010.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  NetD.load_state_dict(torch.load('./Model/test_batch32/test_batch32_5NetD.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T23:47:40.015314Z",
     "start_time": "2024-10-02T23:43:27.978751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generate\n",
    "NetG.eval()\n",
    "NetD.eval()\n",
    "image = test_image[0].unsqueeze(0).to(device)\n",
    "output = NetG.generate(image, 100)\n",
    "output"
   ],
   "id": "aa601343f448bd19",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 991.56it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 998.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.2329,  0.4325,  0.4199,  ..., -0.4702, -0.1142, -0.0098]],\n",
       " \n",
       "         [[ 0.1089,  0.3625,  0.8224,  ...,  0.2763,  0.0975, -0.0043]],\n",
       " \n",
       "         [[-0.1282, -0.1454, -0.1592,  ...,  0.1126,  0.7090,  0.6611]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.4177,  0.1868,  0.0531,  ..., -0.5458,  0.0338, -1.4983]],\n",
       " \n",
       "         [[-0.1912,  0.1032,  0.4763,  ...,  0.7547,  0.7066, -0.5460]],\n",
       " \n",
       "         [[-0.1003, -0.3331, -0.0245,  ..., -0.5132,  0.0633,  0.8948]]],\n",
       "        device='cuda:0'),\n",
       " [2,\n",
       "  540,\n",
       "  235248,\n",
       "  236193,\n",
       "  235248,\n",
       "  235248,\n",
       "  235250,\n",
       "  235274,\n",
       "  35351,\n",
       "  235254,\n",
       "  605,\n",
       "  6935,\n",
       "  235276,\n",
       "  235248,\n",
       "  235248,\n",
       "  235248,\n",
       "  132385,\n",
       "  235248,\n",
       "  235265,\n",
       "  235248,\n",
       "  2173,\n",
       "  235274,\n",
       "  235248,\n",
       "  2465,\n",
       "  3682,\n",
       "  236193,\n",
       "  18824,\n",
       "  235274,\n",
       "  235248,\n",
       "  11200,\n",
       "  235276,\n",
       "  235276,\n",
       "  616,\n",
       "  235248,\n",
       "  235248,\n",
       "  235248,\n",
       "  235274,\n",
       "  235248,\n",
       "  235248,\n",
       "  2012,\n",
       "  235276,\n",
       "  2012,\n",
       "  236193,\n",
       "  235248,\n",
       "  235248,\n",
       "  235248,\n",
       "  235265,\n",
       "  618,\n",
       "  235276,\n",
       "  669,\n",
       "  235248,\n",
       "  235248,\n",
       "  24255,\n",
       "  618,\n",
       "  14383,\n",
       "  236193,\n",
       "  235248,\n",
       "  235362,\n",
       "  236193,\n",
       "  5862,\n",
       "  1420,\n",
       "  236193,\n",
       "  235248,\n",
       "  235248,\n",
       "  46816,\n",
       "  235248,\n",
       "  235248,\n",
       "  1820,\n",
       "  235248,\n",
       "  235276,\n",
       "  235265,\n",
       "  690,\n",
       "  235256,\n",
       "  235274,\n",
       "  236193,\n",
       "  235256,\n",
       "  235248,\n",
       "  235248,\n",
       "  235248,\n",
       "  236193,\n",
       "  819,\n",
       "  235248,\n",
       "  235248,\n",
       "  235254,\n",
       "  235265,\n",
       "  84521,\n",
       "  3550,\n",
       "  235276,\n",
       "  42113,\n",
       "  235265,\n",
       "  235345,\n",
       "  509,\n",
       "  235248,\n",
       "  235276,\n",
       "  4943,\n",
       "  235248,\n",
       "  235248,\n",
       "  236193,\n",
       "  53152,\n",
       "  235269,\n",
       "  235265],\n",
       " tensor([-0.4983], device='cuda:0'))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. Adapter",
   "id": "2c0340b7c2e49bfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NetG = Generator().to(device)\n",
    "NetD = Discriminator().to(device)\n",
    "optimizer_G = optim.Adam(NetG.parameters(), lr=0.001)\n",
    "optimizer_D = optim.Adam(NetD.parameters(), lr=0.001)\n",
    "train_losses_FC = []\n",
    "train_losses_G = []\n",
    "train_losses_D = []\n",
    "test_losses_FC = []\n",
    "test_losses_G = []\n",
    "test_losses_D = []\n",
    "save = []\n",
    "present_epoch = 1\n",
    "best_train_loss_FC = 9999\n",
    "best_train_loss_G = 9999\n",
    "best_train_loss_D = 9999\n",
    "best_test_loss_FC = 9999\n",
    "best_test_loss_G = 9999\n",
    "best_test_loss_D = 9999\n",
    "loss_data = pd.DataFrame()\n",
    "\n",
    "checkpoint = False\n",
    "if checkpoint:\n",
    "    checkpoint_G = torch.load('./Model/test_save/test_save_2NetG.pth')\n",
    "    checkpoint_D = torch.load('./Model/test_save/test_save_2NetD.pth')\n",
    "    NetG.load_state_dict(checkpoint_G['model_state_dict'])\n",
    "    NetD.load_state_dict(checkpoint_D['model_state_dict'])\n",
    "    optimizer_G.load_state_dict(checkpoint_G['optimizer_state_dict'])\n",
    "    optimizer_D.load_state_dict(checkpoint_D['optimizer_state_dict'])\n",
    "    train_losses_FC.append(checkpoint_G['FC_loss'])\n",
    "    train_losses_G.append(checkpoint_G['G_loss'])\n",
    "    train_losses_D.append(checkpoint_G['D_loss'])\n",
    "    present_epoch = checkpoint_G['epoch'] + 1\n",
    "\n",
    "    \n",
    "\n",
    "funnyScoreLoss = nn.MSELoss()\n",
    "\n",
    "def generatorLoss(generator_logits):\n",
    "    result_fake = (torch.zeros(generator_logits[1].shape[0])).to(device)\n",
    "    unc_loss = BCEWithLogitsLoss()(generator_logits[1], result_fake)\n",
    "    con_loss = BCEWithLogitsLoss()(generator_logits[0], result_fake)\n",
    "    loss = con_loss + unc_loss\n",
    "    return loss\n",
    "\n",
    "def discriminatorLoss(uncondition_logits, condition_logits):\n",
    "    result_true = (torch.ones(condition_logits[0].shape[0])).to(device)\n",
    "    result_fake = (torch.zeros(condition_logits[0].shape[0])).to(device)\n",
    "    unc_r = BCEWithLogitsLoss()(condition_logits[0], result_true)\n",
    "    unc_f = BCEWithLogitsLoss()(condition_logits[1], result_fake)\n",
    "    unc_m = BCEWithLogitsLoss()(condition_logits[2], result_fake)\n",
    "    con_r = BCEWithLogitsLoss()(uncondition_logits[0], result_true)\n",
    "    con_f = BCEWithLogitsLoss()(uncondition_logits[1], result_fake)\n",
    "    con_m = BCEWithLogitsLoss()(uncondition_logits[2], result_fake)\n",
    "    loss = unc_r + ((unc_f + unc_m)/2) + con_r + ((con_f + con_m)/2)\n",
    "    return loss"
   ],
   "id": "8dd879c4c76846d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "save_name = '20241011'\n",
    "if not os.path.exists('./Model/'+save_name):\n",
    "    os.makedirs('./Model/'+save_name)\n",
    "    \n",
    "epochs = 10\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(epochs):\n",
    "    print(\"---------------------------------------- epoch \"+ str(epoch + present_epoch) +\" ---------------------------------------\")\n",
    "    train_loss_FC = 0\n",
    "    train_loss_G = 0\n",
    "    train_loss_D = 0\n",
    "    test_loss_FC = 0\n",
    "    test_loss_G = 0\n",
    "    test_loss_D = 0\n",
    "    \n",
    "    ###################################### Train ######################################\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for text_b, image_b, funny_score_b in tepoch:\n",
    "            print(text_b, image_b, funny_score_b)\n",
    "            text, image, funny_score = OxfordDataset.tokenize_batch(text_b, image_b, funny_score_b)\n",
    "            print(text.shape, image.shape, funny_score.shape)\n",
    "            ######################################################\n",
    "                # (1) Generate fake caption\n",
    "            ######################################################\n",
    "            logits, output_funny_score = NetG(text.to(device).to(torch.float32), image.to(device).to(torch.float32))\n",
    "            gen_logits, con_logits, unc_logits = NetD(text.to(device).to(torch.float32), logits.detach().to(torch.float32), image.to(device).to(torch.float32))\n",
    "            ######################################################\n",
    "                # (3) Update Discriminator network\n",
    "            #####################################################\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D = discriminatorLoss(unc_logits, con_logits)\n",
    "            loss_D.backward(retain_graph=True)\n",
    "            optimizer_D.step()\n",
    "            train_loss_D += loss_D.item()\n",
    "            ######################################################\n",
    "                # (4) Update Generator network\n",
    "            ######################################################\n",
    "            optimizer_G.zero_grad()\n",
    "            loss_FC = funnyScoreLoss(output_funny_score, funny_score.to(device).to(torch.float32))\n",
    "            loss_FC.backward(retain_graph=True)\n",
    "            train_loss_FC += loss_FC.item()\n",
    "            loss_G = generatorLoss(gen_logits)\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            train_loss_G += loss_G.item()\n",
    "            ######################################################\n",
    "            tepoch.set_postfix({'FC_loss': loss_FC.item(), 'G_loss': loss_G.item(), 'D_loss': loss_D.item()})\n",
    "            ######################################################\n",
    "    train_loss_FC /= len(train_loader)\n",
    "    train_loss_G /= len(train_loader)\n",
    "    train_loss_D /= len(train_loader)\n",
    "    train_losses_FC.append(train_loss_FC)\n",
    "    train_losses_G.append(train_loss_G)\n",
    "    train_losses_D.append(train_loss_D)\n",
    "    ###################################### Train ######################################\n",
    "    \n",
    "    \n",
    "    ######################################  Test ######################################\n",
    "    with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
    "        for text, image, funny_score in tepoch:\n",
    "            # Generator\n",
    "            logits, output_funny_score = NetG(text.to(device).to(torch.float32), image.to(device).to(torch.float32))\n",
    "            # Discriminator\n",
    "            gen_logits, con_logits, unc_logits = NetD(text.to(device).to(torch.float32), logits.detach().to(torch.float32), image.to(device).to(torch.float32))\n",
    "            # loss\n",
    "            loss_FC = funnyScoreLoss(output_funny_score, funny_score.to(device).to(torch.float32))\n",
    "            loss_G = generatorLoss(gen_logits)\n",
    "            loss_D = discriminatorLoss(unc_logits, con_logits)\n",
    "            test_loss_FC += loss_FC.item()\n",
    "            test_loss_G += loss_G.item()\n",
    "            test_loss_D += loss_D.item()\n",
    "            tepoch.set_postfix({'FC_loss': test_loss_FC, 'G_loss': test_loss_G, 'D_loss': test_loss_D})\n",
    "    test_loss_FC /= len(test_loader)\n",
    "    test_loss_G /= len(test_loader)\n",
    "    test_loss_D /= len(test_loader)\n",
    "    test_losses_FC.append(test_loss_FC)\n",
    "    test_losses_G.append(test_loss_G)\n",
    "    test_losses_D.append(test_loss_D)\n",
    "    ######################################  Test ######################################\n",
    "    \n",
    "    ######################################  Save ######################################\n",
    "    hasSaved = False\n",
    "    # 任一個loss小於最佳loss就存檔\n",
    "    if train_loss_FC < best_train_loss_FC and test_loss_FC < best_test_loss_FC:\n",
    "        best_train_loss_FC = train_loss_FC\n",
    "        best_test_loss_FC = test_loss_FC\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    if train_loss_G < best_train_loss_G and test_loss_G < best_test_loss_G:\n",
    "        best_train_loss_G = train_loss_G\n",
    "        best_test_loss_G = test_loss_G\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    if train_loss_D < best_train_loss_D and test_loss_D < best_test_loss_D:\n",
    "        best_train_loss_D = train_loss_D\n",
    "        best_test_loss_D = test_loss_D\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    \n",
    "    if hasSaved:\n",
    "        save.append(\"V\")\n",
    "    else:\n",
    "        save.append(\" \")\n",
    "\n",
    "    loss_data['train_FC'] = train_losses_FC\n",
    "    loss_data['train_G'] = train_losses_G\n",
    "    loss_data['train_D'] = train_losses_D\n",
    "    loss_data['test_FC'] = test_losses_FC\n",
    "    loss_data['test_G'] = test_losses_G\n",
    "    loss_data['test_D'] = test_losses_D\n",
    "    loss_data['save'] = save\n",
    "    loss_data.to_csv('./Model/' + save_name + \"/\" + save_name + '_loss.csv', index=False)\n",
    "    ######################################  Save ######################################"
   ],
   "id": "d105d60cc85cc3c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T01:09:18.962671Z",
     "start_time": "2024-10-15T01:09:16.930380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoImageProcessor, Swinv2Model\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoConfig\n",
    "\n",
    "def addImagePath(data, imgPath):\n",
    "    data['image_id'] = data['image_id'].apply(lambda x: imgPath + str(x) + '.jpg')\n",
    "    return data\n",
    "\n",
    "def textExtraction(text_data):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "    gemmaConfig = AutoConfig.from_pretrained('google/gemma-2-2b-it')\n",
    "    vocab_size = gemmaConfig.vocab_size  # 詞彙表大小\n",
    "\n",
    "    embedding_dim = 768  # 嵌入维度，與你的圖片嵌入维度相同\n",
    "    text_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    all_features = []\n",
    "    for text in (text_data):\n",
    "        tokens = tokenizer(text, padding='longest', return_tensors='pt')\n",
    "        output = text_embedding(tokens['input_ids'])\n",
    "        linear = torch.nn.Linear(output.shape[1], 64)\n",
    "        projected_output = linear(output.transpose(1, 2)).transpose(1, 2)\n",
    "        all_features.append(projected_output)\n",
    "    return torch.cat(all_features)\n",
    "\n",
    "def textExtractReverse(data):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "\n",
    "    # 有時後空格會失效，所以手動插入空格 <pad> = 0\n",
    "    def insert_zeros(tensor):\n",
    "        zeros = torch.zeros(tensor.shape[0], tensor.shape[1] * 2 - 1)\n",
    "        zeros[:, ::2] = tensor\n",
    "        zeros = zeros.to(int)\n",
    "        return zeros\n",
    "\n",
    "    reverse_data = insert_zeros(data.squeeze(-1))\n",
    "    # reverse the token\n",
    "    reverse = tokenizer.batch_decode(reverse_data, skip_special_tokens=False)\n",
    "    # tokenize with gemma-2b\n",
    "    tokenizer_gemma = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "    prompt = \"Write a humor memetic post for Instagram with the following elements: \"\n",
    "    # all_features = []\n",
    "    for i, text in enumerate(reverse):\n",
    "        text = text.replace(\"<pad>\", \" \").replace(\"  \", \" \")\n",
    "        text = text.split()\n",
    "        text = ', '.join(text)\n",
    "        reverse[i] = prompt + text + \".\"\n",
    "    tokens = tokenizer_gemma(reverse, padding='max_length', max_length=64, return_tensors='pt')\n",
    "    # all_features.append(tokens['input_ids'])\n",
    "    return tokens['input_ids']\n",
    "\n",
    "# 定義批量處理和提取特徵的函數\n",
    "def imageExtraction(image_data):\n",
    "    # 加載 Swinv2 模型和處理器\n",
    "    image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "    swin = Swinv2Model.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "\n",
    "    # 將模型設置為評估模式\n",
    "    swin.eval()\n",
    "\n",
    "    # 如果有 GPU，可以將模型移動到 GPU 上\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    swin.to(device)\n",
    "\n",
    "    all_features = []\n",
    "    for image_path in (image_data):\n",
    "        with torch.no_grad():\n",
    "            # 加載並預處理圖像\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = np.array(image)\n",
    "            image = image[:, :, :3]\n",
    "            # 使用 image_processor 將 batch 圖片處理成適合模型的格式\n",
    "            inputs = image_processor(image, return_tensors=\"pt\")\n",
    "            # 將 inputs 放到 GPU 上（如果可用）\n",
    "            inputs.to(device)\n",
    "            # 獲取 Swinv2 模型的輸出\n",
    "            outputs = swin(**inputs)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "            # 儲存特徵\n",
    "            last_hidden_states = last_hidden_states.cpu()\n",
    "            all_features.append(last_hidden_states)\n",
    "    return torch.cat(all_features)\n",
    "\n"
   ],
   "id": "52943676af162956",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T01:31:46.494891Z",
     "start_time": "2024-10-15T01:09:18.962671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "dirPath = '../Data/Oxford_HIC/Filtered_oxford_hic_data.csv'\n",
    "data = pd.read_csv(dirPath)\n",
    "data = addImagePath(data, '../Data/Oxford_HIC/oxford_img/')\n",
    "with tqdm.tqdm(total=data.shape[0]) as pbar:\n",
    "    for i in range(data.shape[0]):\n",
    "        image_path = data['image_id'][i]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except:\n",
    "            print(image_path)\n",
    "        pbar.update(1)"
   ],
   "id": "2cd70712875151d7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8975/3398160 [00:03<22:26, 2517.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9829/3398160 [00:03<21:28, 2628.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12347/3398160 [00:04<20:46, 2715.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_966.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 24906/3398160 [00:09<21:39, 2596.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_2495.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_2495.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_2495.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 31555/3398160 [00:12<20:52, 2687.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_3049.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 78074/3398160 [00:29<21:03, 2627.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_6997.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_6997.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_6997.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 85056/3398160 [00:32<20:06, 2746.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_7487.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 98382/3398160 [00:37<22:22, 2458.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_8176.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_8176.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_8176.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_8176.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_8176.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_8176.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_8176.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_8176.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 147720/3398160 [00:59<26:00, 2083.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_12501.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 302766/3398160 [02:27<36:12, 1424.97it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 303413/3398160 [02:28<33:43, 1529.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 306121/3398160 [02:29<34:53, 1477.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_25616.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 871760/3398160 [06:27<17:09, 2454.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_67874.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_67874.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_67874.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_67874.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_67874.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_67874.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 879298/3398160 [06:31<16:58, 2472.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_68584.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_68584.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_68584.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_68584.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_68584.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_68584.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 894582/3398160 [06:37<17:12, 2424.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_69659.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 912074/3398160 [06:44<17:42, 2339.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_70929.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 952215/3398160 [07:01<16:54, 2409.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_70929.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 968082/3398160 [07:08<18:30, 2189.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Data/Oxford_HIC/oxford_img/bokete_12501.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_12501.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_12501.jpg\n",
      "../Data/Oxford_HIC/oxford_img/bokete_12501.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3398160/3398160 [22:24<00:00, 2526.94it/s]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load Data",
   "id": "821d6044bf065fc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T06:38:15.469369Z",
     "start_time": "2024-10-23T06:38:11.851537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoImageProcessor, Swinv2Model\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoConfig\n",
    "from extractor import textExtraction, textExtractReverse, imageExtraction, addImagePath\n",
    "import tqdm\n",
    "import gc"
   ],
   "id": "85cc88a5b37af13a",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T06:38:20.183682Z",
     "start_time": "2024-10-23T06:38:17.705295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dirPath = '../Data/Oxford_HIC/Filtered_oxford_hic_data.csv'\n",
    "imgPath = '../Data/Oxford_HIC/oxford_img/'\n",
    "\n",
    "data = pd.read_csv(dirPath)"
   ],
   "id": "91dc51ac29931383",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T07:42:54.366790500Z",
     "start_time": "2024-10-17T07:35:56.598604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "columns = ['image_id', 'image']\n",
    "new_data = pd.DataFrame(columns=columns)\n",
    "# new_data['image_id'] = data['image_id'].astype(object)\n",
    "# new_data['caption'] = new_data['caption'].astype(object)\n",
    "# new_data['funny_score'] = data['funny_score'].astype(float)"
   ],
   "id": "2f43a0d5e0ca746d",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T06:38:23.822096Z",
     "start_time": "2024-10-23T06:38:23.818921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "idcounter = 0\n",
    "def captionID():\n",
    "    global idcounter\n",
    "    idcounter += 1\n",
    "    idName = 'caption' + str(idcounter)\n",
    "    return idName\n",
    "    "
   ],
   "id": "b79b6b8cd344f203",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T06:38:24.713262Z",
     "start_time": "2024-10-23T06:38:24.075572Z"
    }
   },
   "cell_type": "code",
   "source": "data['caption_id'] = data['caption'].apply(lambda x: captionID())",
   "id": "cb2af90016cbac8a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T06:38:25.116979Z",
     "start_time": "2024-10-23T06:38:25.113294Z"
    }
   },
   "cell_type": "code",
   "source": "data.shape",
   "id": "7d01aaf026fadadb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3398081, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T06:01:07.730916Z",
     "start_time": "2024-10-23T06:01:02.675643Z"
    }
   },
   "cell_type": "code",
   "source": "data.to_csv('../Data/Oxford_HIC/CaptionID_oxford_hic_data.csv', index=False)",
   "id": "a2cdbd22e77a1600",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T06:38:29.388737Z",
     "start_time": "2024-10-23T06:38:29.377415Z"
    }
   },
   "cell_type": "code",
   "source": "data.head()",
   "id": "7c5577e50b6d9cf7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   image_id                                            caption  funny_score  \\\n",
       "0  bokete_0                          My driver's license photo          0.0   \n",
       "1  bokete_1                                    Refugee relief.          0.0   \n",
       "2  bokete_2  Now! I think I stepped on a cat! What? Really?...          0.0   \n",
       "3  bokete_3      You wouldn't know I was reading a comic book.          0.0   \n",
       "4  bokete_4                  Oh no! I forgot my ・・・・ clothes!\"          0.0   \n",
       "\n",
       "  caption_id  \n",
       "0   caption1  \n",
       "1   caption2  \n",
       "2   caption3  \n",
       "3   caption4  \n",
       "4   caption5  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>caption</th>\n",
       "      <th>funny_score</th>\n",
       "      <th>caption_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bokete_0</td>\n",
       "      <td>My driver's license photo</td>\n",
       "      <td>0.0</td>\n",
       "      <td>caption1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bokete_1</td>\n",
       "      <td>Refugee relief.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>caption2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bokete_2</td>\n",
       "      <td>Now! I think I stepped on a cat! What? Really?...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>caption3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bokete_3</td>\n",
       "      <td>You wouldn't know I was reading a comic book.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>caption4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bokete_4</td>\n",
       "      <td>Oh no! I forgot my ・・・・ clothes!\"</td>\n",
       "      <td>0.0</td>\n",
       "      <td>caption5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save textExtraction",
   "id": "4ca017f7d006e3e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T06:38:33.525521Z",
     "start_time": "2024-10-23T06:38:33.085788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, BCELoss\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "from extractor import addImagePath, textExtraction, imageExtraction, textExtractReverse\n",
    "\n",
    "eps = torch.finfo(torch.bfloat16).eps"
   ],
   "id": "e3a10f45160d4966",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T06:38:43.390486Z",
     "start_time": "2024-10-23T06:38:34.714197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 官方的Gemma #########################################################################################\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "gemmaConfig =  AutoConfig.from_pretrained('google/gemma-2-2b-it')\n",
    "########################################################################################################"
   ],
   "id": "23604a7d3d362f17",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "257b3eaf34de41a28e66c6950b652e52"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-23T06:39:05.889149Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 50000 一個round直到跑完\n",
    "start_index = 0\n",
    "end_index = 50000\n",
    "filename = 0\n",
    "while start_index < data.shape[0]+1:\n",
    "    a = textExtraction(tokenizer, gemmaConfig, data['caption'][start_index:end_index].tolist())\n",
    "    with tqdm(total=a.shape[0], position=0, leave=True) as pbarr:\n",
    "        for j in range(a.shape[0]):\n",
    "            if os.path.exists('../Data/Oxford_HIC/CaptionData/'+data['caption_id'][j]+'.pt'):\n",
    "                pbarr.update(1)\n",
    "                continue\n",
    "            caption = a[j].clone().detach()\n",
    "            torch.save(caption, '../Data/Oxford_HIC/CaptionData/'+data['caption_id'][j]+'.pt')\n",
    "            pbarr.update(1)\n",
    "            del caption\n",
    "            gc.collect()\n",
    "    # clear memory\n",
    "    del a\n",
    "    gc.collect()\n",
    "    start_index = end_index\n",
    "    if end_index == data.shape[0]:\n",
    "        break\n",
    "    if end_index + 50000 > data.shape[0]:\n",
    "        end_index = data.shape[0]\n",
    "    else:\n",
    "        end_index += 50000\n",
    "        "
   ],
   "id": "8f623a0d8be2ce67",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:07<00:00, 6302.45it/s]\n",
      "  0%|          | 61/50000 [00:03<58:33, 14.21it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T06:38:49.016512Z",
     "start_time": "2024-10-23T06:38:49.008797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = torch.load('../Data/Oxford_HIC/CaptionData/'+data['caption_id'][0]+'.pt', weights_only=False)\n",
    "print(text.shape)\n",
    "print(text.dtype)\n",
    "print(text)"
   ],
   "id": "984c13acaee08eb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 768])\n",
      "torch.float32\n",
      "tensor([[-1.0821, -1.3760, -0.7178,  ...,  0.0934, -0.6337,  1.8725],\n",
      "        [ 0.0397, -0.3388,  1.1762,  ...,  0.7794,  0.4113,  0.6521],\n",
      "        [-0.3500, -0.1279,  0.0934,  ...,  0.6557,  1.3321,  0.6173],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 無 warning 版本\n",
    "# 50000 一個round直到跑完\n",
    "start_index = 0\n",
    "end_index = 10000\n",
    "filename = 1\n",
    "while start_index < data.shape[0]+1:   \n",
    "    with tqdm.tqdm(total=data.shape[0], position=0, leave = True) as pbars:\n",
    "        # tasks = pbars.add_task(\"[white]Processing...\", total=data.shape[0])\n",
    "        a = textExtraction(data['caption'][start_index:end_index].tolist())\n",
    "        df_extended = pd.DataFrame([a]).T\n",
    "        df = pd.DataFrame(data[start_index:end_index])\n",
    "        df['caption'] = df_extended\n",
    "        new_data = pd.concat([new_data, df], ignore_index=True)\n",
    "        # pbars.set_postfix({'filename': filename, 'Status': ' Saving'})\n",
    "        new_data.to_csv('../Data/Oxford_HIC/Caption'+str(filename)+'_oxford_hic_data.csv', index=False)\n",
    "        del a\n",
    "        del df_extended\n",
    "        del df\n",
    "        gc.collect()\n",
    "        pbars.update(end_index)\n",
    "        pbars.set_postfix({'filename': filename})\n",
    "        # pbars.set_postfix({'filename': filename, 'Status': ' Loading'})\n",
    "        new_data = pd.read_csv('../Data/Oxford_HIC/Caption'+str(filename)+'_oxford_hic_data.csv')\n",
    "        start_index = end_index\n",
    "        if end_index == data.shape[0]:\n",
    "            break\n",
    "        if end_index + 10000 > data.shape[0]:\n",
    "            end_index = data.shape[0]\n",
    "        else:\n",
    "            end_index += 10000"
   ],
   "id": "60421bdf1681da57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T06:11:14.687009Z",
     "start_time": "2024-10-17T06:10:52.714116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data1 = pd.read_csv('../Data/Oxford_HIC/Caption_oxford_hic_data.csv')\n",
    "data2 = pd.read_csv('../Data/Oxford_HIC/Caption2_oxford_hic_data.csv')\n",
    "data = pd.concat([data1, data2], ignore_index=True)\n",
    "data.to_csv('../Data/Oxford_HIC/Caption_Done_oxford_hic_data.csv', index=False)"
   ],
   "id": "942ee44c7ff6a1e2",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T08:41:03.513598Z",
     "start_time": "2024-10-17T08:41:03.497640Z"
    }
   },
   "cell_type": "code",
   "source": "data.shape",
   "id": "4532981ecccc049",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3398081, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save imageExtraction",
   "id": "6bf6d2b4077127e8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T21:55:14.052113Z",
     "start_time": "2024-10-20T21:55:08.032749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dirPath = '../Data/Oxford_HIC/Filtered_oxford_hic_data.csv'\n",
    "imgPath = '../Data/Oxford_HIC/oxford_img/'\n",
    "\n",
    "data = pd.read_csv(dirPath)"
   ],
   "id": "87b65e0cd15ad4cd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T21:55:14.420305Z",
     "start_time": "2024-10-20T21:55:14.168341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img_names = data['image_id'].unique()\n",
    "len(img_names)"
   ],
   "id": "6a8c42f5acb96bf8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116649"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T15:27:32.379515Z",
     "start_time": "2024-10-17T15:27:32.374521Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 7,
   "source": [
    "column = ['image_id', 'image']\n",
    "new_data = pd.DataFrame(columns=column)"
   ],
   "id": "6509233eb813b566"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T18:54:40.756031Z",
     "start_time": "2024-10-17T15:27:35.312755Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [10:07<00:00, 16.47it/s]\n",
      "100%|██████████| 10000/10000 [09:28<00:00, 17.60it/s], filename=2, Done=3e+4, All=6e+4]\n",
      "100%|██████████| 10000/10000 [10:46<00:00, 15.46it/s], filename=3, Done=1e+4, All=7e+4]\n",
      "100%|██████████| 10000/10000 [11:43<00:00, 14.21it/s]/s, filename=3, Done=2e+4, All=8e+4]\n",
      "100%|██████████| 10000/10000 [10:10<00:00, 16.38it/s]/s, filename=3, Done=3e+4, All=9e+4]\n",
      "100%|██████████| 10000/10000 [11:25<00:00, 14.59it/s]t/s, filename=4, Done=1e+4, All=1e+5]\n",
      "100%|██████████| 6649/6649 [07:03<00:00, 15.69it/s]1it/s, filename=4, Done=2e+4, All=110000]\n",
      "120000it [3:27:05,  9.66it/s, filename=4, Done=26649, All=116649]                            \n"
     ]
    }
   ],
   "execution_count": 8,
   "source": [
    "imgPath = '../Data/Oxford_HIC/oxford_img/'\n",
    "start_index = 50000\n",
    "end_index = 60000\n",
    "filename = 2\n",
    "start = 0\n",
    "with tqdm.tqdm(total=len(img_names), position=0, leave=True) as pbars:\n",
    "    while start_index < len(img_names):\n",
    "        if start == 0:\n",
    "            pbars.update(start_index)\n",
    "            start = 1\n",
    "        if start_index % 30000 == 0 :\n",
    "            column = ['image_id', 'image']\n",
    "            new_data = pd.DataFrame(columns=column)\n",
    "            filename += 1\n",
    "        else:\n",
    "            new_data = pd.read_csv('../Data/Oxford_HIC/Image'+ str(filename) +'_oxford_hic_data.csv')\n",
    "        \n",
    "        image_path = np.array([imgPath + str(item) + '.jpg' for item in img_names[start_index:end_index]])\n",
    "        image = imageExtraction(image_path)\n",
    "        df_extended1 = pd.DataFrame([image]).T\n",
    "        df_extended1.columns = ['image']\n",
    "        df_extended2 = pd.DataFrame(img_names[start_index:end_index])\n",
    "        df_extended2.columns = ['image_id']\n",
    "        df_extended = pd.DataFrame(columns=column)\n",
    "        df_extended['image_id'] = df_extended2['image_id'].values\n",
    "        df_extended['image'] = df_extended1['image'].values\n",
    "        new_data = pd.concat([new_data, df_extended], ignore_index=True)\n",
    "        new_data.to_csv('../Data/Oxford_HIC/Image'+ str(filename) +'_oxford_hic_data.csv', index=False)\n",
    "        All = (filename-1)*30000 + new_data.shape[0]\n",
    "        pbars.set_postfix({'filename': filename, 'Done': new_data.shape[0], 'All': All})\n",
    "        del image\n",
    "        del df_extended1\n",
    "        del df_extended2\n",
    "        del df_extended\n",
    "        del image_path\n",
    "        del new_data\n",
    "        gc.collect()\n",
    "        pbars.update(10000)\n",
    "        \n",
    "        start_index = end_index\n",
    "        if end_index == len(img_names):\n",
    "            break\n",
    "        if end_index + 10000 > len(img_names):\n",
    "            end_index = len(img_names)\n",
    "        else:\n",
    "            end_index += 10000"
   ],
   "id": "aaed05199b1e38a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Get Image with Index (選擇不要用這個)",
   "id": "8d3528a05354958c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T20:04:16.473510Z",
     "start_time": "2024-10-17T20:04:00.724556Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116649"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "dirPath = '../Data/Oxford_HIC/Filtered_oxford_hic_data.csv'\n",
    "data = pd.read_csv(dirPath)\n"
   ],
   "id": "fda35260fac712dc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T20:09:15.725991Z",
     "start_time": "2024-10-17T20:09:15.498475Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116649"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9,
   "source": [
    "img_names = data['image_id'].unique().tolist()\n",
    "len(img_names)"
   ],
   "id": "fa6036c23c727637"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T20:07:47.350053Z",
     "start_time": "2024-10-17T20:07:47.343931Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function list.index(value, start=0, stop=9223372036854775807, /)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8,
   "source": [
    "# get index of img_names\n",
    "index = []\n",
    "for i in range(len(img_names)):\n",
    "[data['image_id'][0] == img_names].index"
   ],
   "id": "854ef8e02aaf8abd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T20:18:07.188586Z",
     "start_time": "2024-10-17T20:18:07.183232Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 11,
   "source": [
    "def getFile(imgName):\n",
    "    index = img_names.index(imgName)\n",
    "    file = index // 30000\n",
    "    pbar.update(1)\n",
    "    return file\n",
    "def getIndex(imgName):\n",
    "    index = img_names.index(imgName)\n",
    "    idx = index % 30000\n",
    "    pbar.update(1)\n",
    "    return idx"
   ],
   "id": "157b72caae46e7b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T22:34:24.158090Z",
     "start_time": "2024-10-17T20:18:25.110161Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3398081/3398081 [1:07:20<00:00, 841.09it/s]\n",
      "100%|██████████| 3398081/3398081 [1:08:23<00:00, 828.04it/s]\n"
     ]
    }
   ],
   "execution_count": 13,
   "source": [
    "import tqdm\n",
    "new_data = data\n",
    "with tqdm.tqdm(total=data.shape[0]) as pbar:\n",
    "    new_data['image_file'] = new_data['image_id'].apply(lambda x: getFile(x))\n",
    "with tqdm.tqdm(total=data.shape[0]) as pbar:\n",
    "    new_data['image_index'] = new_data['image_id'].apply(lambda x: getIndex(x))\n",
    "new_data.to_csv('../Data/Oxford_HIC/Image_index_oxford_hic_data.csv', index=False)"
   ],
   "id": "f78f6c7bb015b2ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T23:54:39.653954Z",
     "start_time": "2024-10-17T23:34:23.254998Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 2)\n",
      "(30000, 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 11\u001B[0m\n\u001B[0;32m      9\u001B[0m data\u001B[38;5;241m.\u001B[39mto_csv(fileName, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m data\n\u001B[1;32m---> 11\u001B[0m \u001B[43mgc\u001B[49m\u001B[38;5;241m.\u001B[39mcollect()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'gc' is not defined"
     ]
    }
   ],
   "execution_count": 11,
   "source": [
    "import pandas as pd\n",
    "i = 1\n",
    "fileName = '../Data/Oxford_HIC/ImageList/Image'+str(i)+'_oxford_hic_data.csv'\n",
    "data = pd.read_csv(fileName)\n",
    "print(data.shape)\n",
    "data.drop(columns=['image_id'], axis=1, inplace=True)\n",
    "print(data.shape)\n",
    "fileName = '../Data/Oxford_HIC/Image'+str(i)+'_oxford_hic_data.csv'\n",
    "data.to_csv(fileName, index=False)\n",
    "del data\n",
    "gc.collect()"
   ],
   "id": "b9ca9d19740fd58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Save ImageData with pt",
   "id": "4bc7bca3f6cf900f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T19:08:31.766570Z",
     "start_time": "2024-10-17T19:05:20.021143Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import torch\n",
    "# new_data = pd.DataFrame()\n",
    "# temp = pd.read_csv('../Data/Oxford_HIC/Caption2_oxford_hic_data.csv', iterator=True, chunksize=10000)\n",
    "# new_data = pd.concat(temp, ignore_index=True)\n",
    "new_data = pd.read_csv('../Data/Oxford_HIC/Caption2_oxford_hic_data.csv')"
   ],
   "id": "80553a0f19e2d43f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-10-17T19:08:31.795002Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "new_data['caption'] = new_data['caption'].apply(lambda x: torch.tensor(ast.literal_eval(x)))",
   "id": "1f57ca0b8ae0a5ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T18:50:38.561143Z",
     "start_time": "2024-10-20T15:52:49.807479Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26649/26649 [2:52:56<00:00,  2.57it/s]  \n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "import tqdm\n",
    "import ast\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "for i in range(4, 5):\n",
    "    temp = pd.read_csv('../Data/Oxford_HIC/ImageList/Image'+str(i)+'_oxford_hic_data.csv', iterator=True, chunksize=5000)\n",
    "    data = pd.concat(temp, ignore_index=True)\n",
    "    del temp\n",
    "    gc.collect()\n",
    "    with tqdm.tqdm(total=data.shape[0]) as pbar:\n",
    "        for j in range(data.shape[0]):\n",
    "            if os.path.exists('../Data/Oxford_HIC/ImageData/'+data['image_id'][j]+'.pt'):\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            image = torch.tensor(ast.literal_eval(data['image'][j])).to(torch.float32)\n",
    "            torch.save(image, '../Data/Oxford_HIC/ImageData/'+data['image_id'][j]+'.pt')\n",
    "            pbar.update(1)\n",
    "            del image\n",
    "            gc.collect()\n",
    "    del data\n",
    "    gc.collect()"
   ],
   "id": "c402784ce0d7070f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T02:14:55.902381Z",
     "start_time": "2024-10-20T02:14:55.439457Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 9,
   "source": [
    "# save as pt\n",
    "image = torch.tensor(ast.literal_eval(data['image'][0])).to(torch.float32)\n",
    "torch.save(image, '../Data/Oxford_HIC/ImageData/'+data['image_id'][0]+'.pt')"
   ],
   "id": "83d47256a824dc3f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T02:19:09.590092Z",
     "start_time": "2024-10-20T02:19:09.577411Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 768])\n",
      "torch.float32\n",
      "tensor([[-1.8035,  0.5202,  0.4408,  ...,  0.0865, -0.4930, -2.6763],\n",
      "        [ 0.2728,  1.3702,  0.0285,  ...,  0.4052, -0.2069,  0.1937],\n",
      "        [-0.3072,  2.7029,  2.3379,  ..., -0.4808, -0.6480, -0.2691],\n",
      "        ...,\n",
      "        [-2.5183, -1.1495,  1.1463,  ..., -0.0928, -0.4463, -0.2784],\n",
      "        [-2.0778, -0.6066, -0.2198,  ...,  0.0085,  0.5643, -0.0705],\n",
      "        [-0.7022, -0.2345, -0.1471,  ..., -0.4195,  0.8525,  0.2226]])\n"
     ]
    }
   ],
   "execution_count": 13,
   "source": [
    "image = torch.load('../Data/Oxford_HIC/ImageData/'+data['image_id'][0]+'.pt', weights_only=False)\n",
    "print(image.shape)\n",
    "print(image.dtype)\n",
    "print(image)"
   ],
   "id": "e56a2ac2d52f67a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T02:11:58.330948Z",
     "start_time": "2024-10-20T02:11:57.890577Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 768])\n",
      "torch.float32\n",
      "tensor([[-1.8035,  0.5202,  0.4408,  ...,  0.0865, -0.4930, -2.6763],\n",
      "        [ 0.2728,  1.3702,  0.0285,  ...,  0.4052, -0.2069,  0.1937],\n",
      "        [-0.3072,  2.7029,  2.3379,  ..., -0.4808, -0.6480, -0.2691],\n",
      "        ...,\n",
      "        [-2.5183, -1.1495,  1.1463,  ..., -0.0928, -0.4463, -0.2784],\n",
      "        [-2.0778, -0.6066, -0.2198,  ...,  0.0085,  0.5643, -0.0705],\n",
      "        [-0.7022, -0.2345, -0.1471,  ..., -0.4195,  0.8525,  0.2226]])\n"
     ]
    }
   ],
   "execution_count": 7,
   "source": [
    "# data = pd.read_csv('../Data/Oxford_HIC/ImageList/Image'+str(i)+'_oxford_hic_data.csv')\n",
    "image = torch.tensor(ast.literal_eval(data['image'][0])).to(torch.float32)\n",
    "print(image.shape)\n",
    "print(image.dtype)\n",
    "print(image)"
   ],
   "id": "fb6b87051170c92e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-20T02:11:24.116988Z",
     "start_time": "2024-10-20T02:11:23.913709Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 768])\n",
      "torch.float32\n",
      "tensor([[ 0.5960, -1.5175,  0.4210,  ..., -0.4031, -1.3697,  0.1927],\n",
      "        [ 0.4261,  1.9516,  0.3017,  ...,  1.2013,  1.6093, -1.0158],\n",
      "        [-1.1612, -0.3983, -0.2365,  ...,  0.7160, -1.8396, -0.4191],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "execution_count": 5,
   "source": [
    "# caption = pd.read_csv('../Data/Oxford_HIC/Caption1_oxford_hic_data.csv')\n",
    "captionData = torch.tensor(ast.literal_eval(caption['caption'][0])).to(torch.float32)\n",
    "print(captionData.shape)\n",
    "print(captionData.dtype)\n",
    "print(captionData)"
   ],
   "id": "2b558c374daf80b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
