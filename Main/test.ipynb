{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T19:44:53.758444Z",
     "start_time": "2024-09-19T19:44:45.235831Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from extractor import text_extraction, image_extraction"
   ],
   "id": "7123375a60e24a10",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T19:45:09.550968Z",
     "start_time": "2024-09-19T19:44:53.761447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if args.img - dir == 'Oxford_HIC':\n",
    "#     dirPath = '../Data/Oxford_HIC/oxford_hic_data.csv'\n",
    "#     imgPath = '../Data/Oxford_HIC/oxford_img/'\n",
    "# else:\n",
    "dirPath = '../Data/Instagram/Filter_' + 'wendys' + '.csv'\n",
    "imgPath = '../Data/Instagram/' + 'wendys' + '_img/'\n",
    "# load data\n",
    "data = pd.read_csv(dirPath)\n",
    "data = text_extraction(data, imgPath)\n",
    "# split data\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_text = torch.tensor(train.iloc[:,0:768].to_numpy())\n",
    "train_image = image_extraction(train.iloc[:,768])\n",
    "train_funny_score = torch.tensor(train.iloc[:,769].to_numpy())\n",
    "test_text = torch.tensor(test.iloc[:,0:768].to_numpy())\n",
    "test_image = image_extraction(test.iloc[:,768])\n",
    "test_funny_score = torch.tensor(test.iloc[:,769].to_numpy())"
   ],
   "id": "8bddfef236149dd6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 293/293 [00:08<00:00, 33.90it/s]\n",
      "100%|██████████| 74/74 [00:02<00:00, 34.43it/s]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T19:45:09.584182Z",
     "start_time": "2024-09-19T19:45:09.581132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(train_text, train_image, train_funny_score)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_text, test_image, test_funny_score)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "id": "127ff8b78bd6db6a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### 不確定是否為官方的 Gemini #############################################################################\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"describeai/gemini\")\n",
    "gemini = AutoModelForSeq2SeqLM.from_pretrained(\"describeai/gemini\")\n",
    "#######################################################################################################"
   ],
   "id": "9368441165fc2667"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T19:45:45.991835Z",
     "start_time": "2024-09-19T19:45:39.450922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 官方的Gemma #########################################################################################\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "# gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\", revision=\"float16\")\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "########################################################################################################"
   ],
   "id": "2b63e7fd2e5f833e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa24a716dd9b4df0bc357dd62fc9223e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T18:52:34.709855Z",
     "start_time": "2024-09-19T18:52:28.356048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### LLM測試\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = gemma.generate(**input_ids, max_length=500)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "71615226bd69b78b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf9b27d422334d00ab0d501d9b51dcac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a poem about Machine Learning.\n",
      "\n",
      "Answer:\n",
      "\n",
      "Step 1/2\n",
      "Machine Learning is a powerful tool that can help us make sense of the world around us. It can analyze vast amounts of data and make predictions based on that data. It can also learn from our own actions and behaviors, and adapt to new situations. But what does it mean to be a machine? Is it just a computer program that can learn? Or is it something more? Maybe it's a combination of both. Maybe it's a combination of human intelligence and computer power. Maybe it's a combination of both. But whatever it is, it's a powerful tool that can help us make sense of the world around us. And it's a tool that can help us make sense of ourselves.\n",
      "\n",
      "Step 2/2\n",
      "So, what does it mean to be a machine? It means that we can learn from our own actions and behaviors, and adapt to new situations. It means that we can make sense of the world around us, and make predictions based on that data. It means that we can learn from our own experiences, and use that knowledge to make better decisions in the future. So, what does it mean to be a machine? It means that we can learn from our own experiences, and use that knowledge to make better decisions in the future.<eos>\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T19:16:56.560612Z",
     "start_time": "2024-09-19T19:16:55.869597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"Give me three best book.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "4869cf8b21256080",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Give me three best book.\n",
      "\n",
      "Answer:\n",
      "\n",
      "1. The Great Gatsby\n",
      "2. The Catcher in the Rye\n",
      "3. The Grapes of Wrath<eos>\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T19:46:03.120936Z",
     "start_time": "2024-09-19T19:46:03.114989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.self_att = nn.MultiheadAttention(768, 1)\n",
    "        self.multi_att = nn.MultiheadAttention(768, 8)\n",
    "        self.layer_norm = nn.LayerNorm(768)\n",
    "        self.linear = nn.Linear(768, 768)\n",
    "        \n",
    "    def self_attention(self, x):\n",
    "        self_out,_ = self.self_att(x, x, x)\n",
    "        self_out = self.layer_norm(self_out + x)\n",
    "        return self_out\n",
    "    \n",
    "    def multi_head_attention(self, x):\n",
    "        multi_out,_ = self.multi_att(x, x, x)\n",
    "        multi_out = self.linear(multi_out)\n",
    "        multi_out = self.layer_norm(multi_out + x)\n",
    "        return multi_out\n",
    "    \n",
    "    def co_attention(self, x, y):\n",
    "        # x: self, y: another\n",
    "        co_out,_ = self.multi_att(x, y, y)\n",
    "        co_out = self.linear(co_out)\n",
    "        co_out = self.layer_norm(co_out + y)\n",
    "        return co_out\n",
    "    \n",
    "    def feed_forward(self, x):\n",
    "        ff_out = self.linear(x)\n",
    "        ff_out = self.layer_norm(ff_out + x)\n",
    "        return ff_out\n",
    "    \n",
    "    \n",
    "    def forward(self, text, image):\n",
    "        text = text.unsqueeze(1).expand(-1, 64, -1)\n",
    "        text = text.transpose(0, 1)\n",
    "        image = image.transpose(0, 1)\n",
    "        \n",
    "        text = self.self_attention(text)\n",
    "        text = self.feed_forward(text)\n",
    "        \n",
    "        image = self.multi_head_attention(image)\n",
    "        \n",
    "        text = self.co_attention(text, image)\n",
    "        image = self.co_attention(image, text)\n",
    "        \n",
    "        output = text + image\n",
    "        output = self.feed_forward(output)\n",
    "        \n",
    "        return output"
   ],
   "id": "6df9340e53bfbe2a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T19:46:15.815923Z",
     "start_time": "2024-09-19T19:46:15.770164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Generator()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "gemma.to(device)\n",
    "print(1)"
   ],
   "id": "bfd7b3c0683d760f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T19:46:17.002612Z",
     "start_time": "2024-09-19T19:46:16.969978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 10\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for text, image, funny_score in tepoch:\n",
    "            optimizer.zero_grad()\n",
    "            temp_output = model(text.to(device).to(torch.float32), image.to(device).to(torch.float32))\n",
    "            output = gemma.generate(**temp_output, max_new_tokens=200)\n",
    "            print(output)\n",
    "            break\n",
    "            loss = criterion(output, funny_score)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            tepoch.set_postfix(loss=train_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
    "        for text, image, funny_score in tepoch:\n",
    "            output = model(text, image)\n",
    "            loss = criterion(output, funny_score)\n",
    "            test_loss += loss.item()\n",
    "            tepoch.set_postfix(loss=test_loss)\n",
    "    test_losses.append(test_loss)"
   ],
   "id": "8418eb4f49f3759b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 768]) torch.Size([32, 64, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "transformers.generation.utils.GenerationMixin.generate() argument after ** must be a mapping, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(text\u001B[38;5;241m.\u001B[39mshape, image\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m     11\u001B[0m temp_output \u001B[38;5;241m=\u001B[39m model(text\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32), image\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32))\n\u001B[1;32m---> 12\u001B[0m output \u001B[38;5;241m=\u001B[39m gemma\u001B[38;5;241m.\u001B[39mgenerate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtemp_output, max_new_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(output)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: transformers.generation.utils.GenerationMixin.generate() argument after ** must be a mapping, not Tensor"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T19:46:35.770367Z",
     "start_time": "2024-09-19T19:46:35.766391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "temp_output.shape\n",
    "temp = temp_output.transpose(0, 1)[0]\n",
    "temp.shape"
   ],
   "id": "7cd08c7031df1f0a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T19:46:37.905176Z",
     "start_time": "2024-09-19T19:46:37.885095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tensor_to_text(tensor):\n",
    "    # 將 tensor 轉換為 list，然後轉換為一個字符串\n",
    "    tensor_list = tensor.tolist()\n",
    "    tensor_text = str(tensor_list)\n",
    "    return tensor_text\n",
    "\n",
    "input_text = tensor_to_text(temp)"
   ],
   "id": "6a9e8a18279bf688",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-09-19T19:49:12.941522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gemma.to(\"cpu\")\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids.to(\"cpu\")\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "6de7618cb89938f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T19:36:33.818401Z",
     "start_time": "2024-09-19T19:36:33.801913Z"
    }
   },
   "cell_type": "code",
   "source": "output = gemma.generate(**temp, max_new_tokens=200)",
   "id": "ba3d2189ad0ffa68",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "transformers.generation.utils.GenerationMixin.generate() argument after ** must be a mapping, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[148], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m output \u001B[38;5;241m=\u001B[39m gemma\u001B[38;5;241m.\u001B[39mgenerate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtemp, max_new_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m200\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: transformers.generation.utils.GenerationMixin.generate() argument after ** must be a mapping, not Tensor"
     ]
    }
   ],
   "execution_count": 148
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "acdf14280dba6239"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
