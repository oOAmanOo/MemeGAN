{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T22:24:50.967528Z",
     "start_time": "2024-09-25T22:24:40.632178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.data.data_collator import tolist\n",
    "from torch.nn import BCELoss, CrossEntropyLoss\n",
    "from torchmetrics.classification import BinaryHingeLoss\n",
    "from extractor import addImagePath, textExtraction, imageExtraction, textExtractReverse"
   ],
   "id": "7123375a60e24a10",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Check the max length of the text data\n",
   "id": "3dae9bb9554dc2e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:30:22.247599Z",
     "start_time": "2024-09-23T23:30:22.196724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subUrlList = ['wendys','mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada','sonicdrivein']\n",
    "max_length = 0\n",
    "sum_length = 0\n",
    "for subUrl in subUrlList:\n",
    "    dirPath = '../Data/Instagram/Filter_' + subUrl + '.csv'\n",
    "    data = pd.read_csv(dirPath)\n",
    "    \n",
    "    for i in range(len(data['caption'])):\n",
    "        sum_length += len(str(data['caption'][i]).split())\n",
    "        if len(str(data['caption'][i]).split()) > max_length:\n",
    "            max_length = len(str(data['caption'][i]).split())\n",
    "            word = data['caption'][i]\n",
    "print(max_length)\n",
    "print(sum_length/len(data['caption']))"
   ],
   "id": "32667357b99a51a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373\n",
      "31.870794078061913\n"
     ]
    }
   ],
   "execution_count": 188
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:35:01.489143Z",
     "start_time": "2024-09-23T23:34:44.345758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_length = 0\n",
    "word = ''\n",
    "dirPath = '../Data/Oxford_HIC/oxford_hic_data.csv'\n",
    "data = pd.read_csv(dirPath)\n",
    "max_length = 0\n",
    "sum_length = 0\n",
    "index =0 \n",
    "counter  = 5\n",
    "# find the max word count of the text data['caption']\n",
    "for i in range(len(data['caption'])):\n",
    "    sum_length += len(str(data['caption'][i]).split())\n",
    "    if len(str(data['caption'][i]).split()) > max_length:\n",
    "        max_length = len(str(data['caption'][i]).split())\n",
    "        word = data['caption'][i]\n",
    "        index = i       \n",
    "        #\n",
    "print(max_length, i)\n",
    "print(sum_length/len(data['caption']))\n",
    "data.shape"
   ],
   "id": "b3b8a1b3461c9aed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5988\\3251324240.py:4: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(dirPath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9729 3657846\n",
      "10.514508397972905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3657847, 3)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 193
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:35:22.129734Z",
     "start_time": "2024-09-23T23:35:22.126848Z"
    }
   },
   "cell_type": "code",
   "source": "sum_length",
   "id": "21d6947ca3cd9281",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38460463"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 195
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Check the max word count of the text data",
   "id": "ca059487768954a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T18:33:30.989254Z",
     "start_time": "2024-09-25T18:33:28.914261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subUrlList = ['wendys','mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada','sonicdrivein']\n",
    "wordList = []\n",
    "total = 0\n",
    "for subUrl in subUrlList:\n",
    "    dirPath = '../Data/Instagram/Filter_' + subUrl + '.csv'\n",
    "    data = pd.read_csv(dirPath)\n",
    "    \n",
    "    for i in range(len(data['caption'])):\n",
    "        for word in str(data['caption'][i]).split():\n",
    "            if word not in wordList:\n",
    "                wordList.append(word)\n",
    "                total += 1\n",
    "                \n",
    "print(len(wordList))        \n",
    "print(total)"
   ],
   "id": "b2b206fcf154474e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15056\n",
      "15056\n"
     ]
    }
   ],
   "execution_count": 260
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-25T18:33:33.703355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wordList = []\n",
    "total = 0\n",
    "dirPath = '../Data/Oxford_HIC/oxford_hic_data.csv'\n",
    "data = pd.read_csv(dirPath)\n",
    "\n",
    "for i in range(len(data['caption'])):\n",
    "    for word in str(data['caption'][i]).split():\n",
    "        if word not in wordList:\n",
    "            wordList.append(word)\n",
    "            total += 1\n",
    "\n",
    "print(len(wordList))        \n",
    "print(total)"
   ],
   "id": "a02f3990a97e4a87",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5988\\496614120.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(dirPath)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[261], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(dirPath)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaption\u001B[39m\u001B[38;5;124m'\u001B[39m])):\n\u001B[1;32m----> 5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaption\u001B[39m\u001B[38;5;124m'\u001B[39m][i])\u001B[38;5;241m.\u001B[39msplit():\n\u001B[0;32m      6\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m wordList:\n\u001B[0;32m      7\u001B[0m             wordList\u001B[38;5;241m.\u001B[39mappend(word)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 261
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Load the data and split the data",
   "id": "6f301289c5bb2722"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T22:25:17.752605Z",
     "start_time": "2024-09-25T22:24:58.887668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if args.img - dir == 'Oxford_HIC':\n",
    "#     dirPath = '../Data/Oxford_HIC/oxford_hic_data.csv'\n",
    "#     imgPath = '../Data/Oxford_HIC/oxford_img/'\n",
    "# else:\n",
    "dirPath = '../Data/Instagram/Filter_' + 'wendys' + '.csv'\n",
    "imgPath = '../Data/Instagram/' + 'wendys' + '_img/'\n",
    "# load data\n",
    "data = pd.read_csv(dirPath)\n",
    "data = addImagePath(data, imgPath)\n",
    "# split data\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_text = textExtraction(train['caption'])\n",
    "train_image = imageExtraction(train['image_id'])\n",
    "train_funny_score = torch.tensor(train['funny_score'].to_numpy())\n",
    "test_text = textExtraction(test['caption'])\n",
    "test_image = imageExtraction(test['image_id'])\n",
    "test_funny_score = torch.tensor(test['funny_score'].to_numpy())"
   ],
   "id": "8bddfef236149dd6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 293/293 [00:00<00:00, 2343.99it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 293/293 [00:09<00:00, 29.99it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:00<00:00, 2114.29it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:02<00:00, 27.82it/s]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T22:25:17.766418Z",
     "start_time": "2024-09-25T22:25:17.761608Z"
    }
   },
   "cell_type": "code",
   "source": "train_text.shape, train_image.shape, train_funny_score.shape",
   "id": "851fa717c816958b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([293, 64, 768]), torch.Size([293, 64, 768]), torch.Size([293]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T22:25:17.856260Z",
     "start_time": "2024-09-25T22:25:17.852907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(train_text, train_image, train_funny_score)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_text, test_image, test_funny_score)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "id": "127ff8b78bd6db6a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. LLM Test",
   "id": "807b418cfd673b81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T15:31:33.447037Z",
     "start_time": "2024-09-22T15:31:31.893764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### ä¸ç¢ºå®šæ˜¯å¦ç‚ºå®˜æ–¹çš„ Gemini ############################################################################\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"describeai/gemini\")\n",
    "gemini = AutoModelForSeq2SeqLM.from_pretrained(\"describeai/gemini\")\n",
    "#######################################################################################################\n",
    "gemini"
   ],
   "id": "9368441165fc2667",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:48:27.892980Z",
     "start_time": "2024-09-22T21:48:26.279195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### å®˜æ–¹çš„Gemma #########################################################################################\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\", device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "########################################################################################################"
   ],
   "id": "2b63e7fd2e5f833e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1405: UserWarning: Current model requires 4608 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "008dd7a670c14c7d8ba61fde16eba45e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T16:47:42.602569Z",
     "start_time": "2024-09-22T16:47:41.350489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gemma.to(device)\n",
    "vocab_size = 256128  # è¯æ±‡è¡¨å¤§å°\n",
    "embedding_dim = 768  # åµŒå…¥ç»´åº¦ï¼Œä¸Žä½ çš„å›¾åƒåµŒå…¥ç»´åº¦ç›¸åŒ\n",
    "text_embedding = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "\n",
    "words = \"ðŸ‘»ðŸ”¥ðŸ˜‚ðŸ˜ðŸ‘ðŸ¤¦â€â™€ï¸ðŸ¤¦â€â™‚ï¸ðŸ¤·â€â™€ï¸ðŸ¤·â€â™‚ï¸âœŒðŸ¤žðŸ˜‰ðŸ˜ŽðŸŽ¶ðŸ˜¢ðŸ’–ðŸŽ‰ðŸŒ¹ðŸ’‹ðŸ‘ðŸ±â€ðŸ’»ðŸ±â€ðŸ‰ðŸ±â€ðŸ‘“âœ”ðŸ‘€ðŸ˜ƒâœ¨ðŸ˜†ðŸ¤”ðŸ¤¢ðŸŽðŸ«¢ ha ha\"\n",
    "tokens = tokenizer(words, truncation=True, padding= 'max_length', max_length=100, return_tensors=\"pt\")\n",
    "output = text_embedding(tokens['input_ids'].to(device))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def find_closest_embeddings(x, embedding_matrix, top_k=1):\n",
    "    # Normalize both the input tensor x and the embedding matrix\n",
    "    x = F.normalize(x, dim=1)  # Normalize input tensor along feature dimension\n",
    "    embedding_matrix = F.normalize(embedding_matrix, dim=1)  # Normalize embedding matrix\n",
    "    \n",
    "    # Compute cosine similarity between x and embedding matrix\n",
    "    similarity = torch.matmul(x, embedding_matrix.T)  # Shape: [10, 50265]\n",
    "    \n",
    "    # Find top-k closest embeddings for each tensor in x\n",
    "    top_k_values, top_k_indices = torch.topk(similarity, top_k, dim=1)\n",
    "    \n",
    "    return top_k_indices, top_k_values\n",
    "\n",
    "\n",
    "# print(output.squeeze(0).shape)\n",
    "top_k_indices, top_k_values = find_closest_embeddings(output.squeeze(0), text_embedding.weight)\n",
    "# top_k_indices.shape\n",
    "indices = tokenizer.decode(top_k_indices.squeeze(-1))\n",
    "print(indices)"
   ],
   "id": "b5a1bd08816d845f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 768])\n",
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><bos>ðŸ‘»ðŸ”¥ðŸ˜‚ðŸ˜ðŸ‘ðŸ¤¦â€â™€ï¸ðŸ¤¦â€â™‚ï¸ðŸ¤·â€â™€ï¸ðŸ¤·â€â™‚ï¸âœŒðŸ¤žðŸ˜‰ðŸ˜ŽðŸŽ¶ðŸ˜¢ðŸ’–ðŸŽ‰ðŸŒ¹ðŸ’‹ðŸ‘ðŸ±â€ðŸ’»ðŸ±â€ðŸ‰ðŸ±â€ðŸ‘“âœ”ðŸ‘€ðŸ˜ƒâœ¨ðŸ˜†ðŸ¤”ðŸ¤¢ðŸŽðŸ«¢ ha ha\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T20:36:50.602442Z",
     "start_time": "2024-09-23T20:36:50.584473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = \"ðŸ‘»ðŸ”¥ðŸ˜‚ðŸ˜ðŸ‘ðŸ¤¦â€â™€ï¸ðŸ¤¦â€â™‚ï¸ðŸ¤·â€â™€ï¸ðŸ¤·â€â™‚ï¸âœŒðŸ¤žðŸ˜‰ðŸ˜ŽðŸŽ¶ðŸ˜¢ðŸ’–ðŸŽ‰ðŸŒ¹ðŸ’‹ðŸ‘ðŸ±â€ðŸ’»ðŸ±â€ðŸ‰ðŸ±â€ðŸ‘“âœ”ðŸ‘€ðŸ˜ƒâœ¨ðŸ˜†ðŸ¤”ðŸ¤¢ðŸŽðŸ«¢ ha ha\"\n",
    "tokens = tokenizer(words, truncation=True, padding= 'max_length', max_length=100, return_tensors=\"pt\")\n",
    "tokens"
   ],
   "id": "9a5b2095330cfcd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      2, 242538, 237638, 236471, 238429, 237019, 240525,  67292,\n",
       "         240525,  68399, 239921,  67292, 239921,  68399, 239529, 241807, 238309,\n",
       "         238859, 240438, 240116, 239208, 239548, 240315, 240887, 238499, 242993,\n",
       "         235879, 242482, 242993, 235879, 245092, 242993, 235879, 246943, 237488,\n",
       "         239220, 239938, 236309, 239312, 238918, 241769, 241227, 248165,    661,\n",
       "            661]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:48:50.626449Z",
     "start_time": "2024-09-22T21:48:50.445244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_text = \"Give me three best book.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "\n",
    "# outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "# print(tokenizer.decode(outputs[0]))"
   ],
   "id": "4869cf8b21256080",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m tokenizer(input_text, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      4\u001B[0m input_ids\n\u001B[1;32m----> 6\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mgemma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(tokenizer\u001B[38;5;241m.\u001B[39mdecode(outputs[\u001B[38;5;241m0\u001B[39m]))\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2024\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[0;32m   2016\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[0;32m   2017\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   2018\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[0;32m   2019\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[0;32m   2020\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   2021\u001B[0m     )\n\u001B[0;32m   2023\u001B[0m     \u001B[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[1;32m-> 2024\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2025\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2026\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2027\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_warper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_warper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2028\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2029\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2030\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2031\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2032\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2033\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2035\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SAMPLE, GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SEARCH):\n\u001B[0;32m   2036\u001B[0m     \u001B[38;5;66;03m# 11. prepare logits warper\u001B[39;00m\n\u001B[0;32m   2037\u001B[0m     prepared_logits_warper \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2038\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_logits_warper(generation_config, device\u001B[38;5;241m=\u001B[39minput_ids\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m   2039\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m generation_config\u001B[38;5;241m.\u001B[39mdo_sample\n\u001B[0;32m   2040\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   2041\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2982\u001B[0m, in \u001B[0;36mGenerationMixin._sample\u001B[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001B[0m\n\u001B[0;32m   2979\u001B[0m model_inputs\u001B[38;5;241m.\u001B[39mupdate({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_hidden_states\u001B[39m\u001B[38;5;124m\"\u001B[39m: output_hidden_states} \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;28;01melse\u001B[39;00m {})\n\u001B[0;32m   2981\u001B[0m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[1;32m-> 2982\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   2984\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m synced_gpus \u001B[38;5;129;01mand\u001B[39;00m this_peer_finished:\n\u001B[0;32m   2985\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# don't waste resources running the code we don't need\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:1074\u001B[0m, in \u001B[0;36mGemmaForCausalLM.forward\u001B[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[0m\n\u001B[0;32m   1071\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m   1073\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[1;32m-> 1074\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1075\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1076\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1077\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1078\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1079\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1080\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1081\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1082\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1083\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1084\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1085\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1087\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1088\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head(hidden_states)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:827\u001B[0m, in \u001B[0;36mGemmaModel.forward\u001B[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[0m\n\u001B[0;32m    824\u001B[0m     use_cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    826\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 827\u001B[0m     inputs_embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    829\u001B[0m return_legacy_cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# noqa: F841\u001B[39;00m\n\u001B[0;32m    830\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    831\u001B[0m     use_cache \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(past_key_values, Cache) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining\n\u001B[0;32m    832\u001B[0m ):  \u001B[38;5;66;03m# kept for BC (non `Cache` `past_key_values` inputs)\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:164\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    166\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2267\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2261\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[0;32m   2262\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[0;32m   2263\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m   2264\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[0;32m   2265\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[0;32m   2266\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2267\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T19:03:27.261799Z",
     "start_time": "2024-09-23T19:03:27.257163Z"
    }
   },
   "cell_type": "code",
   "source": "gemma",
   "id": "f26de7efeee68a71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Generator",
   "id": "d8c8fefd0a84950b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T22:25:54.871956Z",
     "start_time": "2024-09-25T22:25:46.386760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\", device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")"
   ],
   "id": "3de2433a8664699a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9bec2d78e1a466585ad0fd165817fa8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T20:47:32.821384Z",
     "start_time": "2024-09-25T20:47:32.718461Z"
    }
   },
   "cell_type": "code",
   "source": "type(tokenizer.vocab)",
   "id": "8820d75af5bce5e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 274
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T22:31:14.175430Z",
     "start_time": "2024-09-25T22:31:14.164974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.self_att = nn.MultiheadAttention(768, 1)\n",
    "        self.multi_att = nn.MultiheadAttention(768, 8)\n",
    "        self.layer_norm = nn.LayerNorm(768)\n",
    "        self.linear = nn.Linear(768, 768)\n",
    "        # æ¸›åŽ»æœ€å¾Œä¸€å±¤\n",
    "        self.gemma = nn.Sequential(*list(gemma.children())[:-1])\n",
    "        self.lm_head = nn.Sequential(*list(gemma.children())[1:])\n",
    "        self.linearFunnyScore1 = nn.Linear(2304, 1)\n",
    "        self.linearFunnyScore2 = nn.Linear(64, 1)\n",
    "        \n",
    "    \n",
    "    def selfAttention(self, x):\n",
    "        self_out,_ = self.self_att(x, x, x)\n",
    "        self_out = self.layer_norm(self_out + x)\n",
    "        return self_out\n",
    "    \n",
    "    def multiheadAttention(self, x):\n",
    "        multi_out,_ = self.multi_att(x, x, x)\n",
    "        multi_out = self.linear(multi_out)\n",
    "        multi_out = self.layer_norm(multi_out + x)\n",
    "        return multi_out\n",
    "    \n",
    "    def coAttention(self, x, y):\n",
    "        # x: self, y: another\n",
    "        co_out,_ = self.multi_att(x, y, y)\n",
    "        co_out = self.linear(co_out)\n",
    "        co_out = self.layer_norm(co_out + y)\n",
    "        return co_out\n",
    "    \n",
    "    def feedForward(self, x):\n",
    "        ff_out = self.linear(x)\n",
    "        ff_out = self.layer_norm(ff_out + x)\n",
    "        return ff_out\n",
    "    \n",
    "    def transformer(self, text, image):\n",
    "        # self attention module\n",
    "        image = self.selfAttention(image)\n",
    "        image = self.feedForward(image)\n",
    "        # multihead attention module\n",
    "        text = self.multiheadAttention(text)\n",
    "        # co-attention module\n",
    "        visual_attending_textual = self.coAttention(image, text)\n",
    "        textual_attending_visual = self.coAttention(text, image)\n",
    "        \n",
    "        return visual_attending_textual, textual_attending_visual\n",
    "        \n",
    "    def gemmaGenerate(self, x):\n",
    "        \n",
    "        def find_closest_embeddings(x, embedding_matrix, top_k=1):\n",
    "            # Normalize both the input tensor x and the embedding matrix\n",
    "            x = nn.functional.normalize(x, dim=2)  # Normalize input tensor along feature dimension\n",
    "            embedding_matrix = nn.functional.normalize(embedding_matrix, dim=1)  # Normalize embedding matrix\n",
    "            # Compute cosine similarity between x and embedding matrix\n",
    "            similarity = torch.matmul(x, embedding_matrix.T)  # Shape: [10, 50265]\n",
    "            # Find top-k closest embeddings for each tensor in x\n",
    "            top_k_values, top_k_indices = torch.topk(similarity, top_k, dim=2)\n",
    "        \n",
    "            return top_k_indices, top_k_values\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            vocab_size = 50265  # è¯æ±‡è¡¨å¤§å°\n",
    "            embedding_dim = 768  # åµŒå…¥ç»´åº¦ï¼Œä¸Žä½ çš„å›¾åƒåµŒå…¥ç»´åº¦ç›¸åŒ\n",
    "            text_embedding = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "            top_k_indices, top_k_values = find_closest_embeddings(x, text_embedding.weight)\n",
    "            \n",
    "            # ä½¿ç”¨gemmaä½œç‚ºmodelçš„ä¸€éƒ¨åˆ†\n",
    "            output = self.gemma(top_k_indices.squeeze(-1))\n",
    "            # output[0] = last_hidden_state\n",
    "            # output[1] = past_key_values\n",
    "            \n",
    "        return output[0]\n",
    "               \n",
    "    \n",
    "    def forward(self, text, image):\n",
    "        max_seq_len = max(text.shape[1], image.shape[1])\n",
    "        text = nn.functional.pad(text, (0, 0, 0, max_seq_len - text.shape[1]))\n",
    "        image = nn.functional.pad(image, (0, 0, 0, max_seq_len - image.shape[1]))\n",
    "        text = text.transpose(0, 1)\n",
    "        image = image.transpose(0, 1)\n",
    "        \n",
    "        visual_attending_textual, textual_attending_visual = self.transformer(text, image)\n",
    "        \n",
    "        feature_fusion = visual_attending_textual + textual_attending_visual\n",
    "        feature_fusion = self.feedForward(feature_fusion)        \n",
    "        feature_fusion = feature_fusion.squeeze(-1)\n",
    "        feature_fusion = feature_fusion.transpose(0, 1)\n",
    "        last_hidden_state = self.gemmaGenerate(feature_fusion)\n",
    "        output_text = self.lm_head(last_hidden_state)\n",
    "        output_funny_score = self.linearFunnyScore1(last_hidden_state.to(torch.float32))\n",
    "        output_funny_score = self.linearFunnyScore2(output_funny_score.squeeze(-1))\n",
    "        print(output_text.shape, output_funny_score.shape)\n",
    "        return output_text, output_funny_score\n",
    "    \n",
    "    def generate(self, image, tokenizer, max_length=100):\n",
    "        result_caption = []\n",
    "        text = torch.zeros_like(image)\n",
    "        \n",
    "        # æœ‰æ™‚å¾Œç©ºæ ¼æœƒå¤±æ•ˆï¼Œæ‰€ä»¥æ‰‹å‹•æ’å…¥ç©ºæ ¼\n",
    "        def insert_zeros(list):\n",
    "            # <pad> = 0, <eos> = 1, <bos> = 2, <unk> = 3, <mask> = 4, <2mass> = 5, [@BOS@] = 6\n",
    "            zeros = [0] * (2 * len(list) - 1)\n",
    "            zeros[::2] = list\n",
    "            return zeros\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                text = text.transpose(0, 1)\n",
    "                image = image.transpose(0, 1)\n",
    "                visual_attending_textual, textual_attending_visual = self.transformer(text, image)\n",
    "                feature_fusion = visual_attending_textual + textual_attending_visual\n",
    "                feature_fusion = self.feedForward(feature_fusion)        \n",
    "                feature_fusion = feature_fusion.squeeze(-1)\n",
    "                feature_fusion = feature_fusion.transpose(0, 1)\n",
    "                last_hidden_state = self.gemmaGenerate(feature_fusion)\n",
    "                output_text = self.lm_head(last_hidden_state)\n",
    "\n",
    "                predicted = output_text.argmax(1)\n",
    "                result_caption.append(predicted.item()) # not sure\n",
    "                \n",
    "                text = insert_zeros(result_caption)\n",
    "                text = tokenizer.decode(text, skip_special_tokens=False)\n",
    "                text = text.replace(\"<pad>\", \" \").replace(\"  \", \" \").split()\n",
    "                text = [word for word in text if word[0] != \"<\"]\n",
    "                text = \" \".join(text)\n",
    "                \n",
    "                if predicted.item() == 1: #<eos> = 1 \n",
    "                    break\n",
    "                else:\n",
    "                    text = textExtraction([text])\n",
    "                    max_seq_len = max(text.shape[1], image.shape[1])\n",
    "                    text = nn.functional.pad(text, (0, 0, 0, max_seq_len - text.shape[1]))\n",
    "                    image = nn.functional.pad(image, (0, 0, 0, max_seq_len - image.shape[1]))\n",
    "            \n",
    "        return text"
   ],
   "id": "6df9340e53bfbe2a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate Test",
   "id": "4c44d1e70ced399f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T21:39:29.434132Z",
     "start_time": "2024-09-25T21:39:28.070561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "A = [0,155,2,355,4,544,6,788,8,944,10]\n",
    "def insert_zeros(list):\n",
    "    # <pad> = 0, <eos> = 1, <bos> = 2, <unk> = 3, <mask> = 4, <2mass> = 5, [@BOS@] = 6\n",
    "    zeros = [0] * (2 * len(list) - 1)\n",
    "    zeros[::2] = list\n",
    "    return zeros\n",
    "\n",
    "A = insert_zeros(A) \n",
    "print(A)\n",
    "text = tokenizer.decode(A, skip_special_tokens=False)\n",
    "text = text.replace(\"<pad>\", \" \").replace(\"  \", \" \").split()\n",
    "text = [word for word in text if word[0] != \"<\"]\n",
    "text = \" \".join(text)\n",
    "text = textExtraction([text])\n",
    "print(text.shape)\n"
   ],
   "id": "1256431eb698484f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 155, 0, 2, 0, 355, 0, 4, 0, 544, 0, 6, 0, 788, 0, 8, 0, 944, 0, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1000.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 373, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 302
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Discriminator",
   "id": "e431f844eea97c94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.linearFake = nn.Linear(256000, 768)\n",
    "        self.linear = nn.Linear(768, 768)\n",
    "        self.mlp = nn.sequential(\n",
    "            nn.Linear(768, 768),\n",
    "            nn.Linear(768, 768)\n",
    "        )\n",
    "        \n",
    "    def forward(self, real_text, fake_text, image):        \n",
    "        # real_text = [batch_size, 64, 768]\n",
    "        # fake_text = [batch_size, 64, 256000]\n",
    "        # image = [batch_size, 64, 768]\n",
    "        fake_text = self.linearFake(fake_text)\n",
    "        mismatched_text = torch.roll(real_text, 1, 0)\n",
    "\n",
    "        # conditional (contrastive)\n",
    "        C_r = torch.cat((real_text, image), dim=1)\n",
    "        C_g = torch.cat((fake_text, image), dim=1)\n",
    "        C_m = torch.cat((mismatched_text, image), dim=1)\n",
    "        C_r = self.linear(C_r).unsqueeze(0)\n",
    "        C_g = self.linear(C_g).unsqueeze(0)\n",
    "        C_m = self.linear(C_m).unsqueeze(0)\n",
    "        con_output = torch.cat((C_r, C_g, C_m), dim=0)\n",
    "\n",
    "        # unconditional \n",
    "        real_text = self.mlp(real_text).unsqueeze(0)\n",
    "        fake_text = self.mlp(fake_text).unsqueeze(0)\n",
    "        mismatched_text = self.mlp(mismatched_text).unsqueeze(0)\n",
    "        unc_output = torch.cat((real_text, fake_text, mismatched_text), dim=0)\n",
    "        \n",
    "        return con_output, unc_output"
   ],
   "id": "ff75f88ea94b5045"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.nn import BCELoss, CrossEntropyLoss\n",
    "from torchmetrics.classification import BinaryHingeLoss\n",
    "def generatorLoss(uncondition_logits, image):\n",
    "    condition_logits = torch.cat((uncondition_logits, image), dim=1)\n",
    "    con_loss = BCELoss()(condition_logits, torch.zeros(condition_logits).shape[0])\n",
    "    unc_loss = BinaryHingeLoss()(uncondition_logits, torch.zeros(uncondition_logits.shape[0]))\n",
    "    loss = con_loss + unc_loss\n",
    "    return loss\n",
    "    \n",
    "def discriminatorLoss(condition_logits, uncondition_logits):\n",
    "    unc_r = BCELoss()(condition_logits[0], torch.ones(uncondition_logits.shape[0]))\n",
    "    unc_f = BinaryHingeLoss()(condition_logits[1], torch.zeros(uncondition_logits.shape[0]))\n",
    "    unc_m = BinaryHingeLoss()(condition_logits[2], torch.zeros(uncondition_logits.shape[0]))\n",
    "    ce_input = uncondition_logits[:-1].transpose(0, 1)\n",
    "    con_r = CrossEntropyLoss()(ce_input, torch.zeros(ce_input.shape[0]))\n",
    "    con_f = BinaryHingeLoss()(uncondition_logits[1], torch.zeros(uncondition_logits[1].shape[0]))\n",
    "    con_m = BinaryHingeLoss()(uncondition_logits[2], torch.zeros(uncondition_logits[2].shape[0]))\n",
    "    loss = unc_r + unc_f + unc_m + con_r + con_f + con_m\n",
    "    return loss"
   ],
   "id": "75b7ab757bb601ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T14:39:43.698351Z",
     "start_time": "2024-09-25T14:39:42.661347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer_testetse = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")\n",
    "# tokenizer.pad_token_id\n",
    "tokenizer.cls_token_id"
   ],
   "id": "4c893127bcde28a1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 213
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:44:04.628109Z",
     "start_time": "2024-09-23T22:44:04.584912Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.0617, 0.8585]), tensor([0]), tensor([0.3098, 0.6698]))\n",
      "(tensor([0.0506, 0.2285]), tensor([0]), tensor([0.2333, 0.7159]))\n",
      "(tensor([0.8983, 0.1901]), tensor([0]), tensor([0.4482, 0.9889]))\n",
      "torch.Size([3, 2, 6])\n",
      "{'input_ids': tensor([[[1.0000, 0.0617, 0.8585, 0.0000, 0.3098, 0.6698],\n",
      "         [1.0000, 0.8621, 0.9828, 0.0000, 0.3098, 0.6698]],\n",
      "\n",
      "        [[1.0000, 0.0506, 0.2285, 0.0000, 0.2333, 0.7159],\n",
      "         [1.0000, 0.7058, 0.9992, 0.0000, 0.2333, 0.7159]],\n",
      "\n",
      "        [[1.0000, 0.8983, 0.1901, 0.0000, 0.4482, 0.9889],\n",
      "         [1.0000, 0.1833, 0.6297, 0.0000, 0.4482, 0.9889]]], device='cuda:0')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 185,
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn import functional as F\n",
    "real = torch.rand(3,2)\n",
    "fake = torch.rand(3,2)\n",
    "mismatched = torch.roll(real, 1, 0)\n",
    "image = torch.rand(3,2)\n",
    "\n",
    "inputs = []\n",
    "for i in range(real.size(0)):\n",
    "    print((real[i], torch.tensor([0]), image[i]))\n",
    "    C_r = torch.cat((real[i], torch.tensor([0]), image[i]), dim=0)\n",
    "    C_g = torch.cat((fake[i],torch.tensor([0]), image[i]), dim=0)\n",
    "    inputs.append(C_r)\n",
    "    inputs.append(C_g)\n",
    "input_ids = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "input_ids = F.pad(input_ids, (1, 0), \"constant\", 1)\n",
    "input_ids = input_ids.view(-1, 2, input_ids.shape[-1])\n",
    "print(input_ids.shape)\n",
    "batch = {\"input_ids\": input_ids.to(device)}\n",
    "print(batch)\n",
    "\n"
   ],
   "id": "32d4a8436838d792"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T22:31:21.367211Z",
     "start_time": "2024-09-25T22:31:21.208115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# empty cuda memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Generator()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# gemma.to(device)\n",
    "model"
   ],
   "id": "bfd7b3c0683d760f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (self_att): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (multi_att): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (gemma): Sequential(\n",
       "    (0): Gemma2Model(\n",
       "      (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-25): 26 x Gemma2DecoderLayer(\n",
       "          (self_attn): Gemma2SdpaAttention(\n",
       "            (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "            (rotary_emb): Gemma2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Gemma2MLP(\n",
       "            (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "            (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "            (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Sequential(\n",
       "    (0): Linear(in_features=2304, out_features=256000, bias=False)\n",
       "  )\n",
       "  (linearFunnyScore1): Linear(in_features=2304, out_features=1, bias=True)\n",
       "  (linearFunnyScore2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T22:31:27.027413700Z",
     "start_time": "2024-09-25T22:31:24.034856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 10\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for text, image, funny_score in tepoch:\n",
    "            optimizer.zero_grad()\n",
    "            logits, output_funny_score = model(text.to(device).to(torch.float32), image.to(device).to(torch.float32))\n",
    "            # output = gemma.generate(**temp_output, max_new_tokens=200)\n",
    "            # print(output)\n",
    "\n",
    "    #         loss = criterion(output, funny_score)\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         train_loss += loss.item()\n",
    "    #         tepoch.set_postfix(loss=train_loss)\n",
    "    # train_losses.append(train_loss)\n",
    "    # with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
    "    #     for text, image, funny_score in tepoch:\n",
    "    #         output = model(text, image)\n",
    "    #         loss = criterion(output, funny_score)\n",
    "    #         test_loss += loss.item()\n",
    "    #         tepoch.set_postfix(loss=test_loss)\n",
    "    # test_losses.append(test_loss)"
   ],
   "id": "8418eb4f49f3759b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 1/10 [00:00<00:01,  5.35batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 256000]) torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 2/10 [00:00<00:03,  2.09batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 256000]) torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:01<00:03,  2.23batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 256000]) torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [00:01<00:02,  2.35batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 256000]) torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [00:02<00:02,  2.38batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 256000]) torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [00:02<00:01,  2.40batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 256000]) torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:02<00:01,  2.42batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 256000]) torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [00:03<00:01,  2.25batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 10\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m text, image, funny_score \u001B[38;5;129;01min\u001B[39;00m tepoch:\n\u001B[0;32m      9\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 10\u001B[0m         logits, output_funny_score \u001B[38;5;241m=\u001B[39m model(\u001B[43mtext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32), image\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32))\n\u001B[0;32m     11\u001B[0m         \u001B[38;5;66;03m# output = gemma.generate(**temp_output, max_new_tokens=200)\u001B[39;00m\n\u001B[0;32m     12\u001B[0m         \u001B[38;5;66;03m# print(output)\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m#         tepoch.set_postfix(loss=test_loss)\u001B[39;00m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# test_losses.append(test_loss)\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. ç›´æŽ¥ç”Ÿæˆæ™‚ï¼Œå°‡å…¶è®Šæˆå¯é–±è®€çš„æ–‡å­—",
   "id": "4c50909e32141e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T22:16:21.542187Z",
     "start_time": "2024-09-22T22:16:20.814703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "# Assuming `output` is your tensor with shape [batch_size, seq_len, embedding_dim]\n",
    "filteroutput = output[:, 373:]  # Keep only the last 373 tokens\n",
    "\n",
    "# æœ‰æ™‚å¾Œç©ºæ ¼æœƒå¤±æ•ˆï¼Œæ‰€ä»¥æ‰‹å‹•æ’å…¥ç©ºæ ¼\n",
    "def insert_zeros(tensor):\n",
    "    # <pad> = 0, <eos> = 1, <bos> = 2, <unk> = 3, <mask> = 4, <2mass> = 5, [@BOS@] = 6\n",
    "    batch_size, seq_len = tensor.shape\n",
    "    zeros = torch.zeros(batch_size, 2 * seq_len - 1, dtype=torch.long)\n",
    "    # zeros = zeros * 6\n",
    "    zeros[:, ::2] = tensor\n",
    "    return zeros\n",
    "\n",
    "# è™•ç†å¾Œçš„ Tensor\n",
    "result = insert_zeros(filteroutput)\n",
    "# print(result)\n",
    "tryoo = tokenizer.batch_decode(result, skip_special_tokens=False)\n",
    "print(tryoo[10])\n",
    "tryoo = tokenizer.batch_decode(filteroutput, skip_special_tokens=False)\n",
    "print(tryoo[10])"
   ],
   "id": "b49e20ee32fb54d2",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Loss Function",
   "id": "31ca539aa09e7ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T21:19:15.818120Z",
     "start_time": "2024-09-23T21:19:15.814613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# batch = 2 , seq_len = 3, embedding_dim = 4\n",
    "a = torch.rand(3,2,4)\n",
    "print(a)\n",
    "print(torch.roll(a,1,0))"
   ],
   "id": "2b250171b10c325c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4365, 0.6128, 0.7685, 0.6493],\n",
      "         [0.7793, 0.8083, 0.2490, 0.5736]],\n",
      "\n",
      "        [[0.5002, 0.0683, 0.2360, 0.3595],\n",
      "         [0.9551, 0.5397, 0.0717, 0.0972]],\n",
      "\n",
      "        [[0.5003, 0.5845, 0.0727, 0.4068],\n",
      "         [0.7186, 0.6403, 0.3828, 0.5849]]])\n",
      "tensor([[[0.5003, 0.5845, 0.0727, 0.4068],\n",
      "         [0.7186, 0.6403, 0.3828, 0.5849]],\n",
      "\n",
      "        [[0.4365, 0.6128, 0.7685, 0.6493],\n",
      "         [0.7793, 0.8083, 0.2490, 0.5736]],\n",
      "\n",
      "        [[0.5002, 0.0683, 0.2360, 0.3595],\n",
      "         [0.9551, 0.5397, 0.0717, 0.0972]]])\n"
     ]
    }
   ],
   "execution_count": 175
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n",
   "id": "fb0ca19c9a789318"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T18:44:22.761872Z",
     "start_time": "2024-09-23T18:44:22.754099Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 97,
   "source": [
    "# ##################Loss for G and Ds##############################\n",
    "def discriminator_loss(netD, real_imgs, fake_imgs, conditions,\n",
    "                       real_labels, fake_labels):\n",
    "    # Forward\n",
    "    real_features = netD(real_imgs)\n",
    "    fake_features = netD(fake_imgs.detach())\n",
    "    # loss\n",
    "    #\n",
    "    cond_real_logits = netD.COND_DNET(real_features, conditions)\n",
    "    cond_real_errD = nn.BCELoss()(cond_real_logits, real_labels)\n",
    "    cond_fake_logits = netD.COND_DNET(fake_features, conditions)\n",
    "    cond_fake_errD = nn.BCELoss()(cond_fake_logits, fake_labels)\n",
    "    #\n",
    "    batch_size = real_features.size(0)\n",
    "    cond_wrong_logits = netD.COND_DNET(real_features[:(batch_size - 1)], conditions[1:batch_size])\n",
    "    cond_wrong_errD = nn.BCELoss()(cond_wrong_logits, fake_labels[1:batch_size])\n",
    "\n",
    "    if netD.UNCOND_DNET is not None:\n",
    "        real_logits = netD.UNCOND_DNET(real_features)\n",
    "        fake_logits = netD.UNCOND_DNET(fake_features)\n",
    "        real_errD = nn.BCELoss()(real_logits, real_labels)\n",
    "        fake_errD = nn.BCELoss()(fake_logits, fake_labels)\n",
    "        errD = ((real_errD + cond_real_errD) / 2. +\n",
    "                (fake_errD + cond_fake_errD + cond_wrong_errD) / 3.)\n",
    "    else:\n",
    "        errD = cond_real_errD + (cond_fake_errD + cond_wrong_errD) / 2.\n",
    "    return errD\n",
    "\n",
    "# text, image, funny_score\n",
    "def generator_loss(netsD, image_encoder, fake_imgs, real_labels,\n",
    "                   words_embs, sent_emb, match_labels,\n",
    "                   cap_lens, class_ids):\n",
    "    # numDs = len(netsD)\n",
    "    batch_size = real_labels.size(0)\n",
    "    logs = ''\n",
    "    # Forward\n",
    "    errG_total = 0\n",
    "    features = netsD[i](fake_imgs[i])\n",
    "    cond_logits = netsD[i].COND_DNET(features, sent_emb)\n",
    "    cond_errG = nn.BCELoss()(cond_logits, real_labels)\n",
    "    if netsD[i].UNCOND_DNET is  not None:\n",
    "        logits = netsD[i].UNCOND_DNET(features)\n",
    "        errG = nn.BCELoss()(logits, real_labels)\n",
    "        g_loss = errG + cond_errG\n",
    "    else:\n",
    "        g_loss = cond_errG\n",
    "    errG_total += g_loss\n",
    "    # err_img = errG_total.data[0]\n",
    "    logs += 'g_loss%d: %.2f ' % (i, g_loss.data[0])\n",
    "\n",
    "    # Ranking loss\n",
    "    if i == (numDs - 1):\n",
    "        # words_features: batch_size x nef x 17 x 17\n",
    "        # sent_code: batch_size x nef\n",
    "        region_features, cnn_code = image_encoder(fake_imgs[i])\n",
    "        w_loss0, w_loss1, _ = words_loss(region_features, words_embs,\n",
    "                                         match_labels, cap_lens,\n",
    "                                         class_ids, batch_size)\n",
    "        w_loss = (w_loss0 + w_loss1) * \\\n",
    "                 cfg.TRAIN.SMOOTH.LAMBDA\n",
    "        # err_words = err_words + w_loss.data[0]\n",
    "\n",
    "        s_loss0, s_loss1 = sent_loss(cnn_code, sent_emb,\n",
    "                                     match_labels, class_ids, batch_size)\n",
    "        s_loss = (s_loss0 + s_loss1) * \\\n",
    "                 cfg.TRAIN.SMOOTH.LAMBDA\n",
    "        # err_sent = err_sent + s_loss.data[0]\n",
    "\n",
    "        errG_total += w_loss + s_loss\n",
    "        logs += 'w_loss: %.2f s_loss: %.2f ' % (w_loss.data[0], s_loss.data[0])\n",
    "    return errG_total, logs"
   ],
   "id": "25dd0841e7197ba0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
