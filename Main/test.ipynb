{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T18:53:43.098976Z",
     "start_time": "2024-09-30T18:53:40.257989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.nn import BCELoss, CrossEntropyLoss\n",
    "from extractor import addImagePath, textExtraction, imageExtraction, textExtractReverse"
   ],
   "id": "7123375a60e24a10",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Check the max length of the text data\n",
   "id": "3dae9bb9554dc2e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T16:02:01.441549Z",
     "start_time": "2024-09-30T16:02:01.381097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subUrlList = ['wendys','mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada','sonicdrivein']\n",
    "max_length = 0\n",
    "sum_length = 0\n",
    "for subUrl in subUrlList:\n",
    "    dirPath = '../Data/Instagram/Filter_' + subUrl + '.csv'\n",
    "    data = pd.read_csv(dirPath)\n",
    "    \n",
    "    for i in range(len(data['caption'])):\n",
    "        sum_length += len(str(data['caption'][i]).split())\n",
    "        if len(str(data['caption'][i]).split()) > max_length:\n",
    "            max_length = len(str(data['caption'][i]).split())\n",
    "            word = data['caption'][i]\n",
    "print(max_length)\n",
    "print(sum_length/len(data['caption']))"
   ],
   "id": "32667357b99a51a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373\n",
      "31.870794078061913\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:35:01.489143Z",
     "start_time": "2024-09-23T23:34:44.345758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_length = 0\n",
    "word = ''\n",
    "dirPath = '../Data/Oxford_HIC/oxford_hic_data.csv'\n",
    "data = pd.read_csv(dirPath)\n",
    "max_length = 0\n",
    "sum_length = 0\n",
    "index =0 \n",
    "counter  = 5\n",
    "# find the max word count of the text data['caption']\n",
    "for i in range(len(data['caption'])):\n",
    "    sum_length += len(str(data['caption'][i]).split())\n",
    "    if len(str(data['caption'][i]).split()) > max_length:\n",
    "        max_length = len(str(data['caption'][i]).split())\n",
    "        word = data['caption'][i]\n",
    "        index = i       \n",
    "        #\n",
    "print(max_length, i)\n",
    "print(sum_length/len(data['caption']))\n",
    "data.shape"
   ],
   "id": "b3b8a1b3461c9aed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5988\\3251324240.py:4: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(dirPath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9729 3657846\n",
      "10.514508397972905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3657847, 3)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 193
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:35:22.129734Z",
     "start_time": "2024-09-23T23:35:22.126848Z"
    }
   },
   "cell_type": "code",
   "source": "sum_length",
   "id": "21d6947ca3cd9281",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38460463"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 195
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Check the max word count of the text data",
   "id": "ca059487768954a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T18:33:30.989254Z",
     "start_time": "2024-09-25T18:33:28.914261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subUrlList = ['wendys','mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada','sonicdrivein']\n",
    "wordList = []\n",
    "total = 0\n",
    "for subUrl in subUrlList:\n",
    "    dirPath = '../Data/Instagram/Filter_' + subUrl + '.csv'\n",
    "    data = pd.read_csv(dirPath)\n",
    "    \n",
    "    for i in range(len(data['caption'])):\n",
    "        for word in str(data['caption'][i]).split():\n",
    "            if word not in wordList:\n",
    "                wordList.append(word)\n",
    "                total += 1\n",
    "                \n",
    "print(len(wordList))        \n",
    "print(total)"
   ],
   "id": "b2b206fcf154474e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15056\n",
      "15056\n"
     ]
    }
   ],
   "execution_count": 260
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-25T18:33:33.703355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wordList = []\n",
    "total = 0\n",
    "dirPath = '../Data/Oxford_HIC/oxford_hic_data.csv'\n",
    "data = pd.read_csv(dirPath)\n",
    "\n",
    "for i in range(len(data['caption'])):\n",
    "    for word in str(data['caption'][i]).split():\n",
    "        if word not in wordList:\n",
    "            wordList.append(word)\n",
    "            total += 1\n",
    "\n",
    "print(len(wordList))        \n",
    "print(total)"
   ],
   "id": "a02f3990a97e4a87",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5988\\496614120.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(dirPath)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[261], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(dirPath)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaption\u001B[39m\u001B[38;5;124m'\u001B[39m])):\n\u001B[1;32m----> 5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaption\u001B[39m\u001B[38;5;124m'\u001B[39m][i])\u001B[38;5;241m.\u001B[39msplit():\n\u001B[0;32m      6\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m wordList:\n\u001B[0;32m      7\u001B[0m             wordList\u001B[38;5;241m.\u001B[39mappend(word)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 261
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Load the data and split the data",
   "id": "6f301289c5bb2722"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T18:53:58.730167Z",
     "start_time": "2024-09-30T18:53:44.163230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if args.img - dir == 'Oxford_HIC':\n",
    "#     dirPath = '../Data/Oxford_HIC/oxford_hic_data.csv'\n",
    "#     imgPath = '../Data/Oxford_HIC/oxford_img/'\n",
    "# else:\n",
    "dirPath = '../Data/Instagram/Filter_' + 'wendys' + '.csv'\n",
    "imgPath = '../Data/Instagram/' + 'wendys' + '_img/'\n",
    "# load data\n",
    "data = pd.read_csv(dirPath)\n",
    "data = addImagePath(data, imgPath)\n",
    "# split data\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_text = textExtraction(train['caption'].tolist())\n",
    "train_image = imageExtraction(train['image_id'])\n",
    "train_funny_score = torch.tensor(train['funny_score'].to_numpy())\n",
    "test_text = textExtraction(test['caption'])\n",
    "test_image = imageExtraction(test['image_id'])\n",
    "test_funny_score = torch.tensor(test['funny_score'].to_numpy())"
   ],
   "id": "8bddfef236149dd6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 293/293 [00:00<00:00, 2844.67it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 293/293 [00:07<00:00, 37.84it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:00<00:00, 2215.98it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:01<00:00, 37.90it/s]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T18:53:58.740405Z",
     "start_time": "2024-09-30T18:53:58.736169Z"
    }
   },
   "cell_type": "code",
   "source": "train_text.shape, train_image.shape, train_funny_score.shape",
   "id": "851fa717c816958b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([293, 64, 768]), torch.Size([293, 64, 768]), torch.Size([293]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T18:53:58.763918Z",
     "start_time": "2024-09-30T18:53:58.760498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "train_dataset = torch.utils.data.TensorDataset(train_text, train_image, train_funny_score)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_text, test_image, test_funny_score)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "127ff8b78bd6db6a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. LLM Test",
   "id": "807b418cfd673b81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T16:21:23.472827Z",
     "start_time": "2024-09-30T16:21:22.108642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### ä¸ç¢ºå®šæ˜¯å¦ç‚ºå®˜æ–¹çš„ Gemini ############################################################################\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"describeai/gemini\")\n",
    "gemini = AutoModelForSeq2SeqLM.from_pretrained(\"describeai/gemini\")\n",
    "#######################################################################################################\n",
    "gemini"
   ],
   "id": "9368441165fc2667",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:48:27.892980Z",
     "start_time": "2024-09-22T21:48:26.279195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### å®˜æ–¹çš„Gemma #########################################################################################\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\", device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "########################################################################################################"
   ],
   "id": "2b63e7fd2e5f833e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1405: UserWarning: Current model requires 4608 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "008dd7a670c14c7d8ba61fde16eba45e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T16:47:42.602569Z",
     "start_time": "2024-09-22T16:47:41.350489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gemma.to(device)\n",
    "vocab_size = 256128  # è¯æ±‡è¡¨å¤§å°\n",
    "embedding_dim = 768  # åµŒå…¥ç»´åº¦ï¼Œä¸Žä½ çš„å›¾åƒåµŒå…¥ç»´åº¦ç›¸åŒ\n",
    "text_embedding = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "\n",
    "words = \"ðŸ‘»ðŸ”¥ðŸ˜‚ðŸ˜ðŸ‘ðŸ¤¦â€â™€ï¸ðŸ¤¦â€â™‚ï¸ðŸ¤·â€â™€ï¸ðŸ¤·â€â™‚ï¸âœŒðŸ¤žðŸ˜‰ðŸ˜ŽðŸŽ¶ðŸ˜¢ðŸ’–ðŸŽ‰ðŸŒ¹ðŸ’‹ðŸ‘ðŸ±â€ðŸ’»ðŸ±â€ðŸ‰ðŸ±â€ðŸ‘“âœ”ðŸ‘€ðŸ˜ƒâœ¨ðŸ˜†ðŸ¤”ðŸ¤¢ðŸŽðŸ«¢ ha ha\"\n",
    "tokens = tokenizer(words, truncation=True, padding= 'max_length', max_length=100, return_tensors=\"pt\")\n",
    "output = text_embedding(tokens['input_ids'].to(device))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def find_closest_embeddings(x, embedding_matrix, top_k=1):\n",
    "    # Normalize both the input tensor x and the embedding matrix\n",
    "    x = F.normalize(x, dim=1)  # Normalize input tensor along feature dimension\n",
    "    embedding_matrix = F.normalize(embedding_matrix, dim=1)  # Normalize embedding matrix\n",
    "    \n",
    "    # Compute cosine similarity between x and embedding matrix\n",
    "    similarity = torch.matmul(x, embedding_matrix.T)  # Shape: [10, 50265]\n",
    "    \n",
    "    # Find top-k closest embeddings for each tensor in x\n",
    "    top_k_values, top_k_indices = torch.topk(similarity, top_k, dim=1)\n",
    "    \n",
    "    return top_k_indices, top_k_values\n",
    "\n",
    "\n",
    "# print(output.squeeze(0).shape)\n",
    "top_k_indices, top_k_values = find_closest_embeddings(output.squeeze(0), text_embedding.weight)\n",
    "# top_k_indices.shape\n",
    "indices = tokenizer.decode(top_k_indices.squeeze(-1))\n",
    "print(indices)"
   ],
   "id": "b5a1bd08816d845f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 768])\n",
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><bos>ðŸ‘»ðŸ”¥ðŸ˜‚ðŸ˜ðŸ‘ðŸ¤¦â€â™€ï¸ðŸ¤¦â€â™‚ï¸ðŸ¤·â€â™€ï¸ðŸ¤·â€â™‚ï¸âœŒðŸ¤žðŸ˜‰ðŸ˜ŽðŸŽ¶ðŸ˜¢ðŸ’–ðŸŽ‰ðŸŒ¹ðŸ’‹ðŸ‘ðŸ±â€ðŸ’»ðŸ±â€ðŸ‰ðŸ±â€ðŸ‘“âœ”ðŸ‘€ðŸ˜ƒâœ¨ðŸ˜†ðŸ¤”ðŸ¤¢ðŸŽðŸ«¢ ha ha\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T20:36:50.602442Z",
     "start_time": "2024-09-23T20:36:50.584473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = \"ðŸ‘»ðŸ”¥ðŸ˜‚ðŸ˜ðŸ‘ðŸ¤¦â€â™€ï¸ðŸ¤¦â€â™‚ï¸ðŸ¤·â€â™€ï¸ðŸ¤·â€â™‚ï¸âœŒðŸ¤žðŸ˜‰ðŸ˜ŽðŸŽ¶ðŸ˜¢ðŸ’–ðŸŽ‰ðŸŒ¹ðŸ’‹ðŸ‘ðŸ±â€ðŸ’»ðŸ±â€ðŸ‰ðŸ±â€ðŸ‘“âœ”ðŸ‘€ðŸ˜ƒâœ¨ðŸ˜†ðŸ¤”ðŸ¤¢ðŸŽðŸ«¢ ha ha\"\n",
    "tokens = tokenizer(words, truncation=True, padding= 'max_length', max_length=100, return_tensors=\"pt\")\n",
    "tokens"
   ],
   "id": "9a5b2095330cfcd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      2, 242538, 237638, 236471, 238429, 237019, 240525,  67292,\n",
       "         240525,  68399, 239921,  67292, 239921,  68399, 239529, 241807, 238309,\n",
       "         238859, 240438, 240116, 239208, 239548, 240315, 240887, 238499, 242993,\n",
       "         235879, 242482, 242993, 235879, 245092, 242993, 235879, 246943, 237488,\n",
       "         239220, 239938, 236309, 239312, 238918, 241769, 241227, 248165,    661,\n",
       "            661]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:48:50.626449Z",
     "start_time": "2024-09-22T21:48:50.445244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_text = \"Give me three best book.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "\n",
    "# outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "# print(tokenizer.decode(outputs[0]))"
   ],
   "id": "4869cf8b21256080",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m tokenizer(input_text, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      4\u001B[0m input_ids\n\u001B[1;32m----> 6\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mgemma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(tokenizer\u001B[38;5;241m.\u001B[39mdecode(outputs[\u001B[38;5;241m0\u001B[39m]))\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2024\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[0;32m   2016\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[0;32m   2017\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   2018\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[0;32m   2019\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[0;32m   2020\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   2021\u001B[0m     )\n\u001B[0;32m   2023\u001B[0m     \u001B[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[1;32m-> 2024\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2025\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2026\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2027\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_warper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_warper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2028\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2029\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2030\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2031\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2032\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2033\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2035\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SAMPLE, GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SEARCH):\n\u001B[0;32m   2036\u001B[0m     \u001B[38;5;66;03m# 11. prepare logits warper\u001B[39;00m\n\u001B[0;32m   2037\u001B[0m     prepared_logits_warper \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2038\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_logits_warper(generation_config, device\u001B[38;5;241m=\u001B[39minput_ids\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m   2039\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m generation_config\u001B[38;5;241m.\u001B[39mdo_sample\n\u001B[0;32m   2040\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   2041\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2982\u001B[0m, in \u001B[0;36mGenerationMixin._sample\u001B[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001B[0m\n\u001B[0;32m   2979\u001B[0m model_inputs\u001B[38;5;241m.\u001B[39mupdate({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_hidden_states\u001B[39m\u001B[38;5;124m\"\u001B[39m: output_hidden_states} \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;28;01melse\u001B[39;00m {})\n\u001B[0;32m   2981\u001B[0m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[1;32m-> 2982\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   2984\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m synced_gpus \u001B[38;5;129;01mand\u001B[39;00m this_peer_finished:\n\u001B[0;32m   2985\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# don't waste resources running the code we don't need\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:1074\u001B[0m, in \u001B[0;36mGemmaForCausalLM.forward\u001B[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[0m\n\u001B[0;32m   1071\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m   1073\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[1;32m-> 1074\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1075\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1076\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1077\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1078\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1079\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1080\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1081\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1082\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1083\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1084\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1085\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1087\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1088\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head(hidden_states)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:827\u001B[0m, in \u001B[0;36mGemmaModel.forward\u001B[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[0m\n\u001B[0;32m    824\u001B[0m     use_cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    826\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 827\u001B[0m     inputs_embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    829\u001B[0m return_legacy_cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# noqa: F841\u001B[39;00m\n\u001B[0;32m    830\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    831\u001B[0m     use_cache \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(past_key_values, Cache) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining\n\u001B[0;32m    832\u001B[0m ):  \u001B[38;5;66;03m# kept for BC (non `Cache` `past_key_values` inputs)\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:164\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    166\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2267\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2261\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[0;32m   2262\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[0;32m   2263\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m   2264\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[0;32m   2265\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[0;32m   2266\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2267\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T19:03:27.261799Z",
     "start_time": "2024-09-23T19:03:27.257163Z"
    }
   },
   "cell_type": "code",
   "source": "gemma",
   "id": "f26de7efeee68a71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Generator",
   "id": "d8c8fefd0a84950b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T18:54:02.222130Z",
     "start_time": "2024-09-30T18:53:58.776833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\", device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "gemma"
   ],
   "id": "3de2433a8664699a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5f2fe9ebe09746e8903f3f528719b9e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T18:54:02.263014Z",
     "start_time": "2024-09-30T18:54:02.254186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # transformer\n",
    "        self.self_att = nn.MultiheadAttention(768, 1)\n",
    "        self.multi_att = nn.MultiheadAttention(768, 8)\n",
    "        self.layer_norm = nn.LayerNorm(768)\n",
    "        self.linear = nn.Linear(768, 768)\n",
    "        # gemma\n",
    "        self.linear_bf_gemma = nn.Linear(768, 50265)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.gemma = nn.Sequential(*list(gemma.children())[:-1])\n",
    "        self.linear_af_gemma = nn.Linear(256, 64)\n",
    "        self.lm_head = nn.Sequential(*list(gemma.children())[1:])\n",
    "        self.linearFunnyScore1 = nn.Linear(768, 1)\n",
    "        self.linearFunnyScore2 = nn.Linear(64, 1)\n",
    "        \n",
    "    \n",
    "    def selfAttention(self, x):\n",
    "        self_out,_ = self.self_att(x, x, x)\n",
    "        self_out = self.layer_norm(self_out + x)\n",
    "        return self_out\n",
    "    \n",
    "    def multiheadAttention(self, x):\n",
    "        multi_out,_ = self.multi_att(x, x, x)\n",
    "        multi_out = self.linear(multi_out)\n",
    "        multi_out = self.layer_norm(multi_out + x)\n",
    "        return multi_out\n",
    "    \n",
    "    def coAttention(self, x, y):\n",
    "        # x: self, y: another\n",
    "        co_out,_ = self.multi_att(x, y, y)\n",
    "        co_out = self.linear(co_out)\n",
    "        co_out = self.layer_norm(co_out + y)\n",
    "        return co_out\n",
    "    \n",
    "    def feedForward(self, x):\n",
    "        ff_out = self.linear(x)\n",
    "        ff_out = self.layer_norm(ff_out + x)\n",
    "        return ff_out\n",
    "    \n",
    "    def transformer(self, text, image):\n",
    "        # self attention module\n",
    "        image = self.selfAttention(image)\n",
    "        image = self.feedForward(image)\n",
    "        # multihead attention module\n",
    "        text = self.multiheadAttention(text)\n",
    "        # co-attention module\n",
    "        visual_attending_textual = self.coAttention(image, text)\n",
    "        textual_attending_visual = self.coAttention(text, image)\n",
    "        \n",
    "        return visual_attending_textual, textual_attending_visual\n",
    "        \n",
    "    def gemmaGenerate(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.linear_bf_gemma(x)\n",
    "            x = self.softmax(x)\n",
    "            # get max value of each row, total 32*64\n",
    "            top_k_values, top_k_indices = torch.topk(x, 1, dim=2, largest=True)\n",
    "            toGemma = textExtractReverse(top_k_indices).to(device)\n",
    "            \n",
    "            # ä½¿ç”¨gemmaä½œç‚ºmodelçš„ä¸€éƒ¨åˆ†\n",
    "            output = self.gemma(toGemma)\n",
    "            # output[0] = last_hidden_state\n",
    "            # output[1] = past_key_values\n",
    "            \n",
    "        return output[0]\n",
    "               \n",
    "    \n",
    "    def forward(self, text, image):\n",
    "        max_seq_len = max(text.shape[1], image.shape[1])\n",
    "        text = nn.functional.pad(text, (0, 0, 0, max_seq_len - text.shape[1]))\n",
    "        image = nn.functional.pad(image, (0, 0, 0, max_seq_len - image.shape[1]))\n",
    "        text = text.transpose(0, 1)\n",
    "        image = image.transpose(0, 1)\n",
    "        \n",
    "        visual_attending_textual, textual_attending_visual = self.transformer(text, image)\n",
    "        \n",
    "        feature_fusion = visual_attending_textual + textual_attending_visual\n",
    "        feature_fusion = self.feedForward(feature_fusion)        \n",
    "        feature_fusion = feature_fusion.squeeze(-1)\n",
    "        feature_fusion = feature_fusion.transpose(0, 1)\n",
    "        # gemma generate\n",
    "        last_hidden_state = self.gemmaGenerate(feature_fusion)\n",
    "        # linear_af_gemma = nn.Linear(last_hidden_state.shape[-2], 64)\n",
    "        output_text = last_hidden_state.transpose(1, 2).to(torch.float32)\n",
    "        output_text = self.linear_af_gemma(output_text).transpose(1, 2).to(torch.bfloat16)\n",
    "        output_text = self.lm_head(output_text)\n",
    "        # funny score\n",
    "        output_funny_score = self.linearFunnyScore1(feature_fusion).squeeze(-1)\n",
    "        output_funny_score = self.linearFunnyScore2(output_funny_score)\n",
    "\n",
    "        return output_text, output_funny_score\n",
    "    \n",
    "    def generate(self, image, tokenizer, max_length=100):\n",
    "        result_caption = []\n",
    "        text = torch.zeros_like(image)\n",
    "        \n",
    "        # æœ‰æ™‚å¾Œç©ºæ ¼æœƒå¤±æ•ˆï¼Œæ‰€ä»¥æ‰‹å‹•æ’å…¥ç©ºæ ¼\n",
    "        def insert_zeros(list):\n",
    "            # <pad> = 0, <eos> = 1, <bos> = 2, <unk> = 3, <mask> = 4, <2mass> = 5, [@BOS@] = 6\n",
    "            zeros = [0] * (2 * len(list) - 1)\n",
    "            zeros[::2] = list\n",
    "            return zeros\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                text = text.transpose(0, 1)\n",
    "                image = image.transpose(0, 1)\n",
    "                visual_attending_textual, textual_attending_visual = self.transformer(text, image)\n",
    "                feature_fusion = visual_attending_textual + textual_attending_visual\n",
    "                feature_fusion = self.feedForward(feature_fusion)        \n",
    "                feature_fusion = feature_fusion.squeeze(-1)\n",
    "                feature_fusion = feature_fusion.transpose(0, 1)\n",
    "                last_hidden_state = self.gemmaGenerate(feature_fusion)\n",
    "                output_text = self.lm_head(last_hidden_state)\n",
    "\n",
    "                predicted = output_text.argmax(1)\n",
    "                result_caption.append(predicted.item()) # not sure\n",
    "                \n",
    "                text = insert_zeros(result_caption)\n",
    "                text = tokenizer.decode(text, skip_special_tokens=False)\n",
    "                text = text.replace(\"<pad>\", \" \").replace(\"  \", \" \").split()\n",
    "                text = [word for word in text if word[0] != \"<\"]\n",
    "                text = \" \".join(text)\n",
    "                \n",
    "                if predicted.item() == 1: #<eos> = 1 \n",
    "                    break\n",
    "                else:\n",
    "                    text = textExtraction([text])\n",
    "                    max_seq_len = max(text.shape[1], image.shape[1])\n",
    "                    text = nn.functional.pad(text, (0, 0, 0, max_seq_len - text.shape[1]))\n",
    "                    image = nn.functional.pad(image, (0, 0, 0, max_seq_len - image.shape[1]))\n",
    "            \n",
    "        return text"
   ],
   "id": "6df9340e53bfbe2a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Discriminator",
   "id": "e431f844eea97c94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T19:48:30.831767Z",
     "start_time": "2024-09-30T19:48:30.824362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.linearFake = nn.Linear(256000, 768)\n",
    "        self.con_mlp1 = nn.Linear(768, 1)\n",
    "        self.con_mlp2 = nn.Linear(128, 1)\n",
    "        self.con_mlp_cd = nn.Linear(256, 1)\n",
    "        self.unc_mlp1 = nn.Linear(768, 1)\n",
    "        self.unc_mlp2 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, real_text, fake_text, image):   \n",
    "        # real_text = [batch_size, 64, 768]\n",
    "        # fake_text = [batch_size, 64, 256000]\n",
    "        # image = [batch_size, 64, 768]\n",
    "        fake_text = self.linearFake(fake_text)\n",
    "        mismatched_text = torch.roll(real_text, 1, 0)\n",
    "        #### conditional (contrastive) ####\n",
    "        C_r = torch.cat((real_text, image), dim=1)\n",
    "        C_g = torch.cat((fake_text, image), dim=1)\n",
    "        C_m = torch.cat((mismatched_text, image), dim=1)\n",
    "        # contrastive discriminator\n",
    "        C_r = torch.cat((C_r, C_g), dim=1)\n",
    "        # con_mlp1 \n",
    "        C_r = self.con_mlp1(C_r).squeeze(-1)\n",
    "        C_g = self.con_mlp1(C_g).squeeze(-1)\n",
    "        C_m = self.con_mlp1(C_m).squeeze(-1)\n",
    "        # con_mlp2/ con_mlp_cd\n",
    "        C_r = self.con_mlp_cd(C_r).squeeze(-1).unsqueeze(0)\n",
    "        C_g = self.con_mlp2(C_g).squeeze(-1).unsqueeze(0)\n",
    "        C_m = self.con_mlp2(C_m).squeeze(-1).unsqueeze(0)\n",
    "        # con_output\n",
    "        con_output = torch.cat((C_r, C_g, C_m), dim=0)\n",
    "        \n",
    "        #### unconditional ####\n",
    "        # unc_mlp1\n",
    "        print(\"Before MLP:\", real_text.shape, fake_text.shape, mismatched_text.shape)\n",
    "        # Before MLP: torch.Size([32, 64, 768]) torch.Size([32, 64, 768]) torch.Size([32, 64, 768])\n",
    "        UC_r = self.unc_mlp1(real_text).squeeze(-1)\n",
    "        UC_g = self.unc_mlp1(fake_text).squeeze(-1)\n",
    "        UC_m = self.unc_mlp1(mismatched_text).squeeze(-1)\n",
    "\n",
    "        ##### æ‹†è§£å•é¡Œç”¨ #####\n",
    "        print(\"Before MLP:\", UC_r.shape, UC_g.shape, UC_m.shape)\n",
    "        # Before MLP: torch.Size([32, 64]) torch.Size([32, 64]) torch.Size([32, 64])\n",
    "        UC_r_2 = self.unc_mlp2(UC_r.clone())\n",
    "        UC_g_2 = self.unc_mlp2(UC_g.clone())\n",
    "        UC_m_2 = self.unc_mlp2(UC_m.clone()) #### <----------------- Error\n",
    "        \n",
    "        print(\"After MLP:\", UC_r_2.shape, UC_g_2.shape, UC_m_2.shape)\n",
    "        # After MLP: torch.Size([32, 1]) torch.Size([32, 1]) torch.Size([32, 1])\n",
    "        UC_m_3 = UC_m_2.clone().squeeze(-1).unsqueeze(0)\n",
    "        UC_g_3 = UC_g_2.clone().squeeze(-1).unsqueeze(0)\n",
    "        UC_r_3 = UC_r_2.clone().squeeze(-1).unsqueeze(0)\n",
    "        unc_output = torch.cat((UC_r_3, UC_g_3, UC_m_3), dim=0)\n",
    "        print(\"After squeeze and unsqueeze:\", UC_r_3.shape, UC_g_3.shape, UC_m_3.shape)\n",
    "        # After squeeze and unsqueeze: torch.Size([1, 32, 1]) torch.Size([1, 32, 1]) torch.Size([1, 32, 1])\n",
    "        ##### åŽŸå§‹ Code #####\n",
    "        # UC_r = self.unc_mlp2(UC_r.clone()).squeeze(-1).unsqueeze(0)\n",
    "        # UC_g = self.unc_mlp2(UC_g.clone()).squeeze(-1).unsqueeze(0)\n",
    "        # UC_m = self.unc_mlp2(UC_m.clone()).squeeze(-1).unsqueeze(0)\n",
    "        # unc_output\n",
    "        # unc_output = torch.cat((UC_r, UC_g, UC_m), dim=0)\n",
    "        print(unc_output.shape)\n",
    "        # torch.Size([3, 32, 1])\n",
    "        return con_output, unc_output"
   ],
   "id": "ff75f88ea94b5045",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T19:48:31.376526Z",
     "start_time": "2024-09-30T19:48:31.280705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# empty cuda memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "id": "d5c9b5f31e3aa208",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T19:48:32.455218Z",
     "start_time": "2024-09-30T19:48:31.664298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NetG = Generator().to(device)\n",
    "NetD = Discriminator().to(device)\n",
    "optimizer_G = optim.Adam(NetG.parameters(), lr=0.001)\n",
    "optimizer_D = optim.Adam(NetD.parameters(), lr=0.001)\n",
    "\n",
    "def generatorLoss(uncondition_logits, condition_logits):\n",
    "    m = nn.Sigmoid()\n",
    "    result_fake = (torch.zeros(batch_size)).to(device)\n",
    "    unc_loss = BCELoss()(m(uncondition_logits[1]), result_fake)\n",
    "    con_loss = BCELoss()(m(condition_logits[1]), result_fake)\n",
    "    loss = con_loss + unc_loss\n",
    "    return loss\n",
    "\n",
    "def discriminatorLoss(uncondition_logits, condition_logits):\n",
    "    m = nn.Sigmoid()\n",
    "    result_true = (torch.ones(batch_size)).to(device)\n",
    "    result_fake = (torch.zeros(batch_size)).to(device)\n",
    "    unc_r = BCELoss()(m(condition_logits[0]), result_true)\n",
    "    unc_f = BCELoss()(m(condition_logits[1]), result_fake)\n",
    "    unc_m = BCELoss()(m(condition_logits[2]), result_fake)\n",
    "    con_r = CrossEntropyLoss()(uncondition_logits[0], result_true)\n",
    "    con_f = CrossEntropyLoss()(uncondition_logits[1], result_fake)\n",
    "    con_m = CrossEntropyLoss()(uncondition_logits[2], result_fake)\n",
    "    loss = unc_r + ((unc_f + unc_m)/2) + con_r + ((con_f + con_m)/2)\n",
    "    return loss\n",
    "\n",
    "train_losses_G = []\n",
    "train_losses_D = []\n",
    "test_losses_G = []\n",
    "test_losses_D = []"
   ],
   "id": "bfd7b3c0683d760f",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T19:48:43.874468Z",
     "start_time": "2024-09-30T19:48:33.271889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 10\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(epochs):\n",
    "    train_loss_G = 0\n",
    "    train_loss_D = 0\n",
    "    test_loss_G = 0\n",
    "    test_loss_D = 0\n",
    "    \n",
    "    ###################################### Train ######################################\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for text, image, funny_score in tepoch:\n",
    "            ######################################################\n",
    "                # (1) Generate fake caption\n",
    "            ######################################################\n",
    "            logits, output_funny_score = NetG(text.to(device).to(torch.float32), image.to(device).to(torch.float32))\n",
    "            con_logits, unc_logits = NetD(text.to(device).to(torch.float32), logits.detach().to(torch.float32), image.to(device).to(torch.float32))\n",
    "            ######################################################\n",
    "                # (2) Update Discriminator network\n",
    "            #####################################################\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D = discriminatorLoss(unc_logits, con_logits)\n",
    "            loss_D.backward(retain_graph=True)\n",
    "            optimizer_D.step()\n",
    "            train_loss_D += loss_D.item()\n",
    "            ######################################################\n",
    "                # (3) Update Generator network\n",
    "            ######################################################\n",
    "            optimizer_G.zero_grad()\n",
    "            loss_G = generatorLoss(unc_logits, con_logits)\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            train_loss_G += loss_G.item()\n",
    "            ######################################################\n",
    "            tepoch.set_postfix('G_loss: ', train_loss_G, ' , D_loss: ', train_loss_D)\n",
    "    train_losses_G.append(train_loss_G)\n",
    "    train_losses_D.append(train_loss_D)\n",
    "    ###################################### Train ######################################\n",
    "    \n",
    "    \n",
    "    ######################################  Test ######################################\n",
    "    with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
    "        for text, image, funny_score in tepoch:\n",
    "            # Generator\n",
    "            logits, output_funny_score = NetG(text.to(device).to(torch.float32), image.to(device).to(torch.float32))\n",
    "            # Discriminator\n",
    "            con_logits, unc_logits = NetD(text.to(device).to(torch.float32), logits.to(torch.float32), image.to(device).to(torch.float32))\n",
    "            # loss\n",
    "            loss_G = generatorLoss(unc_logits, con_logits)\n",
    "            loss_D = discriminatorLoss(unc_logits, con_logits)\n",
    "            test_loss_G += loss_G.item()\n",
    "            test_loss_D += loss_D.item()\n",
    "            tepoch.set_postfix('G_loss: ', test_loss_G, ' , D_loss: ', test_loss_D)\n",
    "    test_losses_G.append(test_loss_G)\n",
    "    test_losses_D.append(test_loss_D)\n",
    "    ######################################  Test ######################################\n"
   ],
   "id": "8418eb4f49f3759b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?batch/s]C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before MLP: torch.Size([32, 64, 768]) torch.Size([32, 64, 768]) torch.Size([32, 64, 768])\n",
      "Before MLP: torch.Size([32, 1, 64]) torch.Size([32, 1, 64]) torch.Size([32, 1, 64])\n",
      "After MLP: torch.Size([32, 1, 1]) torch.Size([32, 1, 1]) torch.Size([32, 1, 1])\n",
      "After squeeze and unsqueeze: torch.Size([1, 32]) torch.Size([1, 32]) torch.Size([1, 32])\n",
      "torch.Size([3, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:769: UserWarning: Error detected in AddmmBackward0. Traceback of forward call that caused the error:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_14324\\1602155039.py\", line 16, in <module>\n",
      "    con_logits, unc_logits = NetD(text.to(device).to(torch.float32), logits.detach().to(torch.float32), image.to(device).to(torch.float32))\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_14324\\323382133.py\", line 50, in forward\n",
      "    UC_m_2 = self.unc_mlp2(UC_m.clone()) #### <----------------- Error\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 117, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      " (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:116.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  0%|          | 0/10 [00:10<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [64, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[79], line 30\u001B[0m\n\u001B[0;32m     28\u001B[0m optimizer_G\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     29\u001B[0m loss_G \u001B[38;5;241m=\u001B[39m generatorLoss(unc_logits, con_logits)\n\u001B[1;32m---> 30\u001B[0m \u001B[43mloss_G\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m optimizer_G\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     32\u001B[0m train_loss_G \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss_G\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    520\u001B[0m     )\n\u001B[1;32m--> 521\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 289\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    293\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    294\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    295\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    296\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    297\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    767\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    768\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 769\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    770\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    771\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    772\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    773\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mRuntimeError\u001B[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [64, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate Test",
   "id": "4c44d1e70ced399f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T21:39:29.434132Z",
     "start_time": "2024-09-25T21:39:28.070561Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 155, 0, 2, 0, 355, 0, 4, 0, 544, 0, 6, 0, 788, 0, 8, 0, 944, 0, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1000.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 373, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 302,
   "source": [
    "A = [0,155,2,355,4,544,6,788,8,944,10]\n",
    "def insert_zeros(list):\n",
    "    # <pad> = 0, <eos> = 1, <bos> = 2, <unk> = 3, <mask> = 4, <2mass> = 5, [@BOS@] = 6\n",
    "    zeros = [0] * (2 * len(list) - 1)\n",
    "    zeros[::2] = list\n",
    "    return zeros\n",
    "\n",
    "A = insert_zeros(A) \n",
    "print(A)\n",
    "text = tokenizer.decode(A, skip_special_tokens=False)\n",
    "text = text.replace(\"<pad>\", \" \").replace(\"  \", \" \").split()\n",
    "text = [word for word in text if word[0] != \"<\"]\n",
    "text = \" \".join(text)\n",
    "text = textExtraction([text])\n",
    "print(text.shape)\n"
   ],
   "id": "1256431eb698484f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
