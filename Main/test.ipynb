{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:12:17.192093Z",
     "start_time": "2024-10-02T20:12:13.890033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.nn import BCELoss, CrossEntropyLoss\n",
    "from extractor import addImagePath, textExtraction, imageExtraction, textExtractReverse"
   ],
   "id": "7123375a60e24a10",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Check the max length of the text data\n",
   "id": "3dae9bb9554dc2e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T16:02:01.441549Z",
     "start_time": "2024-09-30T16:02:01.381097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subUrlList = ['wendys','mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada','sonicdrivein']\n",
    "max_length = 0\n",
    "sum_length = 0\n",
    "for subUrl in subUrlList:\n",
    "    dirPath = '../Data/Instagram/Filter_' + subUrl + '.csv'\n",
    "    data = pd.read_csv(dirPath)\n",
    "    \n",
    "    for i in range(len(data['caption'])):\n",
    "        sum_length += len(str(data['caption'][i]).split())\n",
    "        if len(str(data['caption'][i]).split()) > max_length:\n",
    "            max_length = len(str(data['caption'][i]).split())\n",
    "            word = data['caption'][i]\n",
    "print(max_length)\n",
    "print(sum_length/len(data['caption']))"
   ],
   "id": "32667357b99a51a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373\n",
      "31.870794078061913\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:35:01.489143Z",
     "start_time": "2024-09-23T23:34:44.345758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_length = 0\n",
    "word = ''\n",
    "dirPath = '../Data/Oxford_HIC/oxford_hic_data.csv'\n",
    "data = pd.read_csv(dirPath)\n",
    "max_length = 0\n",
    "sum_length = 0\n",
    "index =0 \n",
    "counter  = 5\n",
    "# find the max word count of the text data['caption']\n",
    "for i in range(len(data['caption'])):\n",
    "    sum_length += len(str(data['caption'][i]).split())\n",
    "    if len(str(data['caption'][i]).split()) > max_length:\n",
    "        max_length = len(str(data['caption'][i]).split())\n",
    "        word = data['caption'][i]\n",
    "        index = i       \n",
    "        #\n",
    "print(max_length, i)\n",
    "print(sum_length/len(data['caption']))\n",
    "data.shape"
   ],
   "id": "b3b8a1b3461c9aed",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5988\\3251324240.py:4: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(dirPath)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9729 3657846\n",
      "10.514508397972905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3657847, 3)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 193
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T23:35:22.129734Z",
     "start_time": "2024-09-23T23:35:22.126848Z"
    }
   },
   "cell_type": "code",
   "source": "sum_length",
   "id": "21d6947ca3cd9281",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38460463"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 195
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Check the max word count of the text data",
   "id": "ca059487768954a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T18:33:30.989254Z",
     "start_time": "2024-09-25T18:33:28.914261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subUrlList = ['wendys','mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada','sonicdrivein']\n",
    "wordList = []\n",
    "total = 0\n",
    "for subUrl in subUrlList:\n",
    "    dirPath = '../Data/Instagram/Filter_' + subUrl + '.csv'\n",
    "    data = pd.read_csv(dirPath)\n",
    "    \n",
    "    for i in range(len(data['caption'])):\n",
    "        for word in str(data['caption'][i]).split():\n",
    "            if word not in wordList:\n",
    "                wordList.append(word)\n",
    "                total += 1\n",
    "                \n",
    "print(len(wordList))        \n",
    "print(total)"
   ],
   "id": "b2b206fcf154474e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15056\n",
      "15056\n"
     ]
    }
   ],
   "execution_count": 260
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-25T18:33:33.703355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wordList = []\n",
    "total = 0\n",
    "dirPath = '../Data/Oxford_HIC/oxford_hic_data.csv'\n",
    "data = pd.read_csv(dirPath)\n",
    "\n",
    "for i in range(len(data['caption'])):\n",
    "    for word in str(data['caption'][i]).split():\n",
    "        if word not in wordList:\n",
    "            wordList.append(word)\n",
    "            total += 1\n",
    "\n",
    "print(len(wordList))        \n",
    "print(total)"
   ],
   "id": "a02f3990a97e4a87",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5988\\496614120.py:2: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(dirPath)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[261], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(dirPath)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaption\u001B[39m\u001B[38;5;124m'\u001B[39m])):\n\u001B[1;32m----> 5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcaption\u001B[39m\u001B[38;5;124m'\u001B[39m][i])\u001B[38;5;241m.\u001B[39msplit():\n\u001B[0;32m      6\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m wordList:\n\u001B[0;32m      7\u001B[0m             wordList\u001B[38;5;241m.\u001B[39mappend(word)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 261
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Load the data and split the data",
   "id": "6f301289c5bb2722"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:12:34.779693Z",
     "start_time": "2024-10-02T20:12:19.948968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if args.img - dir == 'Oxford_HIC':\n",
    "#     dirPath = '../Data/Oxford_HIC/oxford_hic_data.csv'\n",
    "#     imgPath = '../Data/Oxford_HIC/oxford_img/'\n",
    "# else:\n",
    "dirPath = '../Data/Instagram/Filter_' + 'wendys' + '.csv'\n",
    "imgPath = '../Data/Instagram/' + 'wendys' + '_img/'\n",
    "# load data\n",
    "data = pd.read_csv(dirPath)\n",
    "data = addImagePath(data, imgPath)\n",
    "# split data\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_text = textExtraction(train['caption'].tolist())\n",
    "train_image = imageExtraction(train['image_id'])\n",
    "train_funny_score = torch.tensor(train['funny_score'].to_numpy())\n",
    "test_text = textExtraction(test['caption'])\n",
    "test_image = imageExtraction(test['image_id'])\n",
    "test_funny_score = torch.tensor(test['funny_score'].to_numpy())"
   ],
   "id": "8bddfef236149dd6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293/293 [00:00<00:00, 2738.30it/s]\n",
      "100%|██████████| 293/293 [00:08<00:00, 35.66it/s]\n",
      "100%|██████████| 74/74 [00:00<00:00, 2642.85it/s]\n",
      "100%|██████████| 74/74 [00:02<00:00, 32.32it/s]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:12:34.787838Z",
     "start_time": "2024-10-02T20:12:34.783695Z"
    }
   },
   "cell_type": "code",
   "source": "train_text.shape, train_image.shape, train_funny_score.shape",
   "id": "851fa717c816958b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([293, 64, 768]), torch.Size([293, 64, 768]), torch.Size([293]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:12:34.809989Z",
     "start_time": "2024-10-02T20:12:34.806877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 32\n",
    "train_dataset = torch.utils.data.TensorDataset(train_text, train_image, train_funny_score)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_text, test_image, test_funny_score)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "127ff8b78bd6db6a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. LLM Test",
   "id": "807b418cfd673b81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T16:21:23.472827Z",
     "start_time": "2024-09-30T16:21:22.108642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 不確定是否為官方的 Gemini ############################################################################\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"describeai/gemini\")\n",
    "gemini = AutoModelForSeq2SeqLM.from_pretrained(\"describeai/gemini\")\n",
    "#######################################################################################################\n",
    "gemini"
   ],
   "id": "9368441165fc2667",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:48:27.892980Z",
     "start_time": "2024-09-22T21:48:26.279195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 官方的Gemma #########################################################################################\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\", device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "########################################################################################################"
   ],
   "id": "2b63e7fd2e5f833e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1405: UserWarning: Current model requires 4608 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "008dd7a670c14c7d8ba61fde16eba45e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T16:47:42.602569Z",
     "start_time": "2024-09-22T16:47:41.350489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "gemma.to(device)\n",
    "vocab_size = 256128  # 词汇表大小\n",
    "embedding_dim = 768  # 嵌入维度，与你的图像嵌入维度相同\n",
    "text_embedding = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "\n",
    "words = \"👻🔥😂😁👍🤦‍♀️🤦‍♂️🤷‍♀️🤷‍♂️✌🤞😉😎🎶😢💖🎉🌹💋👏🐱‍💻🐱‍🐉🐱‍👓✔👀😃✨😆🤔🤢🎁🫢 ha ha\"\n",
    "tokens = tokenizer(words, truncation=True, padding= 'max_length', max_length=100, return_tensors=\"pt\")\n",
    "output = text_embedding(tokens['input_ids'].to(device))\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def find_closest_embeddings(x, embedding_matrix, top_k=1):\n",
    "    # Normalize both the input tensor x and the embedding matrix\n",
    "    x = F.normalize(x, dim=1)  # Normalize input tensor along feature dimension\n",
    "    embedding_matrix = F.normalize(embedding_matrix, dim=1)  # Normalize embedding matrix\n",
    "    \n",
    "    # Compute cosine similarity between x and embedding matrix\n",
    "    similarity = torch.matmul(x, embedding_matrix.T)  # Shape: [10, 50265]\n",
    "    \n",
    "    # Find top-k closest embeddings for each tensor in x\n",
    "    top_k_values, top_k_indices = torch.topk(similarity, top_k, dim=1)\n",
    "    \n",
    "    return top_k_indices, top_k_values\n",
    "\n",
    "\n",
    "# print(output.squeeze(0).shape)\n",
    "top_k_indices, top_k_values = find_closest_embeddings(output.squeeze(0), text_embedding.weight)\n",
    "# top_k_indices.shape\n",
    "indices = tokenizer.decode(top_k_indices.squeeze(-1))\n",
    "print(indices)"
   ],
   "id": "b5a1bd08816d845f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 768])\n",
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><bos>👻🔥😂😁👍🤦‍♀️🤦‍♂️🤷‍♀️🤷‍♂️✌🤞😉😎🎶😢💖🎉🌹💋👏🐱‍💻🐱‍🐉🐱‍👓✔👀😃✨😆🤔🤢🎁🫢 ha ha\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T20:36:50.602442Z",
     "start_time": "2024-09-23T20:36:50.584473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "words = \"👻🔥😂😁👍🤦‍♀️🤦‍♂️🤷‍♀️🤷‍♂️✌🤞😉😎🎶😢💖🎉🌹💋👏🐱‍💻🐱‍🐉🐱‍👓✔👀😃✨😆🤔🤢🎁🫢 ha ha\"\n",
    "tokens = tokenizer(words, truncation=True, padding= 'max_length', max_length=100, return_tensors=\"pt\")\n",
    "tokens"
   ],
   "id": "9a5b2095330cfcd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      2, 242538, 237638, 236471, 238429, 237019, 240525,  67292,\n",
       "         240525,  68399, 239921,  67292, 239921,  68399, 239529, 241807, 238309,\n",
       "         238859, 240438, 240116, 239208, 239548, 240315, 240887, 238499, 242993,\n",
       "         235879, 242482, 242993, 235879, 245092, 242993, 235879, 246943, 237488,\n",
       "         239220, 239938, 236309, 239312, 238918, 241769, 241227, 248165,    661,\n",
       "            661]]), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T21:48:50.626449Z",
     "start_time": "2024-09-22T21:48:50.445244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_text = \"Give me three best book.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "\n",
    "# outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "# print(tokenizer.decode(outputs[0]))"
   ],
   "id": "4869cf8b21256080",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cuda, whereas the model is on cpu. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cpu') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m tokenizer(input_text, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      4\u001B[0m input_ids\n\u001B[1;32m----> 6\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mgemma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(tokenizer\u001B[38;5;241m.\u001B[39mdecode(outputs[\u001B[38;5;241m0\u001B[39m]))\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m--> 116\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2024\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[0;32m   2016\u001B[0m     input_ids, model_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_expand_inputs_for_generation(\n\u001B[0;32m   2017\u001B[0m         input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   2018\u001B[0m         expand_size\u001B[38;5;241m=\u001B[39mgeneration_config\u001B[38;5;241m.\u001B[39mnum_return_sequences,\n\u001B[0;32m   2019\u001B[0m         is_encoder_decoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mis_encoder_decoder,\n\u001B[0;32m   2020\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[0;32m   2021\u001B[0m     )\n\u001B[0;32m   2023\u001B[0m     \u001B[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[1;32m-> 2024\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2025\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2026\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2027\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_warper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_warper\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2028\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2029\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2030\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2031\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2032\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2033\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2035\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SAMPLE, GenerationMode\u001B[38;5;241m.\u001B[39mBEAM_SEARCH):\n\u001B[0;32m   2036\u001B[0m     \u001B[38;5;66;03m# 11. prepare logits warper\u001B[39;00m\n\u001B[0;32m   2037\u001B[0m     prepared_logits_warper \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2038\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_logits_warper(generation_config, device\u001B[38;5;241m=\u001B[39minput_ids\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m   2039\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m generation_config\u001B[38;5;241m.\u001B[39mdo_sample\n\u001B[0;32m   2040\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   2041\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2982\u001B[0m, in \u001B[0;36mGenerationMixin._sample\u001B[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001B[0m\n\u001B[0;32m   2979\u001B[0m model_inputs\u001B[38;5;241m.\u001B[39mupdate({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_hidden_states\u001B[39m\u001B[38;5;124m\"\u001B[39m: output_hidden_states} \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;28;01melse\u001B[39;00m {})\n\u001B[0;32m   2981\u001B[0m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[1;32m-> 2982\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   2984\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m synced_gpus \u001B[38;5;129;01mand\u001B[39;00m this_peer_finished:\n\u001B[0;32m   2985\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# don't waste resources running the code we don't need\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:1074\u001B[0m, in \u001B[0;36mGemmaForCausalLM.forward\u001B[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[0m\n\u001B[0;32m   1071\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m   1073\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[1;32m-> 1074\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1075\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1076\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1077\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1078\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1079\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1080\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1081\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1082\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1083\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1084\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1085\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1087\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1088\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head(hidden_states)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:827\u001B[0m, in \u001B[0;36mGemmaModel.forward\u001B[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[0m\n\u001B[0;32m    824\u001B[0m     use_cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    826\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 827\u001B[0m     inputs_embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    829\u001B[0m return_legacy_cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m  \u001B[38;5;66;03m# noqa: F841\u001B[39;00m\n\u001B[0;32m    830\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    831\u001B[0m     use_cache \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(past_key_values, Cache) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining\n\u001B[0;32m    832\u001B[0m ):  \u001B[38;5;66;03m# kept for BC (non `Cache` `past_key_values` inputs)\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:164\u001B[0m, in \u001B[0;36mEmbedding.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    166\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2267\u001B[0m, in \u001B[0;36membedding\u001B[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[0m\n\u001B[0;32m   2261\u001B[0m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[0;32m   2262\u001B[0m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[0;32m   2263\u001B[0m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[0;32m   2264\u001B[0m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[0;32m   2265\u001B[0m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[0;32m   2266\u001B[0m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[1;32m-> 2267\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T19:03:27.261799Z",
     "start_time": "2024-09-23T19:03:27.257163Z"
    }
   },
   "cell_type": "code",
   "source": "gemma",
   "id": "f26de7efeee68a71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Generator",
   "id": "d8c8fefd0a84950b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:12:39.343090Z",
     "start_time": "2024-10-02T20:12:34.850502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\", device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "gemma"
   ],
   "id": "3de2433a8664699a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "222bb1914878461caa92d0ecec52791e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2SdpaAttention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm()\n",
       "        (post_attention_layernorm): Gemma2RMSNorm()\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:12:39.386911Z",
     "start_time": "2024-10-02T20:12:39.377609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # self attention\n",
    "        self.selfAttentionMultihead = nn.MultiheadAttention(768, 1)\n",
    "        self.selfAttentionLayerNorm = nn.LayerNorm(768)\n",
    "        self.selfAttentionLinear = nn.Linear(768, 768)\n",
    "        self.selfAttentionLayerNorm2 = nn.LayerNorm(768)\n",
    "        \n",
    "        # multihead attention\n",
    "        self.multiheadAttentionMultihead = nn.MultiheadAttention(768, 8)\n",
    "        self.multiheadAttentionLinear = nn.Linear(768, 768)\n",
    "        self.multiheadAttentionLayerNorm = nn.LayerNorm(768)\n",
    "        \n",
    "        # co-attention text\n",
    "        self.coAttentionTextMultihead = nn.MultiheadAttention(768, 1)\n",
    "        self.coAttentionTextLinear = nn.Linear(768, 768)\n",
    "        self.coAttentionTextLayerNorm = nn.LayerNorm(768)\n",
    "\n",
    "        # co-attention image\n",
    "        self.coAttentionImageMultihead = nn.MultiheadAttention(768, 1)\n",
    "        self.coAttentionImageLinear = nn.Linear(768, 768)\n",
    "        self.coAttentionImageLayerNorm = nn.LayerNorm(768)\n",
    "    \n",
    "        # feed forward\n",
    "        self.feedForwardLinear = nn.Linear(768, 768)\n",
    "        self.feedForwardLayerNorm = nn.LayerNorm(768)\n",
    "        \n",
    "        # gemma\n",
    "        self.gemmaLinearBefore = nn.Linear(768, 50265)\n",
    "        self.gemmaSoftmax = nn.Softmax(dim=2)\n",
    "        self.gemma = nn.Sequential(*list(gemma.children())[:-1])\n",
    "        self.gemmaLm_head = nn.Sequential(*list(gemma.children())[1:])\n",
    "        \n",
    "        # funny score\n",
    "        self.FunnyScorelinear1 = nn.Linear(768, 1)\n",
    "        self.FunnyScorelinear2 = nn.Linear(64, 1)\n",
    "        \n",
    "    def gemmaGenerate(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.gemmaLinearBefore(x)\n",
    "            x = self.gemmaSoftmax(x)\n",
    "            # get max value of each row, total 32*64\n",
    "            top_k_values, top_k_indices = torch.topk(x, 1, dim=2, largest=True)\n",
    "            toGemma = textExtractReverse(top_k_indices).to(device)\n",
    "            \n",
    "            # 使用gemma作為model的一部分\n",
    "            output = self.gemma(toGemma)\n",
    "            # output[0] = last_hidden_state\n",
    "            # output[1] = past_key_values\n",
    "            \n",
    "        return output[0]\n",
    "               \n",
    "    \n",
    "    def forward(self, text, image):\n",
    "        # max_seq_len = max(text.shape[1], image.shape[1])\n",
    "        # text = nn.functional.pad(text, (0, 0, 0, max_seq_len - text.shape[1]))\n",
    "        # image = nn.functional.pad(image, (0, 0, 0, max_seq_len - image.shape[1]))\n",
    "        text = text.transpose(0, 1)\n",
    "        image = image.transpose(0, 1)\n",
    "        \n",
    "        ######################### Transformer ######################### \n",
    "        # self attention module\n",
    "        self_out = self.selfAttentionMultihead(image, image, image)[0]\n",
    "        self_out = self.selfAttentionLinear(self_out)\n",
    "        self_out = self.selfAttentionLayerNorm(self_out + image)\n",
    "\n",
    "        # multihead attention module\n",
    "        multi_out = self.multiheadAttentionMultihead(text, text, text)[0]\n",
    "        multi_out = self.multiheadAttentionLinear(multi_out)\n",
    "        multi_out = self.multiheadAttentionLayerNorm(multi_out + text)\n",
    "\n",
    "        # co-attention image module\n",
    "        visual_attending_textual = self.coAttentionTextMultihead(self_out, multi_out, multi_out)[0]\n",
    "        visual_attending_textual = self.coAttentionTextLinear(visual_attending_textual)\n",
    "        visual_attending_textual = self.coAttentionTextLayerNorm(visual_attending_textual + self_out)\n",
    "        \n",
    "        # co-attention text module\n",
    "        textual_attending_visual = self.coAttentionTextMultihead(multi_out, self_out, self_out)[0]\n",
    "        textual_attending_visual = self.coAttentionTextLinear(textual_attending_visual)\n",
    "        textual_attending_visual = self.coAttentionTextLayerNorm(textual_attending_visual + multi_out)\n",
    "        ###############################################################\n",
    "        \n",
    "        # feature fusion\n",
    "        feature_fusion = visual_attending_textual + textual_attending_visual\n",
    "        feature_fusion = self.feedForwardLinear(feature_fusion)\n",
    "        feature_fusion = self.feedForwardLayerNorm(feature_fusion + feature_fusion)\n",
    "        feature_fusion = feature_fusion.squeeze(-1)\n",
    "        feature_fusion = feature_fusion.transpose(0, 1)\n",
    "        \n",
    "        ####################### gemma  generate #######################\n",
    "        last_hidden_state = self.gemmaGenerate(feature_fusion)\n",
    "        output_text = self.gemmaLm_head(last_hidden_state)\n",
    "        ###############################################################\n",
    "        \n",
    "        ######################### funny score #########################\n",
    "        output_funny_score = self.FunnyScorelinear1(feature_fusion).squeeze(-1)\n",
    "        output_funny_score = self.FunnyScorelinear2(output_funny_score).squeeze(-1)\n",
    "        ###############################################################\n",
    "\n",
    "        return output_text, output_funny_score\n",
    "    \n",
    "    def generate(self, image, tokenizer, max_length=100):\n",
    "        result_caption = []\n",
    "        text = torch.zeros_like(image)\n",
    "        \n",
    "        # 有時後空格會失效，所以手動插入空格\n",
    "        def insert_zeros(list):\n",
    "            # <pad> = 0, <eos> = 1, <bos> = 2, <unk> = 3, <mask> = 4, <2mass> = 5, [@BOS@] = 6\n",
    "            zeros = [0] * (2 * len(list) - 1)\n",
    "            zeros[::2] = list\n",
    "            return zeros\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                text = text.transpose(0, 1)\n",
    "                image = image.transpose(0, 1)\n",
    "            \n",
    "                # self attention module\n",
    "                self_out = self.selfAttentionMultihead(image, image, image)[0]\n",
    "                self_out = self.selfAttentionLinear(self_out)\n",
    "                self_out = self.selfAttentionLayerNorm(self_out + image)\n",
    "                \n",
    "                # multihead attention module\n",
    "                multi_out = self.multiheadAttentionMultihead(text, text, text)[0]\n",
    "                multi_out = self.multiheadAttentionLinear(multi_out)\n",
    "                multi_out = self.multiheadAttentionLayerNorm(multi_out + text)\n",
    "                        \n",
    "                # co-attention image module\n",
    "                visual_attending_textual = self.coAttentionTextMultihead(self_out, multi_out, multi_out)[0]\n",
    "                visual_attending_textual = self.coAttentionTextLinear(visual_attending_textual)\n",
    "                visual_attending_textual = self.coAttentionTextLayerNorm(visual_attending_textual + self_out)\n",
    "                \n",
    "                # co-attention text module\n",
    "                textual_attending_visual = self.coAttentionTextMultihead(multi_out, self_out, self_out)[0]\n",
    "                textual_attending_visual = self.coAttentionTextLinear(textual_attending_visual)\n",
    "                textual_attending_visual = self.coAttentionTextLayerNorm(textual_attending_visual + multi_out)\n",
    "                \n",
    "                # feature fusion\n",
    "                feature_fusion = visual_attending_textual + textual_attending_visual\n",
    "                feature_fusion = self.feedForwardLinear(feature_fusion)\n",
    "                feature_fusion = self.feedForwardLayerNorm(feature_fusion + feature_fusion)\n",
    "                feature_fusion = feature_fusion.squeeze(-1)\n",
    "                feature_fusion = feature_fusion.transpose(0, 1)\n",
    "                last_hidden_state = self.gemmaGenerate(feature_fusion)\n",
    "                output_text = self.lm_head(last_hidden_state)\n",
    "                    \n",
    "                predicted = output_text.argmax(1)\n",
    "                result_caption.append(predicted.item())\n",
    "                \n",
    "                text = insert_zeros(result_caption)\n",
    "                text = tokenizer.decode(text, skip_special_tokens=False)\n",
    "                text = text.replace(\"<pad>\", \" \").replace(\"  \", \" \").split()\n",
    "                text = [word for word in text if word[0] != \"<\"]\n",
    "                text = \" \".join(text)\n",
    "                \n",
    "                if predicted.item() == 1: #<eos> = 1\n",
    "                    break\n",
    "                else:\n",
    "                    text = textExtraction([text])\n",
    "                    # max_seq_len = max(text.shape[1], image.shape[1])\n",
    "                    # text = nn.functional.pad(text, (0, 0, 0, max_seq_len - text.shape[1]))\n",
    "                    # image = nn.functional.pad(image, (0, 0, 0, max_seq_len - image.shape[1]))\n",
    "            \n",
    "        return text"
   ],
   "id": "6df9340e53bfbe2a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Discriminator",
   "id": "e431f844eea97c94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:12:39.428222Z",
     "start_time": "2024-10-02T20:12:39.421937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Generator\n",
    "        self.g_linearFake1 = nn.Linear(256000, 768)\n",
    "        self.g_linearFake2 = nn.Linear(256, 64)\n",
    "        self.g_con_mlp1 = nn.Linear(768, 1)\n",
    "        self.g_con_mlp2 = nn.Linear(128, 1)\n",
    "        self.g_unc_mlp1 = nn.Linear(768, 1)\n",
    "        self.g_unc_mlp2 = nn.Linear(64, 1)\n",
    "        # Discriminator\n",
    "        self.d_linearFake1 = nn.Linear(256000, 768)\n",
    "        self.d_linearFake2 = nn.Linear(256, 64)\n",
    "        self.d_con_mlp1_r = nn.Linear(768, 1)\n",
    "        self.d_con_mlp2_r = nn.Linear(256, 1)\n",
    "        self.d_con_mlp1_g = nn.Linear(768, 1)\n",
    "        self.d_con_mlp2_g = nn.Linear(128, 1)\n",
    "        self.d_con_mlp1_m = nn.Linear(768, 1)\n",
    "        self.d_con_mlp2_m = nn.Linear(128, 1)\n",
    "        self.d_unc_mlp1_r = nn.Linear(768, 1)\n",
    "        self.d_unc_mlp2_r = nn.Linear(64, 1)\n",
    "        self.d_unc_mlp1_g = nn.Linear(768, 1)\n",
    "        self.d_unc_mlp2_g = nn.Linear(64, 1)\n",
    "        self.d_unc_mlp1_m = nn.Linear(768, 1)\n",
    "        self.d_unc_mlp2_m = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, real_text, fake_text, image):   \n",
    "        # real_text = [batch_size, 64, 768]\n",
    "        # fake_text = [batch_size, 256, 256000]\n",
    "        # image = [batch_size, 64, 768]\n",
    "        g_fake_text = self.g_linearFake1(fake_text).transpose(1, 2)\n",
    "        g_fake_text = self.g_linearFake2(g_fake_text).transpose(1, 2)\n",
    "        d_fake_text = self.d_linearFake1(fake_text).transpose(1, 2)\n",
    "        d_fake_text = self.d_linearFake2(d_fake_text).transpose(1, 2)\n",
    "        mismatched_text = torch.roll(real_text, 1, 0)\n",
    "        \n",
    "        # conditional (contrastive)\n",
    "        C_r = torch.cat((real_text, image), dim=1)\n",
    "        g_C_g = torch.cat((g_fake_text, image), dim=1)\n",
    "        d_C_g = torch.cat((d_fake_text, image), dim=1)\n",
    "        C_m = torch.cat((mismatched_text, image), dim=1)\n",
    "        # contrastive discriminator\n",
    "        d_C_r = torch.cat((C_r, d_C_g), dim=1)\n",
    "        \n",
    "        ########################## Generator ##########################\n",
    "        g_C_g = self.g_con_mlp1(g_C_g).squeeze(-1)\n",
    "        g_C_g = self.g_con_mlp2(g_C_g).squeeze(-1).unsqueeze(0) #(32x320 and 128x1)\n",
    "        ###############################################################\n",
    "        \n",
    "        ######################## Discriminator ########################\n",
    "        d_C_r = self.d_con_mlp1_r(d_C_r).squeeze(-1)\n",
    "        d_C_g = self.d_con_mlp1_g(d_C_g).squeeze(-1)\n",
    "        d_C_m = self.d_con_mlp1_m(C_m).squeeze(-1)\n",
    "        d_C_r = self.d_con_mlp2_r(d_C_r).squeeze(-1).unsqueeze(0)\n",
    "        d_C_g = self.d_con_mlp2_g(d_C_g).squeeze(-1).unsqueeze(0)\n",
    "        d_C_m = self.d_con_mlp2_m(d_C_m).squeeze(-1).unsqueeze(0)\n",
    "        d_con_output = torch.cat((d_C_r, d_C_g, d_C_m), dim=0)\n",
    "        ###############################################################\n",
    "        \n",
    "        \n",
    "        #### unconditional ####\n",
    "        ########################## Generator ##########################\n",
    "        g_UC_g = self.g_unc_mlp1(g_fake_text).squeeze(-1)\n",
    "        g_UC_g = self.g_unc_mlp2(g_UC_g).squeeze(-1).unsqueeze(0)\n",
    "        g_output = torch.cat((g_C_g, g_UC_g), dim=0)\n",
    "        ###############################################################\n",
    "        \n",
    "        ######################## Discriminator ########################\n",
    "        d_UC_r  = self.d_unc_mlp1_r(real_text).squeeze(-1)\n",
    "        d_UC_g  = self.d_unc_mlp1_g(d_fake_text).squeeze(-1)\n",
    "        d_UC_m  = self.d_unc_mlp1_m(mismatched_text).squeeze(-1)\n",
    "        d_UC_r = self.d_unc_mlp2_r(d_UC_r).squeeze(-1).unsqueeze(0)\n",
    "        d_UC_g = self.d_unc_mlp2_g(d_UC_g).squeeze(-1).unsqueeze(0)\n",
    "        d_UC_m = self.d_unc_mlp2_m(d_UC_m).squeeze(-1).unsqueeze(0)\n",
    "        d_unc_output = torch.cat((d_UC_r, d_UC_g, d_UC_m), dim=0)\n",
    "        ###############################################################\n",
    "\n",
    "        # torch.Size([3, 32, 1])\n",
    "        return g_output, d_con_output, d_unc_output"
   ],
   "id": "ff75f88ea94b5045",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:27:13.205408Z",
     "start_time": "2024-10-02T20:27:13.118951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# empty cuda memory\n",
    "import gc\n",
    "def report_gpu():\n",
    "    print(torch.cuda.list_gpu_processes())\n",
    "report_gpu()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "id": "d5c9b5f31e3aa208",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pynvml module not found, please install pynvml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:27:24.016599Z",
     "start_time": "2024-10-02T20:27:21.825693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NetG = Generator().to(device)\n",
    "NetD = Discriminator().to(device)\n",
    "optimizer_G = optim.Adam(NetG.parameters(), lr=0.001)\n",
    "optimizer_D = optim.Adam(NetD.parameters(), lr=0.001)\n",
    "funnyScoreLoss = nn.MSELoss()\n",
    "\n",
    "def generatorLoss(generator_logits):\n",
    "    m = nn.Sigmoid()\n",
    "    result_fake = (torch.zeros(generator_logits[1].shape[0])).to(device)\n",
    "    unc_loss = BCELoss()(m(generator_logits[1]), result_fake)\n",
    "    con_loss = BCELoss()(m(generator_logits[0]), result_fake)\n",
    "    loss = con_loss + unc_loss\n",
    "    return loss\n",
    "\n",
    "def discriminatorLoss(uncondition_logits, condition_logits):\n",
    "    m = nn.Sigmoid()\n",
    "    result_true = (torch.ones(condition_logits[0].shape[0])).to(device)\n",
    "    result_fake = (torch.zeros(condition_logits[0].shape[0])).to(device)\n",
    "    unc_r = BCELoss()(m(condition_logits[0]), result_true)\n",
    "    unc_f = BCELoss()(m(condition_logits[1]), result_fake)\n",
    "    unc_m = BCELoss()(m(condition_logits[2]), result_fake)\n",
    "    con_r = CrossEntropyLoss()(uncondition_logits[0], result_true)\n",
    "    con_f = CrossEntropyLoss()(uncondition_logits[1], result_fake)\n",
    "    con_m = CrossEntropyLoss()(uncondition_logits[2], result_fake)\n",
    "    loss = unc_r + ((unc_f + unc_m)/2) + con_r + ((con_f + con_m)/2)\n",
    "    return loss\n",
    "\n",
    "train_losses_FC = []\n",
    "train_losses_G = []\n",
    "train_losses_D = []\n",
    "test_losses_FC = []\n",
    "test_losses_G = []\n",
    "test_losses_D = []"
   ],
   "id": "bfd7b3c0683d760f",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:27:39.710542Z",
     "start_time": "2024-10-02T20:27:25.033206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epochs = 10\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(epochs):\n",
    "    print(\"---------------------------------- epoch \"+ str(epoch) +\" ----------------------------------\")\n",
    "    train_loss_FC = 0\n",
    "    train_loss_G = 0\n",
    "    train_loss_D = 0\n",
    "    test_loss_FC = 0\n",
    "    test_loss_G = 0\n",
    "    test_loss_D = 0\n",
    "    \n",
    "    ###################################### Train ######################################\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for text, image, funny_score in tepoch:\n",
    "            ######################################################\n",
    "                # (1) Generate fake caption\n",
    "            ######################################################\n",
    "            logits, output_funny_score = NetG(text.to(device).to(torch.float32), image.to(device).to(torch.float32))\n",
    "            gen_logits, con_logits, unc_logits = NetD(text.to(device).to(torch.float32), logits.detach().to(torch.float32), image.to(device).to(torch.float32))\n",
    "            ######################################################\n",
    "                # (3) Update Discriminator network\n",
    "            #####################################################\n",
    "            optimizer_D.zero_grad()\n",
    "            loss_D = discriminatorLoss(unc_logits, con_logits)\n",
    "            loss_D.backward(retain_graph=True)\n",
    "            optimizer_D.step()\n",
    "            train_loss_D += loss_D.item()\n",
    "            ######################################################\n",
    "                # (4) Update Generator network\n",
    "            ######################################################\n",
    "            optimizer_G.zero_grad()\n",
    "            loss_FC = funnyScoreLoss(output_funny_score, funny_score.to(device).to(torch.float32))\n",
    "            loss_FC.backward(retain_graph=True)\n",
    "            train_loss_FC += loss_FC.item()\n",
    "            loss_G = generatorLoss(gen_logits)\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            train_loss_G += loss_G.item()\n",
    "            ######################################################\n",
    "            # postfix = str(train_loss_G) + ' / ' + str(train_loss_D)\n",
    "            tepoch.set_postfix({'FC_loss': train_loss_FC, 'G_loss': train_loss_G, 'D_loss': train_loss_D})\n",
    "    train_losses_FC.append(train_loss_FC)\n",
    "    train_losses_G.append(train_loss_G)\n",
    "    train_losses_D.append(train_loss_D)\n",
    "    ###################################### Train ######################################\n",
    "    \n",
    "    \n",
    "    ######################################  Test ######################################\n",
    "    with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
    "        for text, image, funny_score in tepoch:\n",
    "            # Generator\n",
    "            logits, output_funny_score = NetG(text.to(device).to(torch.float32), image.to(device).to(torch.float32))\n",
    "            # Discriminator\n",
    "            gen_logits, con_logits, unc_logits = NetD(text.to(device).to(torch.float32), logits.detach().to(torch.float32), image.to(device).to(torch.float32))\n",
    "            # loss\n",
    "            loss_FC = funnyScoreLoss(output_funny_score, funny_score.to(device).to(torch.float32))\n",
    "            loss_G = generatorLoss(gen_logits)\n",
    "            loss_D = discriminatorLoss(unc_logits, con_logits)\n",
    "            test_loss_FC += loss_FC.item()\n",
    "            test_loss_G += loss_G.item()\n",
    "            test_loss_D += loss_D.item()\n",
    "            tepoch.set_postfix({'FC_loss': test_loss_FC, 'G_loss': test_loss_G, 'D_loss': test_loss_D})\n",
    "    test_losses_FC.append(test_loss_FC)\n",
    "    test_losses_G.append(test_loss_G)\n",
    "    test_losses_D.append(test_loss_D)\n",
    "    ######################################  Test ######################################\n"
   ],
   "id": "8418eb4f49f3759b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------- epoch 0 ----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:14<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 24.00 GiB of which 0 bytes is free. Of the allocated memory 28.27 GiB is allocated by PyTorch, and 2.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 19\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m text, image, funny_score \u001B[38;5;129;01min\u001B[39;00m tepoch:\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;66;03m######################################################\u001B[39;00m\n\u001B[0;32m     16\u001B[0m         \u001B[38;5;66;03m# (1) Generate fake caption\u001B[39;00m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;66;03m######################################################\u001B[39;00m\n\u001B[0;32m     18\u001B[0m     logits, output_funny_score \u001B[38;5;241m=\u001B[39m NetG(text\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32), image\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32))\n\u001B[1;32m---> 19\u001B[0m     gen_logits, con_logits, unc_logits \u001B[38;5;241m=\u001B[39m NetD(text\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32), \u001B[43mlogits\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m)\u001B[49m, image\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32))\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;66;03m######################################################\u001B[39;00m\n\u001B[0;32m     21\u001B[0m         \u001B[38;5;66;03m# (3) Update Discriminator network\u001B[39;00m\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;66;03m#####################################################\u001B[39;00m\n\u001B[0;32m     23\u001B[0m     optimizer_D\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 7.81 GiB. GPU 0 has a total capacity of 24.00 GiB of which 0 bytes is free. Of the allocated memory 28.27 GiB is allocated by PyTorch, and 2.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:09:41.210138Z",
     "start_time": "2024-10-02T20:09:40.455289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses_G, label='train')\n",
    "plt.plot(test_losses_G, label='test')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "65770e7128728d32",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGdCAYAAADuR1K7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAmR0lEQVR4nO3de3SU9YH/8c/knoAzMRAyBBJATSVICjUxIXTPwW2yxMsqUTjSLHJbVuoWUBtwAUWotJVVtBKvHM+uy7LKwkKtbZHi0mCVQgwQLOWu6+GOkxBpJlyTkHx/f/hj2pEkBMyTyzfv1zlzaJ75PvN8v8+Jzfs8eWbiMsYYAQAAWCKkvScAAADQmogbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYJa+8JtIeGhgadOHFC1113nVwuV3tPBwAAtIAxRqdPn1ZiYqJCQpq+PtMl4+bEiRNKSkpq72kAAIBrcPToUfXt27fJ57tk3Fx33XWSvjo5bre7nWcDAABaorq6WklJSYGf403pknFz6VdRbrebuAEAoJO50i0l3FAMAACsQtwAAACrEDcAAMAqXfKeGwAAnFJfX6+6urr2nkanFBoaqrCwsG/8MS3EDQAAreTMmTM6duyYjDHtPZVOKyYmRr1791ZERMQ1vwZxAwBAK6ivr9exY8cUExOj+Ph4PiT2KhljVFtbq5MnT+rgwYNKSUlp9oP6mkPcAADQCurq6mSMUXx8vKKjo9t7Op1SdHS0wsPDdfjwYdXW1ioqKuqaXocbigEAaEVcsflmrvVqTdBrtMI8AAAAOgziBgAAWIW4AQAAraJ///5asmRJe0+DG4oBAOjKbr/9dg0dOrRVomTbtm3q1q3bN5/UN0TcAACAJhljVF9fr7CwKydDfHx8G8zoyvi1FAAADjDG6FztxXZ5tPRDBCdNmqQPP/xQRUVFcrlccrlcWrZsmVwul377298qPT1dkZGR+sMf/qDPP/9co0aNUkJCgrp3767bbrtNv/vd74Je7+u/lnK5XPq3f/s33XfffYqJiVFKSop+/etft+ZpbhRXbgAAcMD5unoNmv9+uxx778I8xURc+Ud8UVGRPv30Uw0ePFgLFy6UJO3Zs0eSNGfOHD3//PO64YYbdP311+vo0aO666679LOf/UyRkZFavny57rnnHh04cEDJyclNHuPpp5/Wc889p8WLF+vll1/WuHHjdPjwYcXFxbXOYhvBlRsAALooj8ejiIgIxcTEyOv1yuv1KjQ0VJK0cOFC/d3f/Z1uvPFGxcXFaciQIfrBD36gwYMHKyUlRT/5yU904403XvFKzKRJk1RQUKCbbrpJzzzzjM6cOaOtW7c6ui6u3AAA4IDo8FDtXZjXbsf+pjIyMoK+PnPmjH784x/rvffe0xdffKGLFy/q/PnzOnLkSLOv8+1vfzvwv7t16ya3262KiopvPL/mEDcAADjA5XK16FdDHdXX3/U0a9YsbdiwQc8//7xuuukmRUdHa8yYMaqtrW32dcLDw4O+drlcamhoaPX5/rXOe9YBAMA3FhERofr6+iuO27x5syZNmqT77rtP0ldXcg4dOuTw7K4N99wAANCF9e/fX6WlpTp06JAqKyubvKqSkpKid955R3/84x+1c+dO/cM//IPjV2CuFXEDAEAXNmvWLIWGhmrQoEGKj49v8h6an//857r++us1fPhw3XPPPcrLy9Ott97axrNtGZdp6ZvhLVJdXS2PxyO/3y+3293e0wEAWODChQs6ePCgBgwYoKioqPaeTqfV3Hls6c9vrtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AAB0Ybfffrsee+yxVnu9SZMmKT8/v9Ve71oQNwAAwCrEDQAAXdSkSZP04YcfqqioSC6XSy6XS4cOHdLu3bt15513qnv37kpISND48eNVWVkZ2G/NmjVKS0tTdHS0evToodzcXJ09e1Y//vGP9Z//+Z/61a9+FXi93//+922+rrA2PyIAAF2BMVLdufY5dniM5HJdcVhRUZE+/fRTDR48WAsXLvxq1/BwZWZm6p/+6Z/04osv6vz585o9e7YeeOABbdy4UV988YUKCgr03HPP6b777tPp06e1adMmGWM0a9Ys7du3T9XV1fqP//gPSVJcXJyjS20McQMAgBPqzknPJLbPsZ84IUV0u+Iwj8ejiIgIxcTEyOv1SpJ++tOf6jvf+Y6eeeaZwLg333xTSUlJ+vTTT3XmzBldvHhR999/v/r16ydJSktLC4yNjo5WTU1N4PXaA3EDAAACdu7cqQ8++EDdu3e/7LnPP/9cI0eOVE5OjtLS0pSXl6eRI0dqzJgxuv7669thto0jbgAAcEJ4zFdXUNrr2NfozJkzuueee/Tss89e9lzv3r0VGhqqDRs2aMuWLfrf//1fvfzyy3ryySdVWlqqAQMGfJNZtxriBgAAJ7hcLfrVUHuLiIhQfX194Otbb71Vv/jFL9S/f3+FhTWeCS6XS9/97nf13e9+V/Pnz1e/fv30y1/+UoWFhZe9Xnvg3VIAAHRh/fv3V2lpqQ4dOqTKykpNmzZNp06dUkFBgbZt26bPP/9c77//viZPnqz6+nqVlpbqmWee0fbt23XkyBG98847OnnypFJTUwOv96c//UkHDhxQZWWl6urq2nxNxA0AAF3YrFmzFBoaqkGDBik+Pl61tbXavHmz6uvrNXLkSKWlpemxxx5TbGysQkJC5Ha79dFHH+muu+7St771Lc2bN08vvPCC7rzzTknSQw89pJtvvlkZGRmKj4/X5s2b23xNLmOMafOjtrPq6mp5PB75/X653e72ng4AwAIXLlzQwYMHNWDAAEVFRbX3dDqt5s5jS39+c+UGAABYpU3i5tVXX1X//v0VFRWlrKwsbd26tdnxq1ev1sCBAxUVFaW0tDStW7euybEPP/ywXC6XlixZ0sqzBgAAnZHjcbNq1SoVFhZqwYIF2rFjh4YMGaK8vDxVVFQ0On7Lli0qKCjQlClT9Mknnyg/P1/5+fnavXv3ZWN/+ctf6uOPP1ZiYjt9SBIAAOhwHI+bn//853rooYc0efJkDRo0SEuXLlVMTIzefPPNRscXFRXpjjvu0OOPP67U1FT95Cc/0a233qpXXnklaNzx48c1Y8YMvf322woPD3d6GQAAoJNwNG5qa2tVVlam3NzcvxwwJES5ubkqKSlpdJ+SkpKg8ZKUl5cXNL6hoUHjx4/X448/rltuueWK86ipqVF1dXXQAwAA2MnRuKmsrFR9fb0SEhKCtickJMjn8zW6j8/nu+L4Z599VmFhYXrkkUdaNI9FixbJ4/EEHklJSVe5EgAAWqYLvgm5VbXG+et075YqKytTUVGRli1bJlcL/uKpJM2dO1d+vz/wOHr0qMOzBAB0NaGhoZK++q0Frt25c1/9JfVvcsuJo39+oWfPngoNDVV5eXnQ9vLy8ib/WqjX6212/KZNm1RRUaHk5OTA8/X19Zo5c6aWLFmiQ4cOXfaakZGRioyM/IarAQCgaWFhYYqJidHJkycVHh6ukJBOd/2gXRljdO7cOVVUVCg2NjYQi9fC0biJiIhQenq6iouLlZ+fL+mr+2WKi4s1ffr0RvfJzs5WcXGxHnvsscC2DRs2KDs7W5I0fvz4Ru/JGT9+vCZPnuzIOgAAuBKXy6XevXvr4MGDOnz4cHtPp9OKjY1t8gJISzn+hzMLCws1ceJEZWRkKDMzU0uWLNHZs2cDITJhwgT16dNHixYtkiQ9+uijGjFihF544QXdfffdWrlypbZv36433nhDktSjRw/16NEj6Bjh4eHyer26+eabnV4OAABNioiIUEpKCr+aukbh4eHf6IrNJY7HzdixY3Xy5EnNnz9fPp9PQ4cO1fr16wM3DR85ciTo0t3w4cO1YsUKzZs3T0888YRSUlL07rvvavDgwU5PFQCAbywkJIQ/v9DO+NtS/G0pAAA6Bf62FAAA6JKIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWaZO4efXVV9W/f39FRUUpKytLW7dubXb86tWrNXDgQEVFRSktLU3r1q0LPFdXV6fZs2crLS1N3bp1U2JioiZMmKATJ044vQwAANAJOB43q1atUmFhoRYsWKAdO3ZoyJAhysvLU0VFRaPjt2zZooKCAk2ZMkWffPKJ8vPzlZ+fr927d0uSzp07px07duipp57Sjh079M477+jAgQO69957nV4KAADoBFzGGOPkAbKysnTbbbfplVdekSQ1NDQoKSlJM2bM0Jw5cy4bP3bsWJ09e1Zr164NbBs2bJiGDh2qpUuXNnqMbdu2KTMzU4cPH1ZycvIV51RdXS2PxyO/3y+3232NKwMAAG2ppT+/Hb1yU1tbq7KyMuXm5v7lgCEhys3NVUlJSaP7lJSUBI2XpLy8vCbHS5Lf75fL5VJsbGyjz9fU1Ki6ujroAQAA7ORo3FRWVqq+vl4JCQlB2xMSEuTz+Rrdx+fzXdX4CxcuaPbs2SooKGiy4hYtWiSPxxN4JCUlXcNqAABAZ9Cp3y1VV1enBx54QMYYvf76602Omzt3rvx+f+Bx9OjRNpwlAABoS2FOvnjPnj0VGhqq8vLyoO3l5eXyer2N7uP1els0/lLYHD58WBs3bmz2d2+RkZGKjIy8xlUAAIDOxNErNxEREUpPT1dxcXFgW0NDg4qLi5Wdnd3oPtnZ2UHjJWnDhg1B4y+FzWeffabf/e536tGjhzMLAAAAnY6jV24kqbCwUBMnTlRGRoYyMzO1ZMkSnT17VpMnT5YkTZgwQX369NGiRYskSY8++qhGjBihF154QXfffbdWrlyp7du364033pD0VdiMGTNGO3bs0Nq1a1VfXx+4HycuLk4RERFOLwkAAHRgjsfN2LFjdfLkSc2fP18+n09Dhw7V+vXrAzcNHzlyRCEhf7mANHz4cK1YsULz5s3TE088oZSUFL377rsaPHiwJOn48eP69a9/LUkaOnRo0LE++OAD3X777U4vCQAAdGCOf85NR8Tn3AAA0Pl0iM+5AQAAaGvEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrtEncvPrqq+rfv7+ioqKUlZWlrVu3Njt+9erVGjhwoKKiopSWlqZ169YFPW+M0fz589W7d29FR0crNzdXn332mZNLAAAAnYTjcbNq1SoVFhZqwYIF2rFjh4YMGaK8vDxVVFQ0On7Lli0qKCjQlClT9Mknnyg/P1/5+fnavXt3YMxzzz2nl156SUuXLlVpaam6deumvLw8XbhwwenlAACADs5ljDFOHiArK0u33XabXnnlFUlSQ0ODkpKSNGPGDM2ZM+ey8WPHjtXZs2e1du3awLZhw4Zp6NChWrp0qYwxSkxM1MyZMzVr1ixJkt/vV0JCgpYtW6bvf//7V5xTdXW1PB6P/H6/3G53K60UAAA4qaU/vx29clNbW6uysjLl5ub+5YAhIcrNzVVJSUmj+5SUlASNl6S8vLzA+IMHD8rn8wWN8Xg8ysrKavI1a2pqVF1dHfQAAAB2cjRuKisrVV9fr4SEhKDtCQkJ8vl8je7j8/maHX/p36t5zUWLFsnj8QQeSUlJ17QeAADQ8XWJd0vNnTtXfr8/8Dh69Gh7TwkAADjE0bjp2bOnQkNDVV5eHrS9vLxcXq+30X28Xm+z4y/9ezWvGRkZKbfbHfQAAAB2cjRuIiIilJ6eruLi4sC2hoYGFRcXKzs7u9F9srOzg8ZL0oYNGwLjBwwYIK/XGzSmurpapaWlTb4mAADoOsKcPkBhYaEmTpyojIwMZWZmasmSJTp79qwmT54sSZowYYL69OmjRYsWSZIeffRRjRgxQi+88ILuvvturVy5Utu3b9cbb7whSXK5XHrsscf005/+VCkpKRowYICeeuopJSYmKj8/3+nlAACADs7xuBk7dqxOnjyp+fPny+fzaejQoVq/fn3ghuAjR44oJOQvF5CGDx+uFStWaN68eXriiSeUkpKid999V4MHDw6M+Zd/+RedPXtWU6dOVVVVlf7mb/5G69evV1RUlNPLAQAAHZzjn3PTEfE5NwAAdD4d4nNuAAAA2hpxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqjsXNqVOnNG7cOLndbsXGxmrKlCk6c+ZMs/tcuHBB06ZNU48ePdS9e3eNHj1a5eXlged37typgoICJSUlKTo6WqmpqSoqKnJqCQAAoBNyLG7GjRunPXv2aMOGDVq7dq0++ugjTZ06tdl9fvSjH+k3v/mNVq9erQ8//FAnTpzQ/fffH3i+rKxMvXr10ltvvaU9e/boySef1Ny5c/XKK684tQwAANDJuIwxprVfdN++fRo0aJC2bdumjIwMSdL69et111136dixY0pMTLxsH7/fr/j4eK1YsUJjxoyRJO3fv1+pqakqKSnRsGHDGj3WtGnTtG/fPm3cuLHF86uurpbH45Hf75fb7b6GFQIAgLbW0p/fjly5KSkpUWxsbCBsJCk3N1chISEqLS1tdJ+ysjLV1dUpNzc3sG3gwIFKTk5WSUlJk8fy+/2Ki4trvckDAIBOLcyJF/X5fOrVq1fwgcLCFBcXJ5/P1+Q+ERERio2NDdqekJDQ5D5btmzRqlWr9N577zU7n5qaGtXU1AS+rq6ubsEqAABAZ3RVV27mzJkjl8vV7GP//v1OzTXI7t27NWrUKC1YsEAjR45sduyiRYvk8XgCj6SkpDaZIwAAaHtXdeVm5syZmjRpUrNjbrjhBnm9XlVUVARtv3jxok6dOiWv19vofl6vV7W1taqqqgq6elNeXn7ZPnv37lVOTo6mTp2qefPmXXHec+fOVWFhYeDr6upqAgcAAEtdVdzEx8crPj7+iuOys7NVVVWlsrIypaenS5I2btyohoYGZWVlNbpPenq6wsPDVVxcrNGjR0uSDhw4oCNHjig7Ozswbs+ePfre976niRMn6mc/+1mL5h0ZGanIyMgWjQUAAJ2bI++WkqQ777xT5eXlWrp0qerq6jR58mRlZGRoxYoVkqTjx48rJydHy5cvV2ZmpiTpn//5n7Vu3TotW7ZMbrdbM2bMkPTVvTXSV7+K+t73vqe8vDwtXrw4cKzQ0NAWRdclvFsKAIDOp6U/vx25oViS3n77bU2fPl05OTkKCQnR6NGj9dJLLwWer6ur04EDB3Tu3LnAthdffDEwtqamRnl5eXrttdcCz69Zs0YnT57UW2+9pbfeeiuwvV+/fjp06JBTSwEAAJ2IY1duOjKu3AAA0Pm06+fcAAAAtBfiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAV4gYAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGAVx+Lm1KlTGjdunNxut2JjYzVlyhSdOXOm2X0uXLigadOmqUePHurevbtGjx6t8vLyRsd++eWX6tu3r1wul6qqqhxYAQAA6Iwci5tx48Zpz5492rBhg9auXauPPvpIU6dObXafH/3oR/rNb36j1atX68MPP9SJEyd0//33Nzp2ypQp+va3v+3E1AEAQCfmMsaY1n7Rffv2adCgQdq2bZsyMjIkSevXr9ddd92lY8eOKTEx8bJ9/H6/4uPjtWLFCo0ZM0aStH//fqWmpqqkpETDhg0LjH399de1atUqzZ8/Xzk5Ofrzn/+s2NjYFs+vurpaHo9Hfr9fbrf7my0WAAC0iZb+/Hbkyk1JSYliY2MDYSNJubm5CgkJUWlpaaP7lJWVqa6uTrm5uYFtAwcOVHJyskpKSgLb9u7dq4ULF2r58uUKCWnZ9GtqalRdXR30AAAAdnIkbnw+n3r16hW0LSwsTHFxcfL5fE3uExERcdkVmISEhMA+NTU1Kigo0OLFi5WcnNzi+SxatEgejyfwSEpKuroFAQCATuOq4mbOnDlyuVzNPvbv3+/UXDV37lylpqbqwQcfvOr9/H5/4HH06FGHZggAANpb2NUMnjlzpiZNmtTsmBtuuEFer1cVFRVB2y9evKhTp07J6/U2up/X61Vtba2qqqqCrt6Ul5cH9tm4caN27dqlNWvWSJIu3S7Us2dPPfnkk3r66acbfe3IyEhFRka2ZIkAAKCTu6q4iY+PV3x8/BXHZWdnq6qqSmVlZUpPT5f0VZg0NDQoKyur0X3S09MVHh6u4uJijR49WpJ04MABHTlyRNnZ2ZKkX/ziFzp//nxgn23btukf//EftWnTJt14441XsxQAAGCpq4qblkpNTdUdd9yhhx56SEuXLlVdXZ2mT5+u73//+4F3Sh0/flw5OTlavny5MjMz5fF4NGXKFBUWFiouLk5ut1szZsxQdnZ24J1SXw+YysrKwPGu5t1SAADAXo7EjSS9/fbbmj59unJychQSEqLRo0frpZdeCjxfV1enAwcO6Ny5c4FtL774YmBsTU2N8vLy9Nprrzk1RQAAYCFHPuemo+NzbgAA6Hza9XNuAAAA2gtxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsQtwAAACrEDcAAMAqxA0AALAKcQMAAKxC3AAAAKsQNwAAwCrEDQAAsApxAwAArELcAAAAqxA3AADAKsQNAACwCnEDAACsEtbeE2gPxhhJUnV1dTvPBAAAtNSln9uXfo43pUvGzenTpyVJSUlJ7TwTAABwtU6fPi2Px9Pk8y5zpfyxUENDg06cOKHrrrtOLpervafT7qqrq5WUlKSjR4/K7Xa393SsxXluG5zntsF5bhuc52DGGJ0+fVqJiYkKCWn6zpoueeUmJCREffv2be9pdDhut5v/eNoA57ltcJ7bBue5bXCe/6K5KzaXcEMxAACwCnEDAACsQtxAkZGRWrBggSIjI9t7KlbjPLcNznPb4Dy3Dc7ztemSNxQDAAB7ceUGAABYhbgBAABWIW4AAIBViBsAAGAV4qYLOHXqlMaNGye3263Y2FhNmTJFZ86caXafCxcuaNq0aerRo4e6d++u0aNHq7y8vNGxX375pfr27SuXy6WqqioHVtA5OHGed+7cqYKCAiUlJSk6OlqpqakqKipyeikdzquvvqr+/fsrKipKWVlZ2rp1a7PjV69erYEDByoqKkppaWlat25d0PPGGM2fP1+9e/dWdHS0cnNz9dlnnzm5hE6hNc9zXV2dZs+erbS0NHXr1k2JiYmaMGGCTpw44fQyOrzW/n7+aw8//LBcLpeWLFnSyrPuZAysd8cdd5ghQ4aYjz/+2GzatMncdNNNpqCgoNl9Hn74YZOUlGSKi4vN9u3bzbBhw8zw4cMbHTtq1Chz5513Gknmz3/+swMr6BycOM///u//bh555BHz+9//3nz++efmv/7rv0x0dLR5+eWXnV5Oh7Fy5UoTERFh3nzzTbNnzx7z0EMPmdjYWFNeXt7o+M2bN5vQ0FDz3HPPmb1795p58+aZ8PBws2vXrsCYf/3XfzUej8e8++67ZufOnebee+81AwYMMOfPn2+rZXU4rX2eq6qqTG5urlm1apXZv3+/KSkpMZmZmSY9Pb0tl9XhOPH9fMk777xjhgwZYhITE82LL77o8Eo6NuLGcnv37jWSzLZt2wLbfvvb3xqXy2WOHz/e6D5VVVUmPDzcrF69OrBt3759RpIpKSkJGvvaa6+ZESNGmOLi4i4dN06f57/2wx/+0Pzt3/5t602+g8vMzDTTpk0LfF1fX28SExPNokWLGh3/wAMPmLvvvjtoW1ZWlvnBD35gjDGmoaHBeL1es3jx4sDzVVVVJjIy0vz3f/+3AyvoHFr7PDdm69atRpI5fPhw60y6E3LqPB87dsz06dPH7N692/Tr16/Lxw2/lrJcSUmJYmNjlZGREdiWm5urkJAQlZaWNrpPWVmZ6urqlJubG9g2cOBAJScnq6SkJLBt7969WrhwoZYvX97sHzDrCpw8z1/n9/sVFxfXepPvwGpra1VWVhZ0jkJCQpSbm9vkOSopKQkaL0l5eXmB8QcPHpTP5wsa4/F4lJWV1ex5t5kT57kxfr9fLpdLsbGxrTLvzsap89zQ0KDx48fr8ccf1y233OLM5DuZrv0TqQvw+Xzq1atX0LawsDDFxcXJ5/M1uU9ERMRl/weUkJAQ2KempkYFBQVavHixkpOTHZl7Z+LUef66LVu2aNWqVZo6dWqrzLujq6ysVH19vRISEoK2N3eOfD5fs+Mv/Xs1r2k7J87z1124cEGzZ89WQUFBl/0DkE6d52effVZhYWF65JFHWn/SnRRx00nNmTNHLper2cf+/fsdO/7cuXOVmpqqBx980LFjdATtfZ7/2u7duzVq1CgtWLBAI0eObJNjAq2hrq5ODzzwgIwxev3119t7OlYpKytTUVGRli1bJpfL1d7T6TDC2nsCuDYzZ87UpEmTmh1zww03yOv1qqKiImj7xYsXderUKXm93kb383q9qq2tVVVVVdBVhfLy8sA+Gzdu1K5du7RmzRpJX737RJJ69uypJ598Uk8//fQ1rqxjae/zfMnevXuVk5OjqVOnat68ede0ls6oZ8+eCg0Nveydeo2do0u8Xm+z4y/9W15ert69eweNGTp0aCvOvvNw4jxfcilsDh8+rI0bN3bZqzaSM+d506ZNqqioCLqCXl9fr5kzZ2rJkiU6dOhQ6y6is2jvm37grEs3um7fvj2w7f3332/Rja5r1qwJbNu/f3/Qja7/93//Z3bt2hV4vPnmm0aS2bJlS5N3/dvMqfNsjDG7d+82vXr1Mo8//rhzC+jAMjMzzfTp0wNf19fXmz59+jR7A+bf//3fB23Lzs6+7Ibi559/PvC83+/nhuJWPs/GGFNbW2vy8/PNLbfcYioqKpyZeCfT2ue5srIy6P+Ld+3aZRITE83s2bPN/v37nVtIB0fcdAF33HGH+c53vmNKS0vNH/7wB5OSkhL0FuVjx46Zm2++2ZSWlga2PfzwwyY5Odls3LjRbN++3WRnZ5vs7Owmj/HBBx906XdLGePMed61a5eJj483Dz74oPniiy8Cj670g2LlypUmMjLSLFu2zOzdu9dMnTrVxMbGGp/PZ4wxZvz48WbOnDmB8Zs3bzZhYWHm+eefN/v27TMLFixo9K3gsbGx5le/+pX505/+ZEaNGsVbwVv5PNfW1pp7773X9O3b1/zxj38M+v6tqalplzV2BE58P38d75YibrqEL7/80hQUFJju3bsbt9ttJk+ebE6fPh14/uDBg0aS+eCDDwLbzp8/b374wx+a66+/3sTExJj77rvPfPHFF00eg7hx5jwvWLDASLrs0a9fvzZcWft7+eWXTXJysomIiDCZmZnm448/Djw3YsQIM3HixKDx//M//2O+9a1vmYiICHPLLbeY9957L+j5hoYG89RTT5mEhAQTGRlpcnJyzIEDB9piKR1aa57nS9/vjT3++r+Brqi1v5+/jrgxxmXM/79ZAgAAwAK8WwoAAFiFuAEAAFYhbgAAgFWIGwAAYBXiBgAAWIW4AQAAViFuAACAVYgbAABgFeIGAABYhbgBAABWIW4AAIBViBsAAGCV/wcditlNe34YawAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate Test",
   "id": "4c44d1e70ced399f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-25T21:39:29.434132Z",
     "start_time": "2024-09-25T21:39:28.070561Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 155, 0, 2, 0, 355, 0, 4, 0, 544, 0, 6, 0, 788, 0, 8, 0, 944, 0, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 373, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 302,
   "source": [
    "A = [0,155,2,355,4,544,6,788,8,944,10]\n",
    "def insert_zeros(list):\n",
    "    # <pad> = 0, <eos> = 1, <bos> = 2, <unk> = 3, <mask> = 4, <2mass> = 5, [@BOS@] = 6\n",
    "    zeros = [0] * (2 * len(list) - 1)\n",
    "    zeros[::2] = list\n",
    "    return zeros\n",
    "\n",
    "A = insert_zeros(A) \n",
    "print(A)\n",
    "text = tokenizer.decode(A, skip_special_tokens=False)\n",
    "text = text.replace(\"<pad>\", \" \").replace(\"  \", \" \").split()\n",
    "text = [word for word in text if word[0] != \"<\"]\n",
    "text = \" \".join(text)\n",
    "text = textExtraction([text])\n",
    "print(text.shape)\n"
   ],
   "id": "1256431eb698484f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
