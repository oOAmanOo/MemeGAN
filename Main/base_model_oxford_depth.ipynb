{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "! pip install Pillow",
   "id": "7c6df9d905fdb8be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "! pip install --upgrade huggingface_hub",
   "id": "db628d2dd81a5c98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "! huggingface-cli login  # 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",
   "id": "dcb920958539f226"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T01:50:54.297626Z",
     "start_time": "2024-10-29T01:50:54.259891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, BCELoss\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# from local_gemma import LocalGemma2ForCausalLM\n",
    "\n",
    "from extractor import addImagePath, textExtraction, imageExtraction, textExtractReverse\n",
    "\n",
    "eps = torch.finfo(torch.float32).eps"
   ],
   "id": "7123375a60e24a10",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Load the data and split the data",
   "id": "6f301289c5bb2722"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:11:11.223572Z",
     "start_time": "2024-10-27T16:11:11.219722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class OxfordDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, image, funny_score):\n",
    "        self.text = text\n",
    "        self.image = image\n",
    "        self.funny_score = funny_score\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):    \n",
    "        imageData = torch.load('../../Oxford_HIC/ImageData/'+ self.image[idx] +'.pt', weights_only=False)\n",
    "        # all dtype to torch.float16\n",
    "        imageData = imageData.to(torch.float32)\n",
    "        \n",
    "        return self.text[idx], imageData, self.funny_score[idx]"
   ],
   "id": "6284fc6cdb32c442",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:11:13.652227Z",
     "start_time": "2024-10-27T16:11:11.310880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if args.img - dir == 'Oxford_HIC':\n",
    "dirPath = '../Data/Oxford_HIC/Filtered_oxford_hic_data.csv'\n",
    "imgPath = '../Data/Oxford_HIC/oxford_img/'\n",
    "# else:\n",
    "# dirPath = '../Data/Instagram/Filter_' + 'wendys' + '.csv'\n",
    "# imgPath = '../Data/Instagram/' + 'wendys' + '_img/'\n",
    "# load data\n",
    "data = pd.read_csv(dirPath)\n",
    "print(\"shape of data: \", data.shape)\n",
    "data = data.sample(n = 169920, random_state=42, replace=True).reset_index(drop=True)\n",
    "# frac = 0.05 ==> 5% of the data = 169904\n",
    "# n = 169920 ==> 72 * 2360 = 169920 (F2G)\n",
    "# n = 169988 ==> 91 * 1868 = 169988 (G2F)\n",
    "print(\"sample of data: \", data.shape)\n",
    "data.head()"
   ],
   "id": "6d1d47d2ceb7bd39",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data:  (3398081, 3)\n",
      "sample of data:  (169920, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      image_id                                            caption  funny_score\n",
       "0   imgflip_33                    TRIES TO ROB BANK; LEAVES MONEY     0.000000\n",
       "1  imgflip_123  THAT WAS MY SISTER'S PICTURE, WE LOOK ALMOST I...     0.000041\n",
       "2   imgflip_34  when a kid asks why 2020 is blank in the text ...     0.000071\n",
       "3   imgflip_48          OFFICIAL PARDON; PLUTO IS A PLANET AGAIN!     0.000092\n",
       "4    imgflip_4  Not every single problem is the President's fa...     0.000051"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>caption</th>\n",
       "      <th>funny_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imgflip_33</td>\n",
       "      <td>TRIES TO ROB BANK; LEAVES MONEY</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>imgflip_123</td>\n",
       "      <td>THAT WAS MY SISTER'S PICTURE, WE LOOK ALMOST I...</td>\n",
       "      <td>0.000041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>imgflip_34</td>\n",
       "      <td>when a kid asks why 2020 is blank in the text ...</td>\n",
       "      <td>0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>imgflip_48</td>\n",
       "      <td>OFFICIAL PARDON; PLUTO IS A PLANET AGAIN!</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>imgflip_4</td>\n",
       "      <td>Not every single problem is the President's fa...</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:11:13.695653Z",
     "start_time": "2024-10-27T16:11:13.678115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_text = train['caption'].tolist()\n",
    "train_image = train['image_id'].tolist()\n",
    "train_funny_score = train['funny_score'].tolist()\n",
    "test_text = test['caption'].tolist()\n",
    "test_image = test['image_id'].tolist()\n",
    "test_funny_score = test['funny_score'].tolist()\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = OxfordDataset(train_text, train_image, train_funny_score)\n",
    "test_dataset = OxfordDataset(test_text, test_image, test_funny_score)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)"
   ],
   "id": "b3a2a78c51a21fc3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:11:13.893521Z",
     "start_time": "2024-10-27T16:11:13.719884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataL = iter(train_loader)\n",
    "text, imgs, funny_score = next(dataL)\n",
    "# print(\"shape of text: \", text.shape)\n",
    "# print(\"shape of image: \", imgs.shape)\n",
    "print(\"shape of funny_score: \", funny_score.shape)"
   ],
   "id": "71e883f401a9b059",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of funny_score:  torch.Size([32])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Load Gemma",
   "id": "c43cb6bdba292329"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-28T19:54:59.346508Z",
     "start_time": "2024-10-28T18:22:08.795461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 官方的Gemma #########################################################################################\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-27b-it\")\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-27b-it\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "# gemma = LocalGemma2ForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", preset=\"auto\", torch_dtype=torch.bfloat16)\n",
    "gemmaConfig =  AutoConfig.from_pretrained('google/gemma-2-27b-it')\n",
    "########################################################################################################"
   ],
   "id": "e5f8f2c83fc25118",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TonyLab\\PycharmProjects\\MemeGAN\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\TonyLab\\.cache\\huggingface\\hub\\models--google--gemma-2-27b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 12/12 [1:31:41<00:00, 458.44s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 12/12 [00:32<00:00,  2.73s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:11:25.849807Z",
     "start_time": "2024-10-27T16:11:25.320646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2-2b-it\",\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ],
   "id": "140efabb2a2242c9",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# quantization_config = BitsAndBytesConfig(load_in_4bit=True)\u001B[39;00m\n\u001B[0;32m      2\u001B[0m quantization_config \u001B[38;5;241m=\u001B[39m BitsAndBytesConfig(load_in_8bit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m----> 4\u001B[0m gemma \u001B[38;5;241m=\u001B[39m \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgoogle/gemma-2-2b-it\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquantization_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\MemeGAN\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001B[0m, in \u001B[0;36m_BaseAutoModelClass.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m    562\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[0;32m    563\u001B[0m     model_class \u001B[38;5;241m=\u001B[39m _get_model_class(config, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping)\n\u001B[1;32m--> 564\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    565\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    566\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    567\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    568\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(c\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m_model_mapping\u001B[38;5;241m.\u001B[39mkeys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    570\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\MemeGAN\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3656\u001B[0m, in \u001B[0;36mPreTrainedModel.from_pretrained\u001B[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[0m\n\u001B[0;32m   3653\u001B[0m     hf_quantizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   3655\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 3656\u001B[0m     \u001B[43mhf_quantizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidate_environment\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3657\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtorch_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_tf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrom_tf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_flax\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfrom_flax\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice_map\u001B[49m\n\u001B[0;32m   3658\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3659\u001B[0m     torch_dtype \u001B[38;5;241m=\u001B[39m hf_quantizer\u001B[38;5;241m.\u001B[39mupdate_torch_dtype(torch_dtype)\n\u001B[0;32m   3660\u001B[0m     device_map \u001B[38;5;241m=\u001B[39m hf_quantizer\u001B[38;5;241m.\u001B[39mupdate_device_map(device_map)\n",
      "File \u001B[1;32m~\\PycharmProjects\\MemeGAN\\.venv\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_8bit.py:73\u001B[0m, in \u001B[0;36mBnb8BitHfQuantizer.validate_environment\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m     70\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `bitsandbytes` 8-bit quantization requires Accelerate: `pip install \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccelerate>=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mACCELERATE_MIN_VERSION\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     71\u001B[0m     )\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_bitsandbytes_available():\n\u001B[1;32m---> 73\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m     74\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     75\u001B[0m     )\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mintegrations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m validate_bnb_backend_availability\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001B[1;31mImportError\u001B[0m: Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:11:25.949779Z",
     "start_time": "2024-10-27T16:11:25.946018Z"
    }
   },
   "cell_type": "code",
   "source": "gemma",
   "id": "6c46669a0175d6cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=2304, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2304, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2304, bias=False)\n",
       "          (rotary_emb): Gemma2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (up_proj): Linear(in_features=2304, out_features=9216, bias=False)\n",
       "          (down_proj): Linear(in_features=9216, out_features=2304, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Generator",
   "id": "d8c8fefd0a84950b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:11:27.399554Z",
     "start_time": "2024-10-27T16:11:27.389533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class self_multi(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self_multi, self).__init__()\n",
    "        # self attention\n",
    "        self.selfAttentionMultihead = nn.MultiheadAttention(768, 1)\n",
    "        self.selfAttentionLayerNorm = nn.LayerNorm(768, eps= eps)\n",
    "        self.selfAttentionLinear = nn.Linear(768, 768)\n",
    "        self.selfAttentionLayerNorm2 = nn.LayerNorm(768, eps= eps)\n",
    "        \n",
    "        # multihead attention\n",
    "        self.multiheadAttentionMultihead = nn.MultiheadAttention(768, 8)\n",
    "        self.multiheadAttentionLinear = nn.Linear(768, 768)\n",
    "        self.multiheadAttentionLayerNorm = nn.LayerNorm(768, eps= eps)\n",
    "        \n",
    "    def forward(self, image, text):\n",
    "        # self attention module\n",
    "        self_out = self.selfAttentionMultihead(image, image, image)[0]\n",
    "        self_out = self.selfAttentionLinear(self_out)\n",
    "        self_out = self.selfAttentionLayerNorm(self_out + image)\n",
    "\n",
    "        # multihead attention module\n",
    "        multi_out = self.multiheadAttentionMultihead(text, text, text)[0]\n",
    "        multi_out = self.multiheadAttentionLinear(multi_out)\n",
    "        multi_out = self.multiheadAttentionLayerNorm(multi_out + text)\n",
    "        \n",
    "        return self_out, multi_out"
   ],
   "id": "40396b2be022a57e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:11:27.853078Z",
     "start_time": "2024-10-27T16:11:27.846121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class co_attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(co_attention, self).__init__()\n",
    "        # co-attention text\n",
    "        self.coAttentionTextMultihead = nn.MultiheadAttention(768, 1)\n",
    "        self.coAttentionTextLinear = nn.Linear(768, 768)\n",
    "        self.coAttentionTextLayerNorm = nn.LayerNorm(768, eps= eps)\n",
    "\n",
    "        # co-attention image\n",
    "        self.coAttentionImageMultihead = nn.MultiheadAttention(768, 1)\n",
    "        self.coAttentionImageLinear = nn.Linear(768, 768)\n",
    "        self.coAttentionImageLayerNorm = nn.LayerNorm(768, eps= eps)\n",
    "        \n",
    "    def forward(self, image, text):\n",
    "        # co-attention image module\n",
    "        visual_attending_textual = self.coAttentionTextMultihead(image, text, text)[0]\n",
    "        visual_attending_textual = self.coAttentionTextLinear(visual_attending_textual)\n",
    "        visual_attending_textual = self.coAttentionTextLayerNorm(visual_attending_textual + image)\n",
    "        \n",
    "        # co-attention text module\n",
    "        textual_attending_visual = self.coAttentionTextMultihead(text, image, image)[0]\n",
    "        textual_attending_visual = self.coAttentionTextLinear(textual_attending_visual)\n",
    "        textual_attending_visual = self.coAttentionTextLayerNorm(textual_attending_visual + text) \n",
    "        \n",
    "        return visual_attending_textual, textual_attending_visual"
   ],
   "id": "553cad97871b6f0a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:11:28.265646Z",
     "start_time": "2024-10-27T16:11:28.250923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, depth=12):\n",
    "        super(Generator, self).__init__()\n",
    "        self.layers_self_multi = nn.ModuleList([self_multi() for _ in range(depth)])\n",
    "        self.layers_co_attention = nn.ModuleList([co_attention() for _ in range(depth)])   \n",
    "    \n",
    "        # feed forward\n",
    "        self.feedForwardLinear = nn.Linear(768, 768)\n",
    "        self.feedForwardLayerNorm = nn.LayerNorm(768, eps= eps)\n",
    "        \n",
    "        # gemma\n",
    "        self.gemmaLinearMaxTokens = nn.Linear(64, 16)\n",
    "        self.gemmaLinearBefore = nn.Linear(768, gemmaConfig.vocab_size)\n",
    "        self.gemmaSoftmax = nn.Softmax(dim=2)\n",
    "        self.gemma = nn.Sequential(*list(gemma.children())[:-1])\n",
    "        # self.gemmaLm_head = nn.Sequential(*list(gemma.children())[1:])\n",
    "        self.gemmaLm_head = nn.Linear(2304, gemmaConfig.vocab_size)\n",
    "        \n",
    "        # funny score\n",
    "        self.FunnyScorelinear1 = nn.Linear(768, 1)\n",
    "        self.FunnyScorelinear2 = nn.Linear(64, 1)          \n",
    "        \n",
    "    def gemmaGenerate(self, x):\n",
    "        with torch.no_grad():\n",
    "            # maximum 32 tokens\n",
    "            x = self.gemmaLinearMaxTokens(x.transpose(1, 2)).transpose(1, 2)\n",
    "            x = self.gemmaLinearBefore(x)\n",
    "            x2 = self.gemmaSoftmax(x + eps)\n",
    "            \n",
    "            # get max value of each row, total 32*64\n",
    "            top_k_values, top_k_indices = torch.topk(x2, 1, dim=2, largest=True)\n",
    "            toGemma = textExtractReverse(top_k_indices).to(device)\n",
    "            # 使用gemma作為model的一部分\n",
    "            output = self.gemma(toGemma)\n",
    "            # output[0] = last_hidden_state\n",
    "            # output[1] = past_key_values\n",
    "            \n",
    "        return output[0]\n",
    "               \n",
    "    \n",
    "    def forward(self, text, image):\n",
    "        # max_seq_len = max(text.shape[1], image.shape[1])\n",
    "        # text = nn.functional.pad(text, (0, 0, 0, max_seq_len - text.shape[1]))\n",
    "        # image = nn.functional.pad(image, (0, 0, 0, max_seq_len - image.shape[1]))\n",
    "        text = text.transpose(0, 1)\n",
    "        image = image.transpose(0, 1)\n",
    "        \n",
    "        ######################### Transformer ######################### \n",
    "        for self_multi_layer in self.layers_self_multi:\n",
    "            image, text = self_multi_layer(image, text)\n",
    "        for co_attention_layer in self.layers_co_attention:\n",
    "            image, text = co_attention_layer(image, text)\n",
    "        ###############################################################\n",
    "        \n",
    "        # feature fusion\n",
    "        feature_fusion = image + text   #visual_attending_textual + textual_attending_visual\n",
    "        feature_fusion = self.feedForwardLinear(feature_fusion)\n",
    "        feature_fusion = self.feedForwardLayerNorm(feature_fusion + feature_fusion)\n",
    "        feature_fusion = feature_fusion.squeeze(-1)\n",
    "        feature_fusion = feature_fusion.transpose(0, 1)\n",
    "        ####################### gemma  generate #######################\n",
    "        last_hidden_state = self.gemmaGenerate(feature_fusion)\n",
    "        output_text = self.gemmaLm_head(last_hidden_state)        \n",
    "        ###############################################################\n",
    "        output_text = output_text.to(torch.float32)\n",
    "        ######################### funny score #########################\n",
    "        output_funny_score = self.FunnyScorelinear1(feature_fusion).squeeze(-1)\n",
    "        output_funny_score = self.FunnyScorelinear2(output_funny_score).squeeze(-1)\n",
    "        ###############################################################\n",
    "        \n",
    "        return output_text, output_funny_score\n",
    "    \n",
    "    def generate(self, image, max_length = 100):\n",
    "        generated_tokens = []\n",
    "        generated_tokens.append(2) #<bos> = 2\n",
    "        text = torch.zeros_like(image).to(device)\n",
    "        text = text.transpose(0, 1)\n",
    "        image = image.transpose(0, 1)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "        depth = len(self.layers_self_multi)\n",
    "        # 有時後空格會失效，所以手動插入空格 <pad> = 0\n",
    "        def insert_zeros(list):\n",
    "            zeros = [0] * (2 * len(list) - 1)\n",
    "            zeros[::2] = list\n",
    "            return zeros\n",
    "        \n",
    "        lastTurn = False\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length + 1):\n",
    "                # Transformer\n",
    "                for i in range(depth):\n",
    "                    # self attention\n",
    "                    image, text = self.layers_self_multi[i](image, text)\n",
    "                    # co-attention\n",
    "                    image, text = self.layers_co_attention[i](image, text)\n",
    "                \n",
    "                # feature fusion\n",
    "                feature_fusion = image + text # visual_attending_textual + textual_attending_visual\n",
    "                feature_fusion = self.feedForwardLinear(feature_fusion)\n",
    "                feature_fusion = self.feedForwardLayerNorm(feature_fusion + feature_fusion)\n",
    "                feature_fusion = feature_fusion.squeeze(-1)\n",
    "                feature_fusion = feature_fusion.transpose(0, 1)\n",
    "                \n",
    "                # gemma generate\n",
    "                last_hidden_state = self.gemmaGenerate(feature_fusion)\n",
    "                output_text = self.gemmaLm_head(last_hidden_state)\n",
    "                \n",
    "                # funny score\n",
    "                output_funny_score = self.FunnyScorelinear1(feature_fusion).squeeze(-1)\n",
    "                output_funny_score = self.FunnyScorelinear2(output_funny_score).squeeze(-1)\n",
    "                \n",
    "                if lastTurn: # show final funny score\n",
    "                    return generated_caption, output_funny_score\n",
    "                else:\n",
    "                    next_token_logits = output_text[:, -1, :]\n",
    "                    next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "                    next_token_id = torch.argmax(next_token_probs, dim=-1).item()\n",
    "                    generated_tokens.append(next_token_id)\n",
    "                    \n",
    "                    generated_caption = insert_zeros(generated_tokens)\n",
    "                    generated_caption = tokenizer.decode(generated_caption, skip_special_tokens=False)\n",
    "                    generated_caption = generated_caption.replace(\"<pad>\", \" \").replace(\"  \", \" \").split()\n",
    "                    generated_caption = [word for word in generated_caption if word[0] != \"<\"]\n",
    "                    generated_caption = \" \".join(generated_caption)\n",
    "                                               \n",
    "                    text = textExtraction([generated_caption]).to(device)\n",
    "                    text = text.transpose(0, 1)\n",
    "                    \n",
    "                    if next_token_id in gemmaConfig.eos_token_id or len(generated_caption.split()) > max_length: \n",
    "                        #<eos> = 1; <end_of_turn> = 107\n",
    "                        lastTurn = True"
   ],
   "id": "6df9340e53bfbe2a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Discriminator",
   "id": "e431f844eea97c94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.linearFake = nn.Linear(gemmaConfig.vocab_size, 768)\n",
    "        # Generator\n",
    "        self.g_con_mlp1 = nn.Linear(1536, 2)\n",
    "        self.g_con_mlp2 = nn.Linear(64, 1)\n",
    "        self.g_unc_mlp1 = nn.Linear(768, 1)\n",
    "        self.g_unc_mlp2 = nn.Linear(64, 1)\n",
    "        # Discriminator\n",
    "        self.d_linearFake = nn.Linear(gemmaConfig.vocab_size, 768)\n",
    "        self.d_con_mlp1_r2f = nn.Linear(196608, 2)\n",
    "        self.d_con_mlp2_r2f = nn.Linear(batch_size, 1)\n",
    "        self.d_con_mlp1_f2r = nn.Linear(196608, 2)\n",
    "        self.d_con_mlp2_f2r = nn.Linear(batch_size, 1)\n",
    "        self.d_con_mlp1_g = nn.Linear(1536, 2)\n",
    "        self.d_con_mlp2_g = nn.Linear(64, 1)\n",
    "        self.d_con_mlp1_m = nn.Linear(1536, 2)\n",
    "        self.d_con_mlp2_m = nn.Linear(64, 1)\n",
    "        self.d_unc_mlp1_r = nn.Linear(768, 1)\n",
    "        self.d_unc_mlp2_r = nn.Linear(64, 1)\n",
    "        self.d_unc_mlp1_g = nn.Linear(768, 1)\n",
    "        self.d_unc_mlp2_g = nn.Linear(64, 1)\n",
    "        self.d_unc_mlp1_m = nn.Linear(768, 1)\n",
    "        self.d_unc_mlp2_m = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, real_text, fake_text, image, GorD):\n",
    "        # real_text = [batch_size, 64, 768]\n",
    "        # fake_text = [batch_size, 64, 256000]\n",
    "        # image = [batch_size, 64, 768]\n",
    "        fake_text = self.linearFake(fake_text)\n",
    "        if GorD == \"G\":\n",
    "            g_C_g = torch.cat((fake_text, image), dim=-1)\n",
    "            ########################  conditional  ########################\n",
    "            g_C_g = self.g_con_mlp1(g_C_g)\n",
    "            g_C_g = self.g_con_mlp2(g_C_g.transpose(1, 2)).squeeze(-1)\n",
    "            ###############################################################\n",
    "            ######################## unconditional ########################\n",
    "            g_UC_g = self.g_unc_mlp1(fake_text).squeeze(-1)\n",
    "            g_UC_g = self.g_unc_mlp2(g_UC_g).squeeze(-1)\n",
    "            ###############################################################\n",
    "            return g_C_g, g_UC_g\n",
    "\n",
    "        elif GorD == \"D\":\n",
    "            mismatched_text = torch.roll(real_text, 1, 0)\n",
    "            C_r = torch.cat((real_text, image), dim=-1)\n",
    "            C_g = torch.cat((fake_text, image), dim=-1)\n",
    "            C_m = torch.cat((mismatched_text, image), dim=-1)\n",
    "            # contrastive discriminator\n",
    "            cd_C_r = C_r.unsqueeze(0).expand(C_r.shape[0], -1, -1, -1)\n",
    "            cd_C_g = C_g.unsqueeze(0).expand(C_g.shape[0], -1, -1, -1)\n",
    "            d_C_r2f = torch.cat((cd_C_r, cd_C_g.transpose(0, 1)), dim=-1)\n",
    "            d_C_f2r = torch.cat((cd_C_g, cd_C_r.transpose(0, 1)), dim=-1)\n",
    "            d_C_r2f = d_C_r2f.view(d_C_r2f.shape[0], d_C_r2f.shape[1], -1)\n",
    "            d_C_f2r = d_C_f2r.view(d_C_f2r.shape[0], d_C_f2r.shape[1], -1)\n",
    "\n",
    "            ######################## conditional ########################\n",
    "            d_C_r2f = self.d_con_mlp1_r2f(d_C_r2f)\n",
    "            d_C_f2r = self.d_con_mlp1_f2r(d_C_f2r)\n",
    "            d_C_g = self.d_con_mlp1_g(C_g)\n",
    "            d_C_m = self.d_con_mlp1_m(C_m)\n",
    "\n",
    "            d_C_r2f = self.d_con_mlp2_r2f(d_C_r2f.transpose(1,2)).squeeze(-1).unsqueeze(0)\n",
    "            d_C_f2r = self.d_con_mlp2_r2f(d_C_f2r.transpose(1,2)).squeeze(-1).unsqueeze(0)\n",
    "            d_C_g = self.d_con_mlp2_g(d_C_g.transpose(1,2)).squeeze(-1).unsqueeze(0)\n",
    "            d_C_m = self.d_con_mlp2_m(d_C_m.transpose(1,2)).squeeze(-1).unsqueeze(0)\n",
    "\n",
    "            d_con_output = torch.cat((d_C_r2f, d_C_f2r, d_C_g, d_C_m), dim=0)\n",
    "            ###############################################################\n",
    "\n",
    "            ######################## unconditional ########################\n",
    "            d_UC_r = self.d_unc_mlp1_r(real_text).squeeze(-1)\n",
    "            d_UC_g = self.d_unc_mlp1_g(fake_text).squeeze(-1)\n",
    "            d_UC_m = self.d_unc_mlp1_m(mismatched_text).squeeze(-1)\n",
    "\n",
    "            d_UC_r = self.d_unc_mlp2_r(d_UC_r).squeeze(-1).unsqueeze(0)\n",
    "            d_UC_g = self.d_unc_mlp2_g(d_UC_g).squeeze(-1).unsqueeze(0)\n",
    "            d_UC_m = self.d_unc_mlp2_m(d_UC_m).squeeze(-1).unsqueeze(0)\n",
    "\n",
    "            d_unc_output = torch.cat((d_UC_r, d_UC_g, d_UC_m), dim=0)\n",
    "            ###############################################################\n",
    "            return d_con_output, d_unc_output"
   ],
   "id": "d64fcb8e4190a6c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:21:44.761325Z",
     "start_time": "2024-10-27T16:21:44.556035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# empty cuda memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "id": "d5c9b5f31e3aa208",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "742"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:21:44.828394Z",
     "start_time": "2024-10-27T16:21:44.820633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "id": "d7e3bfcce1a75fbd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:21:50.048942Z",
     "start_time": "2024-10-27T16:21:44.981387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NetG = Generator().to(torch.float32).to(device)\n",
    "NetD = Discriminator().to(torch.float32).to(device)\n",
    "optimizer_G = optim.Adam(NetG.parameters(), lr=0.001)\n",
    "optimizer_D = optim.Adam(NetD.parameters(), lr=0.001)\n",
    "\n",
    "train_losses_FC = []\n",
    "train_losses_G = []\n",
    "train_losses_D = []\n",
    "test_losses_FC = []\n",
    "test_losses_G = []\n",
    "test_losses_D = []\n",
    "save = []\n",
    "present_epoch = 1\n",
    "best_train_loss_FC = 9999\n",
    "best_train_loss_G = 9999\n",
    "best_train_loss_D = 9999\n",
    "best_test_loss_FC = 9999\n",
    "best_test_loss_G = 9999\n",
    "best_test_loss_D = 9999\n",
    "loss_data = pd.DataFrame()\n",
    "\n",
    "g_fc_loss_list = []\n",
    "g_con_loss_list = []\n",
    "g_unc_loss_list = []\n",
    "d_con_r2f_loss_list = []\n",
    "d_con_f2r_loss_list = []\n",
    "d_con_f_loss_list = []\n",
    "d_con_m_loss_list = []\n",
    "d_unc_r_loss_list = []\n",
    "d_unc_f_loss_list = []\n",
    "d_unc_m_loss_list = []\n",
    "\n",
    "checkpoint = False\n",
    "if checkpoint:\n",
    "    checkpoint_G = torch.load('./Model/test_save/test_save_2NetG.pth')\n",
    "    checkpoint_D = torch.load('./Model/test_save/test_save_2NetD.pth')\n",
    "    NetG.load_state_dict(checkpoint_G['model_state_dict'])\n",
    "    NetD.load_state_dict(checkpoint_D['model_state_dict'])\n",
    "    optimizer_G.load_state_dict(checkpoint_G['optimizer_state_dict'])\n",
    "    optimizer_D.load_state_dict(checkpoint_D['optimizer_state_dict'])\n",
    "    train_losses_FC.append(checkpoint_G['FC_loss'])\n",
    "    train_losses_G.append(checkpoint_G['G_loss'])\n",
    "    train_losses_D.append(checkpoint_G['D_loss'])\n",
    "    present_epoch = checkpoint_G['epoch'] + 1\n",
    "\n",
    "\n",
    "def funnyScoreLoss(output_funny_score, funny_score):\n",
    "    loss = nn.MSELoss()(output_funny_score, funny_score)\n",
    "    g_fc_loss_list.append(loss.item())\n",
    "    return loss\n",
    "\n",
    "def generatorLoss(condition_logits, uncondition_logits):\n",
    "    result_fake_con = torch.FloatTensor(uncondition_logits.shape[0]).uniform_(0.0, 0.1).to(device)\n",
    "    result_fake_unc = torch.FloatTensor(uncondition_logits.shape[0]).uniform_(0.0, 0.1).to(device)\n",
    "    con_loss = CrossEntropyLoss()(condition_logits, result_fake_con.to(torch.long))\n",
    "    unc_loss = BCEWithLogitsLoss()(uncondition_logits, result_fake_unc)\n",
    "    g_con_loss_list.append(con_loss.item())\n",
    "    g_unc_loss_list.append(unc_loss.item())\n",
    "    loss = con_loss + unc_loss\n",
    "    return loss\n",
    "\n",
    "def discriminatorLoss(condition_logits, uncondition_logits):\n",
    "    result_true = torch.FloatTensor(uncondition_logits[0].shape[0]).uniform_(0.9, 1.0).to(device)\n",
    "    result_fake_con_r2f = torch.FloatTensor(uncondition_logits[0].shape[0]).uniform_(0.0, 0.1).to(device)\n",
    "    result_fake_con_f2r = torch.FloatTensor(uncondition_logits[0].shape[0]).uniform_(0.0, 0.1).to(device)\n",
    "    result_fake_con_f = torch.FloatTensor(uncondition_logits[0].shape[0]).uniform_(0.0, 0.1).to(device)\n",
    "    result_fake_con_m = torch.FloatTensor(uncondition_logits[0].shape[0]).uniform_(0.0, 0.1).to(device)\n",
    "    result_fake_unc_f = torch.FloatTensor(uncondition_logits[0].shape[0]).uniform_(0.0, 0.1).to(device)\n",
    "    result_fake_unc_m = torch.FloatTensor(uncondition_logits[0].shape[0]).uniform_(0.0, 0.1).to(device)\n",
    "    \n",
    "    con_r2f = CrossEntropyLoss()(condition_logits[0], result_fake_con_r2f.to(torch.long))\n",
    "    con_f2r = CrossEntropyLoss()(condition_logits[1], result_fake_con_f2r.to(torch.long))\n",
    "    con_f = CrossEntropyLoss()(condition_logits[2], result_fake_con_f.to(torch.long))\n",
    "    con_m = CrossEntropyLoss()(condition_logits[3], result_fake_con_m.to(torch.long))\n",
    "    unc_r = BCEWithLogitsLoss()(uncondition_logits[0], result_true)\n",
    "    unc_f = BCEWithLogitsLoss()(uncondition_logits[1], result_fake_unc_f)\n",
    "    unc_m = BCEWithLogitsLoss()(uncondition_logits[2], result_fake_unc_m)\n",
    "    d_con_r2f_loss_list.append(con_r2f.item())\n",
    "    d_con_f2r_loss_list.append(con_f2r.item())\n",
    "    d_con_f_loss_list.append(con_f.item())\n",
    "    d_con_m_loss_list.append(con_m.item())\n",
    "    d_unc_r_loss_list.append(unc_r.item())\n",
    "    d_unc_f_loss_list.append(unc_f.item())\n",
    "    d_unc_m_loss_list.append(unc_m.item())\n",
    "    loss = ((con_r2f + con_f2r)/2) + ((con_f + con_m)/2) + unc_r + ((unc_f + unc_m)/2)\n",
    "    return loss"
   ],
   "id": "9dc4843819037573",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T16:22:15.759965Z",
     "start_time": "2024-10-27T16:21:50.049519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_name = '20241028'\n",
    "if not os.path.exists('./Model/'+save_name):\n",
    "    os.makedirs('./Model/'+save_name)\n",
    "    \n",
    "epochs = 30\n",
    "startTime = 0\n",
    "textExtractionTime = 0\n",
    "GeneratorForwardTime = 0\n",
    "GeneratorBackwardFCTime = 0\n",
    "GeneratorBackwardGTime = 0\n",
    "DiscriminatorForwardTime = 0\n",
    "DiscriminatorBackwardTime = 0\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(epochs):\n",
    "    print(\"---------------------------------------- epoch \"+ str(epoch + present_epoch) +\" ---------------------------------------\")\n",
    "    train_loss_FC = 0\n",
    "    train_loss_G = 0\n",
    "    train_loss_D = 0\n",
    "    test_loss_FC = 0\n",
    "    test_loss_G = 0\n",
    "    test_loss_D = 0\n",
    "    pre = 0\n",
    "    ###################################### Train ######################################\n",
    "    with tqdm(train_loader, unit=\"batch\", leave=True) as tepoch:\n",
    "        for idx, (text, image, funny_score) in enumerate(tepoch):\n",
    "            # tepoch.set_postfix({'Now': tepoch.format_dict['elapsed'], 'Status': \" New batch preprocessing\"})\n",
    "            if idx == 0:\n",
    "                startTime = tepoch.format_dict['elapsed']\n",
    "            elif idx == 1:\n",
    "                startTime = (tepoch.format_dict['elapsed'] - pre) * 2\n",
    "            else:\n",
    "                startTime += tepoch.format_dict['elapsed'] - pre\n",
    "            pre = tepoch.format_dict['elapsed']\n",
    "            text = textExtraction(tokenizer, gemmaConfig, text).to(torch.bfloat16)\n",
    "            image = image.to(torch.bfloat16)\n",
    "            funny_score = funny_score.to(torch.bfloat16)\n",
    "            # print(text.dtype, image.dtype, funny_score.dtype)\n",
    "            # print(text.shape, image.shape, funny_score.shape)\n",
    "            # torch.Size([32, 64, 768]) torch.Size([32, 64, 768]) torch.Size([32, 1])\n",
    "            textExtractionTime += tepoch.format_dict['elapsed'] - pre\n",
    "            pre = tepoch.format_dict['elapsed']\n",
    "            ######################################################\n",
    "            # (1) Update Generator network\n",
    "            ######################################################\n",
    "            optimizer_G.zero_grad()\n",
    "            # tepoch.set_postfix({'Now': tepoch.format_dict['elapsed'], 'Status': \" Generator Forward\"})\n",
    "            logits, output_funny_score = NetG(text.to(device), image.to(device))\n",
    "            g_con_logits, g_unc_logits = NetD(text.to(device), logits, image.to(device), \"G\")\n",
    "            GeneratorForwardTime += tepoch.format_dict['elapsed'] - pre\n",
    "            pre = tepoch.format_dict['elapsed']\n",
    "\n",
    "            # tepoch.set_postfix({'Now': tepoch.format_dict['elapsed'], 'Status': \" Generator Backward - Generator\"})\n",
    "            loss_G = (1e-06 * generatorLoss(g_con_logits.to(device), g_unc_logits.to(device))) + funnyScoreLoss(output_funny_score.to(device), funny_score.to(device))\n",
    "            loss_G.backward(retain_graph=True)\n",
    "            train_loss_G += loss_G.item()\n",
    "            optimizer_G.step()\n",
    "            GeneratorBackwardGTime += tepoch.format_dict['elapsed'] - pre\n",
    "            pre = tepoch.format_dict['elapsed']\n",
    "            ######################################################\n",
    "            # (4) Update Discriminator network\n",
    "            ######################################################\n",
    "            optimizer_D.zero_grad()\n",
    "            # tepoch.set_postfix({'Now': tepoch.format_dict['elapsed'], 'Status': \" Discriminator Forward\"})\n",
    "            d_con_logits, d_unc_logits = NetD(text.to(device).detach(), logits.detach(), image.to(device).detach(),\n",
    "                                              \"D\")\n",
    "            # tepoch.set_postfix({'Now': tepoch.format_dict['elapsed'], 'Status': \" Discriminator Backward\"})\n",
    "            DiscriminatorForwardTime += tepoch.format_dict['elapsed'] - pre\n",
    "            pre = tepoch.format_dict['elapsed']\n",
    "            loss_D = discriminatorLoss(d_con_logits.to(device), d_unc_logits.to(device))\n",
    "            loss_D.backward()\n",
    "            DiscriminatorBackwardTime += tepoch.format_dict['elapsed'] - pre\n",
    "            pre = tepoch.format_dict['elapsed']\n",
    "            optimizer_D.step()\n",
    "            train_loss_D += loss_D.item()\n",
    "            ######################################################\n",
    "            # tepoch.set_postfix({'FC_loss': train_loss_FC/ (idx+1), 'G_loss': train_loss_G/ (idx+1), 'D_loss': train_loss_D/ (idx+1)})\n",
    "            tepoch.set_postfix({'start': startTime/(idx+1), 'textExtraction': textExtractionTime/(idx+1), 'GeneratorForward': GeneratorForwardTime/(idx+1), 'GeneratorBackwardFC': GeneratorBackwardFCTime/(idx+1), 'GeneratorBackwardG': GeneratorBackwardGTime/(idx+1), 'DiscriminatorForward': DiscriminatorForwardTime/(idx+1), 'DiscriminatorBackward': DiscriminatorBackwardTime/(idx+1)})\n",
    "            ######################################################\n",
    "    train_loss_FC /= len(train_loader)\n",
    "    train_loss_G /= len(train_loader)\n",
    "    train_loss_D /= len(train_loader)\n",
    "    train_losses_FC.append(train_loss_FC)\n",
    "    train_losses_G.append(train_loss_G)\n",
    "    train_losses_D.append(train_loss_D)\n",
    "    ###################################### Train ######################################\n",
    "    \n",
    "    \n",
    "    ######################################  Test ######################################\n",
    "    with tqdm(test_loader, unit=\"batch\", leave=True) as tepoch:\n",
    "        for idx, (text, image, funny_score) in enumerate(tepoch):\n",
    "            text = textExtraction(tokenizer, gemmaConfig, text).to(torch.float32)\n",
    "            image = image.to(torch.float32)\n",
    "            funny_score = funny_score.to(torch.float32)\n",
    "            # Generator\n",
    "            logits, output_funny_score = NetG(text.to(device), image.to(device))\n",
    "            # Discriminator\n",
    "            g_con_logits, g_unc_logits = NetD(text.to(device), logits, image.to(device), \"G\")\n",
    "            d_con_logits, d_unc_logits = NetD(text.to(device).detach(), logits.detach(), image.to(device).detach(),\"D\")\n",
    "            # loss\n",
    "            loss_FC = funnyScoreLoss(output_funny_score, funny_score.to(device))\n",
    "            loss_G = generatorLoss(g_con_logits, g_unc_logits)\n",
    "            loss_D = discriminatorLoss(d_con_logits, d_unc_logits)\n",
    "            test_loss_FC += loss_FC.item()\n",
    "            test_loss_G += loss_G.item()\n",
    "            test_loss_D += loss_D.item()\n",
    "            tepoch.set_postfix({'FC_loss': test_loss_FC / (idx + 1), 'G_loss': test_loss_G / (idx + 1),\n",
    "                                'D_loss': test_loss_D / (idx + 1)})\n",
    "    test_loss_FC /= len(test_loader)\n",
    "    test_loss_G /= len(test_loader)\n",
    "    test_loss_D /= len(test_loader)\n",
    "    test_losses_FC.append(test_loss_FC)\n",
    "    test_losses_G.append(test_loss_G)\n",
    "    test_losses_D.append(test_loss_D)\n",
    "    ######################################  Test ######################################\n",
    "    \n",
    "    ######################################  Save ######################################\n",
    "    hasSaved = False\n",
    "    # 任一個loss小於最佳loss就存檔\n",
    "    if train_loss_FC < best_train_loss_FC and test_loss_FC < best_test_loss_FC:\n",
    "        best_train_loss_FC = train_loss_FC\n",
    "        best_test_loss_FC = test_loss_FC\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    if train_loss_G < best_train_loss_G and test_loss_G < best_test_loss_G:\n",
    "        best_train_loss_G = train_loss_G\n",
    "        best_test_loss_G = test_loss_G\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    if train_loss_D < best_train_loss_D and test_loss_D < best_test_loss_D:\n",
    "        best_train_loss_D = train_loss_D\n",
    "        best_test_loss_D = test_loss_D\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    \n",
    "    if hasSaved:\n",
    "        save.append(\"V\")\n",
    "    else:\n",
    "        save.append(\" \")\n",
    "\n",
    "    loss_data['train_FC'] = train_losses_FC\n",
    "    loss_data['train_G'] = train_losses_G\n",
    "    loss_data['train_D'] = train_losses_D\n",
    "    loss_data['test_FC'] = test_losses_FC\n",
    "    loss_data['test_G'] = test_losses_G\n",
    "    loss_data['test_D'] = test_losses_D\n",
    "    loss_data['save'] = save\n",
    "    loss_data.to_csv('./Model/' + save_name + \"/\" + save_name + '_loss.csv', index=False)\n",
    "    ######################################  Save ######################################"
   ],
   "id": "bb691e03b98cb61e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- epoch 1 ---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4248 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32, 256, 768]) torch.Size([32, 32, 256, 768]) torch.Size([32, 128, 768]) torch.Size([32, 128, 768])\n",
      "torch.Size([32, 32, 256, 2]) torch.Size([32, 32, 256, 2]) torch.Size([32, 128, 768]) torch.Size([32, 128, 768])\n",
      "torch.Size([32, 32, 2]) torch.Size([32, 32, 2]) torch.Size([32, 128, 768]) torch.Size([32, 128, 768])\n",
      "torch.Size([1, 32, 2]) torch.Size([1, 32, 2]) torch.Size([32, 128, 768]) torch.Size([32, 128, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/4248 [00:25<30:14:19, 25.63s/batch, start=0.052, textExtraction=0.673, GeneratorForward=8.24, GeneratorBackwardFC=1.5, GeneratorBackwardG=4.57, DiscriminatorForward=0.727, DiscriminatorBackward=1.62]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 67\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# # tepoch.set_postfix({'Now': tepoch.format_dict['elapsed'], 'Status': \" Generator Backward - FunnyScore\"})\u001B[39;00m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m# loss_FC = funnyScoreLoss(output_funny_score.to(device), funny_score.to(device))\u001B[39;00m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;66;03m# loss_FC.backward(retain_graph=True)\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     64\u001B[0m \n\u001B[0;32m     65\u001B[0m \u001B[38;5;66;03m# tepoch.set_postfix({'Now': tepoch.format_dict['elapsed'], 'Status': \" Generator Backward - Generator\"})\u001B[39;00m\n\u001B[0;32m     66\u001B[0m loss_G \u001B[38;5;241m=\u001B[39m generatorLoss(g_con_logits\u001B[38;5;241m.\u001B[39mto(device), g_unc_logits\u001B[38;5;241m.\u001B[39mto(device))\n\u001B[1;32m---> 67\u001B[0m \u001B[43mloss_G\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m train_loss_G \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss_G\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     69\u001B[0m GeneratorBackwardGTime\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tepoch\u001B[38;5;241m.\u001B[39mformat_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124melapsed\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m-\u001B[39mpre\n",
      "File \u001B[1;32m~\\PycharmProjects\\MemeGAN\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    580\u001B[0m     )\n\u001B[1;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\MemeGAN\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\MemeGAN\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    826\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    827\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sepateLoss = pd.DataFrame()\n",
    "sepateLoss['g_fc_loss'] = g_fc_loss_list\n",
    "sepateLoss['g_con_loss'] = g_con_loss_list\n",
    "sepateLoss['g_unc_loss'] = g_unc_loss_list\n",
    "sepateLoss['d_con_r2f_loss'] = d_con_r2f_loss_list\n",
    "sepateLoss['d_con_f2r_loss'] = d_con_f2r_loss_list\n",
    "sepateLoss['d_con_f_loss'] = d_con_f_loss_list\n",
    "sepateLoss['d_con_m_loss'] = d_con_m_loss_list\n",
    "sepateLoss['d_unc_r_loss'] = d_unc_r_loss_list\n",
    "sepateLoss['d_unc_f_loss'] = d_unc_f_loss_list\n",
    "sepateLoss['d_unc_m_loss'] = d_unc_m_loss_list\n",
    "sepateLoss.to_csv('./Model/' + save_name + \"/\" + save_name + '_sepateLoss.csv', index=False)"
   ],
   "id": "bb6d941c7a576f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T01:46:49.902283900Z",
     "start_time": "2024-10-03T00:46:37.439345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses_FC, label='train')\n",
    "plt.plot(train_losses_G, label='train')\n",
    "plt.plot(train_losses_D, label='train')\n",
    "plt.plot(test_losses_FC, label='test')\n",
    "plt.plot(test_losses_G, label='test')\n",
    "plt.plot(test_losses_D, label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# save plot\n",
    "plt.savefig('./Model/' + save_name + \"/\" + save_name + '_loss.png')"
   ],
   "id": "65770e7128728d32",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate Test",
   "id": "4c44d1e70ced399f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:37:30.366022600Z",
     "start_time": "2024-10-14T06:05:20.980447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#load model\n",
    "NetG = Generator().to(device)\n",
    "NetD = Discriminator().to(device)\n",
    "NetG.load_state_dict(torch.load('./Model/test_batch32/test_batch32_5NetG.pth'))\n",
    "NetD.load_state_dict(torch.load('./Model/test_batch32/test_batch32_5NetD.pth'))\n",
    "# train with load model\n",
    "NetG.train()\n",
    "NetD.train()\n"
   ],
   "id": "c3f72adc27a5f410",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#load model\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m NetG \u001B[38;5;241m=\u001B[39m \u001B[43mGenerator\u001B[49m()\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      3\u001B[0m NetD \u001B[38;5;241m=\u001B[39m Discriminator()\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      4\u001B[0m NetG\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./Model/test_batch32/test_batch32_5NetG.pth\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Generator' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:37:30.366022600Z",
     "start_time": "2024-10-02T23:43:27.978751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generate\n",
    "NetG.eval()\n",
    "NetD.eval()\n",
    "image = imageExtraction(\"./test_img.jpg\")\n",
    "output = NetG.generate(image, 200)\n",
    "output"
   ],
   "id": "aa601343f448bd19",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 991.56it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 998.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.2329,  0.4325,  0.4199,  ..., -0.4702, -0.1142, -0.0098]],\n",
       " \n",
       "         [[ 0.1089,  0.3625,  0.8224,  ...,  0.2763,  0.0975, -0.0043]],\n",
       " \n",
       "         [[-0.1282, -0.1454, -0.1592,  ...,  0.1126,  0.7090,  0.6611]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.4177,  0.1868,  0.0531,  ..., -0.5458,  0.0338, -1.4983]],\n",
       " \n",
       "         [[-0.1912,  0.1032,  0.4763,  ...,  0.7547,  0.7066, -0.5460]],\n",
       " \n",
       "         [[-0.1003, -0.3331, -0.0245,  ..., -0.5132,  0.0633,  0.8948]]],\n",
       "        device='cuda:0'),\n",
       " [2,\n",
       "  540,\n",
       "  235248,\n",
       "  236193,\n",
       "  235248,\n",
       "  235248,\n",
       "  235250,\n",
       "  235274,\n",
       "  35351,\n",
       "  235254,\n",
       "  605,\n",
       "  6935,\n",
       "  235276,\n",
       "  235248,\n",
       "  235248,\n",
       "  235248,\n",
       "  132385,\n",
       "  235248,\n",
       "  235265,\n",
       "  235248,\n",
       "  2173,\n",
       "  235274,\n",
       "  235248,\n",
       "  2465,\n",
       "  3682,\n",
       "  236193,\n",
       "  18824,\n",
       "  235274,\n",
       "  235248,\n",
       "  11200,\n",
       "  235276,\n",
       "  235276,\n",
       "  616,\n",
       "  235248,\n",
       "  235248,\n",
       "  235248,\n",
       "  235274,\n",
       "  235248,\n",
       "  235248,\n",
       "  2012,\n",
       "  235276,\n",
       "  2012,\n",
       "  236193,\n",
       "  235248,\n",
       "  235248,\n",
       "  235248,\n",
       "  235265,\n",
       "  618,\n",
       "  235276,\n",
       "  669,\n",
       "  235248,\n",
       "  235248,\n",
       "  24255,\n",
       "  618,\n",
       "  14383,\n",
       "  236193,\n",
       "  235248,\n",
       "  235362,\n",
       "  236193,\n",
       "  5862,\n",
       "  1420,\n",
       "  236193,\n",
       "  235248,\n",
       "  235248,\n",
       "  46816,\n",
       "  235248,\n",
       "  235248,\n",
       "  1820,\n",
       "  235248,\n",
       "  235276,\n",
       "  235265,\n",
       "  690,\n",
       "  235256,\n",
       "  235274,\n",
       "  236193,\n",
       "  235256,\n",
       "  235248,\n",
       "  235248,\n",
       "  235248,\n",
       "  236193,\n",
       "  819,\n",
       "  235248,\n",
       "  235248,\n",
       "  235254,\n",
       "  235265,\n",
       "  84521,\n",
       "  3550,\n",
       "  235276,\n",
       "  42113,\n",
       "  235265,\n",
       "  235345,\n",
       "  509,\n",
       "  235248,\n",
       "  235276,\n",
       "  4943,\n",
       "  235248,\n",
       "  235248,\n",
       "  236193,\n",
       "  53152,\n",
       "  235269,\n",
       "  235265],\n",
       " tensor([-0.4983], device='cuda:0'))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
