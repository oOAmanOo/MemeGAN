{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "! pip install Pillow",
   "id": "7c6df9d905fdb8be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "! pip install --upgrade huggingface_hub",
   "id": "db628d2dd81a5c98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "! huggingface-cli login  # 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",
   "id": "dcb920958539f226"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T00:22:29.106739Z",
     "start_time": "2024-10-21T00:22:21.543246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from extractor import addImagePath, textExtraction, imageExtraction, textExtractReverse"
   ],
   "id": "7123375a60e24a10",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TonyLab\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1. Load the data and split the data",
   "id": "6f301289c5bb2722"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T00:22:29.115522Z",
     "start_time": "2024-10-21T00:22:29.106739Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class OxfordDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, image, funny_score):\n",
    "        self.text = text\n",
    "        self.image = image\n",
    "        self.funny_score = funny_score\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx):    \n",
    "        imageData = torch.load('../Data/Oxford_HIC/ImageData/'+ self.image[idx] +'.pt', weights_only=False)\n",
    "            \n",
    "        return self.text[idx], imageData, self.funny_score[idx]"
   ],
   "id": "6284fc6cdb32c442",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T00:22:34.655785Z",
     "start_time": "2024-10-21T00:22:29.370765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if args.img - dir == 'Oxford_HIC':\n",
    "dirPath = '../Data/Oxford_HIC/Filtered_oxford_hic_data.csv'\n",
    "imgPath = '../Data/Oxford_HIC/oxford_img/'\n",
    "# else:\n",
    "# dirPath = '../Data/Instagram/Filter_' + 'wendys' + '.csv'\n",
    "# imgPath = '../Data/Instagram/' + 'wendys' + '_img/'\n",
    "# load data\n",
    "data = pd.read_csv(dirPath)\n",
    "print(\"shape of data: \", data.shape)\n",
    "data.head()"
   ],
   "id": "6d1d47d2ceb7bd39",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data:  (3398081, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   image_id                                            caption  funny_score\n",
       "0  bokete_0                          My driver's license photo          0.0\n",
       "1  bokete_1                                    Refugee relief.          0.0\n",
       "2  bokete_2  Now! I think I stepped on a cat! What? Really?...          0.0\n",
       "3  bokete_3      You wouldn't know I was reading a comic book.          0.0\n",
       "4  bokete_4                  Oh no! I forgot my ・・・・ clothes!\"          0.0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>caption</th>\n",
       "      <th>funny_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bokete_0</td>\n",
       "      <td>My driver's license photo</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bokete_1</td>\n",
       "      <td>Refugee relief.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bokete_2</td>\n",
       "      <td>Now! I think I stepped on a cat! What? Really?...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bokete_3</td>\n",
       "      <td>You wouldn't know I was reading a comic book.</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bokete_4</td>\n",
       "      <td>Oh no! I forgot my ・・・・ clothes!\"</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T00:22:36.488040Z",
     "start_time": "2024-10-21T00:22:34.719455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_text = train['caption'].tolist()\n",
    "train_image = train['image_id'].tolist()\n",
    "train_funny_score = train['funny_score'].tolist()\n",
    "test_text = test['caption'].tolist()\n",
    "test_image = test['image_id'].tolist()\n",
    "test_funny_score = test['funny_score'].tolist()\n",
    "\n",
    "train_dataset = OxfordDataset(train_text, train_image, train_funny_score)\n",
    "test_dataset = OxfordDataset(test_text, test_image, test_funny_score)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ],
   "id": "b3a2a78c51a21fc3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T00:22:37.033481Z",
     "start_time": "2024-10-21T00:22:36.837641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataL = iter(train_loader)\n",
    "text, imgs, funny_score = next(dataL)\n",
    "# print(\"shape of text: \", text.shape)\n",
    "# print(\"shape of image: \", imgs.shape)\n",
    "print(\"shape of funny_score: \", funny_score.shape)"
   ],
   "id": "71e883f401a9b059",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of funny_score:  torch.Size([32])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Load Gemma",
   "id": "c43cb6bdba292329"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T00:22:43.850756Z",
     "start_time": "2024-10-21T00:22:37.063879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 官方的Gemma #########################################################################################\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\",  torch_dtype=torch.float32)\n",
    "gemmaConfig =  AutoConfig.from_pretrained('google/gemma-2-2b-it')\n",
    "########################################################################################################"
   ],
   "id": "e5f8f2c83fc25118",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.10s/it]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Generator",
   "id": "d8c8fefd0a84950b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T00:22:43.932701Z",
     "start_time": "2024-10-21T00:22:43.925039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class self_multi(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self_multi, self).__init__()\n",
    "        # self attention\n",
    "        self.selfAttentionMultihead = nn.MultiheadAttention(768, 1)\n",
    "        self.selfAttentionLayerNorm = nn.LayerNorm(768)\n",
    "        self.selfAttentionLinear = nn.Linear(768, 768)\n",
    "        self.selfAttentionLayerNorm2 = nn.LayerNorm(768)\n",
    "        \n",
    "        # multihead attention\n",
    "        self.multiheadAttentionMultihead = nn.MultiheadAttention(768, 8)\n",
    "        self.multiheadAttentionLinear = nn.Linear(768, 768)\n",
    "        self.multiheadAttentionLayerNorm = nn.LayerNorm(768)\n",
    "        \n",
    "    def forward(self, image, text):\n",
    "        # self attention module\n",
    "        self_out = self.selfAttentionMultihead(image, image, image)[0]\n",
    "        self_out = self.selfAttentionLinear(self_out)\n",
    "        self_out = self.selfAttentionLayerNorm(self_out + image)\n",
    "\n",
    "        # multihead attention module\n",
    "        multi_out = self.multiheadAttentionMultihead(text, text, text)[0]\n",
    "        multi_out = self.multiheadAttentionLinear(multi_out)\n",
    "        multi_out = self.multiheadAttentionLayerNorm(multi_out + text)\n",
    "        \n",
    "        return self_out, multi_out"
   ],
   "id": "40396b2be022a57e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T00:22:44.003978Z",
     "start_time": "2024-10-21T00:22:43.996418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class co_attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(co_attention, self).__init__()\n",
    "        # co-attention text\n",
    "        self.coAttentionTextMultihead = nn.MultiheadAttention(768, 1)\n",
    "        self.coAttentionTextLinear = nn.Linear(768, 768)\n",
    "        self.coAttentionTextLayerNorm = nn.LayerNorm(768)\n",
    "\n",
    "        # co-attention image\n",
    "        self.coAttentionImageMultihead = nn.MultiheadAttention(768, 1)\n",
    "        self.coAttentionImageLinear = nn.Linear(768, 768)\n",
    "        self.coAttentionImageLayerNorm = nn.LayerNorm(768)\n",
    "        \n",
    "    def forward(self, image, text):\n",
    "        # co-attention image module\n",
    "        visual_attending_textual = self.coAttentionTextMultihead(image, text, text)[0]\n",
    "        visual_attending_textual = self.coAttentionTextLinear(visual_attending_textual)\n",
    "        visual_attending_textual = self.coAttentionTextLayerNorm(visual_attending_textual + image)\n",
    "        \n",
    "        # co-attention text module\n",
    "        textual_attending_visual = self.coAttentionTextMultihead(text, image, image)[0]\n",
    "        textual_attending_visual = self.coAttentionTextLinear(textual_attending_visual)\n",
    "        textual_attending_visual = self.coAttentionTextLayerNorm(textual_attending_visual + text) \n",
    "        \n",
    "        return visual_attending_textual, textual_attending_visual"
   ],
   "id": "553cad97871b6f0a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T00:22:44.086944Z",
     "start_time": "2024-10-21T00:22:44.067529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, depth=12):\n",
    "        super(Generator, self).__init__()\n",
    "        self.layers_self_multi = nn.ModuleList([self_multi() for _ in range(depth)])\n",
    "        self.layers_co_attention = nn.ModuleList([co_attention() for _ in range(depth)])   \n",
    "    \n",
    "        # feed forward\n",
    "        self.feedForwardLinear = nn.Linear(768, 768)\n",
    "        self.feedForwardLayerNorm = nn.LayerNorm(768)\n",
    "        \n",
    "        # gemma\n",
    "        self.gemmaLinearMaxTokens = nn.Linear(64, 16)\n",
    "        self.gemmaLinearBefore = nn.Linear(768, gemmaConfig.vocab_size)\n",
    "        self.gemmaSoftmax = nn.Softmax(dim=2)\n",
    "        self.gemma = nn.Sequential(*list(gemma.children())[:-1])\n",
    "        self.gemmaLm_head = nn.Sequential(*list(gemma.children())[1:])\n",
    "        \n",
    "        # funny score\n",
    "        self.FunnyScorelinear1 = nn.Linear(768, 1)\n",
    "        self.FunnyScorelinear2 = nn.Linear(64, 1)          \n",
    "        \n",
    "    def gemmaGenerate(self, x):\n",
    "        with torch.no_grad():\n",
    "            # maximum 32 tokens\n",
    "            x = self.gemmaLinearMaxTokens(x.transpose(1, 2)).transpose(1, 2)\n",
    "            x = self.gemmaLinearBefore(x)\n",
    "            x = self.gemmaSoftmax(x)\n",
    "            # get max value of each row, total 32*64\n",
    "            top_k_values, top_k_indices = torch.topk(x, 1, dim=2, largest=True)\n",
    "            toGemma = textExtractReverse(top_k_indices).to(device)\n",
    "            # 使用gemma作為model的一部分\n",
    "            output = self.gemma(toGemma)\n",
    "            # output[0] = last_hidden_state\n",
    "            # output[1] = past_key_values\n",
    "            \n",
    "        return output[0]\n",
    "               \n",
    "    \n",
    "    def forward(self, text, image):\n",
    "        # max_seq_len = max(text.shape[1], image.shape[1])\n",
    "        # text = nn.functional.pad(text, (0, 0, 0, max_seq_len - text.shape[1]))\n",
    "        # image = nn.functional.pad(image, (0, 0, 0, max_seq_len - image.shape[1]))\n",
    "        text = text.transpose(0, 1)\n",
    "        image = image.transpose(0, 1)\n",
    "        \n",
    "        ######################### Transformer ######################### \n",
    "        for self_multi_layer in self.layers_self_multi:\n",
    "            image, text = self_multi_layer(image, text)\n",
    "        for co_attention_layer in self.layers_co_attention:\n",
    "            image, text = co_attention_layer(image, text)\n",
    "        ###############################################################\n",
    "        \n",
    "        # feature fusion\n",
    "        feature_fusion = image + text   #visual_attending_textual + textual_attending_visual\n",
    "        feature_fusion = self.feedForwardLinear(feature_fusion)\n",
    "        feature_fusion = self.feedForwardLayerNorm(feature_fusion + feature_fusion)\n",
    "        feature_fusion = feature_fusion.squeeze(-1)\n",
    "        feature_fusion = feature_fusion.transpose(0, 1)\n",
    "        ####################### gemma  generate #######################\n",
    "        last_hidden_state = self.gemmaGenerate(feature_fusion)\n",
    "        output_text = self.gemmaLm_head(last_hidden_state)\n",
    "        ###############################################################\n",
    "        \n",
    "        ######################### funny score #########################\n",
    "        output_funny_score = self.FunnyScorelinear1(feature_fusion).squeeze(-1)\n",
    "        output_funny_score = self.FunnyScorelinear2(output_funny_score).squeeze(-1)\n",
    "        ###############################################################\n",
    "        \n",
    "        return output_text, output_funny_score\n",
    "    \n",
    "    def generate(self, image, max_length = 100):\n",
    "        generated_tokens = []\n",
    "        generated_tokens.append(2) #<bos> = 2\n",
    "        text = torch.zeros_like(image).to(device)\n",
    "        text = text.transpose(0, 1)\n",
    "        image = image.transpose(0, 1)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "        depth = len(self.layers_self_multi)\n",
    "        # 有時後空格會失效，所以手動插入空格 <pad> = 0\n",
    "        def insert_zeros(list):\n",
    "            zeros = [0] * (2 * len(list) - 1)\n",
    "            zeros[::2] = list\n",
    "            return zeros\n",
    "        \n",
    "        lastTurn = False\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length + 1):\n",
    "                # Transformer\n",
    "                for i in range(depth):\n",
    "                    # self attention\n",
    "                    image, text = self.layers_self_multi[i](image, text)\n",
    "                    # co-attention\n",
    "                    image, text = self.layers_co_attention[i](image, text)\n",
    "                \n",
    "                # feature fusion\n",
    "                feature_fusion = image + text # visual_attending_textual + textual_attending_visual\n",
    "                feature_fusion = self.feedForwardLinear(feature_fusion)\n",
    "                feature_fusion = self.feedForwardLayerNorm(feature_fusion + feature_fusion)\n",
    "                feature_fusion = feature_fusion.squeeze(-1)\n",
    "                feature_fusion = feature_fusion.transpose(0, 1)\n",
    "                \n",
    "                # gemma generate\n",
    "                last_hidden_state = self.gemmaGenerate(feature_fusion)\n",
    "                output_text = self.gemmaLm_head(last_hidden_state)\n",
    "                \n",
    "                # funny score\n",
    "                output_funny_score = self.FunnyScorelinear1(feature_fusion).squeeze(-1)\n",
    "                output_funny_score = self.FunnyScorelinear2(output_funny_score).squeeze(-1)\n",
    "                \n",
    "                if lastTurn: # show final funny score\n",
    "                    return generated_caption, output_funny_score\n",
    "                else:\n",
    "                    next_token_logits = output_text[:, -1, :]\n",
    "                    next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "                    next_token_id = torch.argmax(next_token_probs, dim=-1).item()\n",
    "                    generated_tokens.append(next_token_id)\n",
    "                    \n",
    "                    generated_caption = insert_zeros(generated_tokens)\n",
    "                    generated_caption = tokenizer.decode(generated_caption, skip_special_tokens=False)\n",
    "                    generated_caption = generated_caption.replace(\"<pad>\", \" \").replace(\"  \", \" \").split()\n",
    "                    generated_caption = [word for word in generated_caption if word[0] != \"<\"]\n",
    "                    generated_caption = \" \".join(generated_caption)\n",
    "                                               \n",
    "                    text = textExtraction([generated_caption]).to(device)\n",
    "                    text = text.transpose(0, 1)\n",
    "                    \n",
    "                    if next_token_id in gemmaConfig.eos_token_id or len(generated_caption.split()) > max_length: \n",
    "                        #<eos> = 1; <end_of_turn> = 107\n",
    "                        lastTurn = True"
   ],
   "id": "6df9340e53bfbe2a",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Discriminator",
   "id": "e431f844eea97c94"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T00:22:44.163311Z",
     "start_time": "2024-10-21T00:22:44.148193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Generator\n",
    "        self.g_linearFake = nn.Linear(256000, 768)\n",
    "        self.g_con_mlp1 = nn.Linear(768, 2)\n",
    "        self.g_con_mlp2 = nn.Linear(128, 1)\n",
    "        self.g_unc_mlp1 = nn.Linear(768, 1)\n",
    "        self.g_unc_mlp2 = nn.Linear(64, 1)\n",
    "        # Discriminator\n",
    "        self.d_linearFake = nn.Linear(gemmaConfig.vocab_size, 768)\n",
    "        self.d_con_mlp1_r2f = nn.Linear(768, 2)\n",
    "        self.d_con_mlp2_r2f = nn.Linear(256, 1)\n",
    "        self.d_con_mlp1_f2r = nn.Linear(768, 2)\n",
    "        self.d_con_mlp2_f2r = nn.Linear(256, 1)\n",
    "        self.d_con_mlp1_g = nn.Linear(768, 2)\n",
    "        self.d_con_mlp2_g = nn.Linear(128, 1)\n",
    "        self.d_con_mlp1_m = nn.Linear(768, 2)\n",
    "        self.d_con_mlp2_m = nn.Linear(128, 1)\n",
    "        self.d_unc_mlp1_r = nn.Linear(768, 1)\n",
    "        self.d_unc_mlp2_r = nn.Linear(64, 1)\n",
    "        self.d_unc_mlp1_g = nn.Linear(768, 1)\n",
    "        self.d_unc_mlp2_g = nn.Linear(64, 1)\n",
    "        self.d_unc_mlp1_m = nn.Linear(768, 1)\n",
    "        self.d_unc_mlp2_m = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, real_text, fake_text, image, GorD): \n",
    "        # real_text = [batch_size, 64, 768]\n",
    "        # fake_text = [batch_size, 256, 256000]\n",
    "        # image = [batch_size, 64, 768]\n",
    "        \n",
    "        if GorD == \"G\":\n",
    "            g_fake_text = self.g_linearFake(fake_text)\n",
    "            g_fake_text = self.g_linearFake(fake_text)\n",
    "            g_C_g = torch.cat((g_fake_text, image), dim=1)\n",
    "            ########################  conditional  ########################\n",
    "            g_C_g = self.g_con_mlp1(g_C_g)\n",
    "            g_C_g = self.g_con_mlp2(g_C_g.transpose(1,2)).squeeze(-1)\n",
    "            ###############################################################\n",
    "            ######################## unconditional ########################\n",
    "            g_UC_g = self.g_unc_mlp1(g_fake_text).squeeze(-1)\n",
    "            g_UC_g = self.g_unc_mlp2(g_UC_g).squeeze(-1)\n",
    "            ###############################################################\n",
    "            return g_C_g, g_UC_g\n",
    "        \n",
    "        elif GorD == \"D\":\n",
    "            d_fake_text = self.d_linearFake(fake_text)\n",
    "            mismatched_text = torch.roll(real_text, 1, 0)\n",
    "            C_r = torch.cat((real_text, image), dim=1)\n",
    "            C_g = torch.cat((d_fake_text, image), dim=1)\n",
    "            C_m = torch.cat((mismatched_text, image), dim=1)\n",
    "            # contrastive discriminator\n",
    "            d_C_r2f = torch.cat((C_r, C_g), dim=1)\n",
    "            d_C_f2r = torch.cat((C_g, C_r), dim=1)\n",
    "            \n",
    "            ######################## conditional ########################\n",
    "            d_C_r2f = self.d_con_mlp1_r2f(d_C_r2f)\n",
    "            d_C_f2r = self.d_con_mlp1_f2r(d_C_f2r)\n",
    "            d_C_g = self.d_con_mlp1_g(C_g)\n",
    "            d_C_m = self.d_con_mlp1_m(C_m)\n",
    "            \n",
    "            d_C_r2f = self.d_con_mlp2_r2f(d_C_r2f.transpose(1,2)).squeeze(-1).unsqueeze(0)\n",
    "            d_C_f2r = self.d_con_mlp2_r2f(d_C_f2r.transpose(1,2)).squeeze(-1).unsqueeze(0)\n",
    "            d_C_g = self.d_con_mlp2_g(d_C_g.transpose(1,2)).squeeze(-1).unsqueeze(0)\n",
    "            d_C_m = self.d_con_mlp2_m(d_C_m.transpose(1,2)).squeeze(-1).unsqueeze(0)\n",
    "            \n",
    "            d_con_output = torch.cat((d_C_r2f, d_C_f2r, d_C_g, d_C_m), dim=0)\n",
    "            ###############################################################\n",
    "            ######################## unconditional ########################\n",
    "            d_UC_r  = self.d_unc_mlp1_r(real_text).squeeze(-1)\n",
    "            d_UC_g  = self.d_unc_mlp1_g(d_fake_text).squeeze(-1)\n",
    "            d_UC_m  = self.d_unc_mlp1_m(mismatched_text).squeeze(-1)\n",
    "            \n",
    "            d_UC_r = self.d_unc_mlp2_r(d_UC_r).squeeze(-1).unsqueeze(0)\n",
    "            d_UC_g = self.d_unc_mlp2_g(d_UC_g).squeeze(-1).unsqueeze(0)\n",
    "            d_UC_m = self.d_unc_mlp2_m(d_UC_m).squeeze(-1).unsqueeze(0)\n",
    "            \n",
    "            d_unc_output = torch.cat((d_UC_r, d_UC_g, d_UC_m), dim=0)\n",
    "            ###############################################################\n",
    "            return d_con_output, d_unc_output"
   ],
   "id": "ff75f88ea94b5045",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T00:22:44.513082Z",
     "start_time": "2024-10-21T00:22:44.228304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# empty cuda memory\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "id": "d5c9b5f31e3aa208",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T00:22:44.583842Z",
     "start_time": "2024-10-21T00:22:44.578023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "id": "d7e3bfcce1a75fbd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T00:22:50.546922Z",
     "start_time": "2024-10-21T00:22:44.647264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NetG = Generator().to(device)\n",
    "NetD = Discriminator().to(device)\n",
    "optimizer_G = optim.Adam(NetG.parameters(), lr=0.001)\n",
    "optimizer_D = optim.Adam(NetD.parameters(), lr=0.001)\n",
    "train_losses_FC = []\n",
    "train_losses_G = []\n",
    "train_losses_D = []\n",
    "test_losses_FC = []\n",
    "test_losses_G = []\n",
    "test_losses_D = []\n",
    "save = []\n",
    "present_epoch = 1\n",
    "best_train_loss_FC = 9999\n",
    "best_train_loss_G = 9999\n",
    "best_train_loss_D = 9999\n",
    "best_test_loss_FC = 9999\n",
    "best_test_loss_G = 9999\n",
    "best_test_loss_D = 9999\n",
    "loss_data = pd.DataFrame()\n",
    "\n",
    "checkpoint = False\n",
    "if checkpoint:\n",
    "    checkpoint_G = torch.load('./Model/test_save/test_save_2NetG.pth')\n",
    "    checkpoint_D = torch.load('./Model/test_save/test_save_2NetD.pth')\n",
    "    NetG.load_state_dict(checkpoint_G['model_state_dict'])\n",
    "    NetD.load_state_dict(checkpoint_D['model_state_dict'])\n",
    "    optimizer_G.load_state_dict(checkpoint_G['optimizer_state_dict'])\n",
    "    optimizer_D.load_state_dict(checkpoint_D['optimizer_state_dict'])\n",
    "    train_losses_FC.append(checkpoint_G['FC_loss'])\n",
    "    train_losses_G.append(checkpoint_G['G_loss'])\n",
    "    train_losses_D.append(checkpoint_G['D_loss'])\n",
    "    present_epoch = checkpoint_G['epoch'] + 1\n",
    "\n",
    "    \n",
    "\n",
    "funnyScoreLoss = nn.MSELoss()\n",
    "\n",
    "def generatorLoss(condition_logits, uncondition_logits):\n",
    "    result_fake = (torch.zeros(uncondition_logits.shape[0])).to(device)\n",
    "    con_loss = CrossEntropyLoss()(condition_logits, result_fake.to(torch.long))\n",
    "    unc_loss = BCEWithLogitsLoss()(uncondition_logits, result_fake)\n",
    "    loss = con_loss + unc_loss\n",
    "    return loss\n",
    "\n",
    "def discriminatorLoss(condition_logits, uncondition_logits):\n",
    "    result_true = (torch.ones(uncondition_logits[0].shape[0])).to(device)\n",
    "    result_fake = (torch.zeros(uncondition_logits[0].shape[0])).to(device)\n",
    "    \n",
    "    con_r2f = CrossEntropyLoss()(condition_logits[0], result_fake.to(torch.long))\n",
    "    con_f2r = CrossEntropyLoss()(condition_logits[1], result_fake.to(torch.long))\n",
    "    con_f = CrossEntropyLoss()(condition_logits[2], result_fake.to(torch.long))\n",
    "    con_m = CrossEntropyLoss()(condition_logits[3], result_fake.to(torch.long))\n",
    "    unc_r = BCEWithLogitsLoss()(uncondition_logits[0], result_true)\n",
    "    unc_f = BCEWithLogitsLoss()(uncondition_logits[1], result_fake)\n",
    "    unc_m = BCEWithLogitsLoss()(uncondition_logits[2], result_fake)\n",
    "    loss = ((con_r2f + con_f2r)/2) + ((con_f + con_m)/2) + unc_r + ((unc_f + unc_m)/2)\n",
    "    return loss"
   ],
   "id": "9dc4843819037573",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-21T00:26:31.565145Z",
     "start_time": "2024-10-21T00:23:03.790150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_name = '20241021'\n",
    "if not os.path.exists('./Model/'+save_name):\n",
    "    os.makedirs('./Model/'+save_name)\n",
    "    \n",
    "epochs = 30\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for epoch in range(epochs):\n",
    "    print(\"---------------------------------------- epoch \"+ str(epoch + present_epoch) +\" ---------------------------------------\")\n",
    "    train_loss_FC = 0\n",
    "    train_loss_G = 0\n",
    "    train_loss_D = 0\n",
    "    test_loss_FC = 0\n",
    "    test_loss_G = 0\n",
    "    test_loss_D = 0\n",
    "    \n",
    "    ###################################### Train ######################################\n",
    "    with tqdm(train_loader, unit=\"batch\", leave=True) as tepoch:\n",
    "        for idx, (text, image, funny_score) in enumerate(tepoch):  \n",
    "            tepoch.set_postfix({'Now': tepoch.format_dict['elapsed'], 'Status': \" New batch preprocessing\"})\n",
    "            text = textExtraction(tokenizer, gemmaConfig, text)\n",
    "            tepoch.set_postfix({'Now': tepoch.format_dict['elapsed'], 'Status': \" textExtraction Done\"})\n",
    "            # print(text.shape, image.shape, funny_score.shape)\n",
    "            # torch.Size([32, 64, 768]) torch.Size([32, 64, 768]) torch.Size([32, 1])\n",
    "            ######################################################\n",
    "            # (1) Update Generator network\n",
    "            ######################################################\n",
    "            optimizer_G.zero_grad()\n",
    "            # logits_detach = logits.clone().detach()\n",
    "            tepoch.set_postfix({'Now': tepoch.format_dict['elapsed'], 'Status': \" Generator Forward\"})\n",
    "            logits, output_funny_score = NetG(text.to(device), image.to(device))\n",
    "            g_con_logits, g_unc_logits = NetD(text.to(device), logits, image.to(device), \"G\")\n",
    "            \n",
    "            tepoch.set_postfix({'Now': tepoch.format_dict['elapsed'], 'Status': \" Generator Backward - FunnyScore\"})\n",
    "            loss_FC = funnyScoreLoss(output_funny_score.to(device), funny_score.to(device).to(torch.float32))\n",
    "            loss_FC.backward(retain_graph=True)\n",
    "            train_loss_FC += loss_FC.item()\n",
    "\n",
    "            tepoch.set_postfix({'Now': tepoch.format_dict['elapsed'], 'Status': \" Generator Backward - Generator\"})\n",
    "            loss_G = generatorLoss(g_con_logits.to(device), g_unc_logits.to(device))\n",
    "            loss_G.backward()\n",
    "            train_loss_G += loss_G.item()\n",
    "\n",
    "\n",
    "            optimizer_G.step()\n",
    "            # tepoch.set_postfix({'Now': tepoch.format_dict['elapsed'], 'Status': \" Generator Backward - Generator\"})\n",
    "            # loss_G = generatorLoss(g_con_logits.to(device), g_unc_logits.to(device))\n",
    "            # loss_G.backward(retain_graph=True)\n",
    "            # train_loss_G += loss_G.item()\n",
    "            # \n",
    "            # tepoch.set_postfix({'Now': tepoch.format_dict['elapsed'], 'Status': \" Generator Backward - FunnyScore\"})\n",
    "            # loss_FC = funnyScoreLoss(output_funny_score.to(device), funny_score.to(device).to(torch.float32))\n",
    "            # loss_FC.backward()\n",
    "            # train_loss_FC += loss_FC.item()\n",
    "            # optimizer_G.step()\n",
    "            ######################################################\n",
    "            # (4) Update Discriminator network\n",
    "            ######################################################\n",
    "            optimizer_D.zero_grad()\n",
    "            tepoch.set_postfix({'Now': tepoch.format_dict['elapsed'], 'Status': \" Discriminator Forward\"})\n",
    "            d_con_logits, d_unc_logits = NetD(text.to(device).detach(), logits.detach(), image.to(device).detach(), \"D\")\n",
    "            tepoch.set_postfix({'Now': tepoch.format_dict['elapsed'], 'Status': \" Discriminator Backward\"})\n",
    "            loss_D = discriminatorLoss(d_con_logits.to(device), d_unc_logits.to(device))\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "            train_loss_D += loss_D.item()\n",
    "            ######################################################\n",
    "            tepoch.set_postfix({'FC_loss': train_loss_FC/ (idx+1), 'G_loss': train_loss_G/ (idx+1), 'D_loss': train_loss_D/ (idx+1)})\n",
    "            ######################################################\n",
    "    train_loss_FC /= len(train_loader)\n",
    "    train_loss_G /= len(train_loader)\n",
    "    train_loss_D /= len(train_loader)\n",
    "    train_losses_FC.append(train_loss_FC)\n",
    "    train_losses_G.append(train_loss_G)\n",
    "    train_losses_D.append(train_loss_D)\n",
    "    ###################################### Train ######################################\n",
    "    \n",
    "    \n",
    "    ######################################  Test ######################################\n",
    "    with tqdm(test_loader, unit=\"batch\", leave=True) as tepoch:\n",
    "        tepoch.set_postfix({'Now': \" New batch preprocessing\"})\n",
    "        for idx, (text, image, funny_score) in enumerate(tepoch):            \n",
    "            text = textExtraction(tokenizer, gemmaConfig, text)\n",
    "            # Generator\n",
    "            tepoch.set_postfix({'Now': \" Generator\"})\n",
    "            logits, output_funny_score = NetG(text.to(device), image.to(device))\n",
    "            # Discriminator\n",
    "            tepoch.set_postfix({'Now': \" Discriminator - Generator\"})\n",
    "            g_con_logits, g_unc_logits = NetD(text.to(device), logits, image.to(device), \"G\")\n",
    "            tepoch.set_postfix({'Now': \" Discriminator - Discriminator\"})\n",
    "            d_con_logits, d_unc_logits = NetD(text.to(device), logits.clone().detach(), image.to(device), \"D\")\n",
    "            # loss\n",
    "            tepoch.set_postfix({'Now': \" Computing loss\"})\n",
    "            loss_FC = funnyScoreLoss(output_funny_score, funny_score.to(device))\n",
    "            loss_G = generatorLoss(g_con_logits, g_unc_logits)\n",
    "            loss_D = discriminatorLoss(d_con_logits, d_unc_logits)\n",
    "            test_loss_FC += loss_FC.item()\n",
    "            test_loss_G += loss_G.item()\n",
    "            test_loss_D += loss_D.item()\n",
    "            tepoch.set_postfix({'FC_loss': test_loss_FC/ (idx+1), 'G_loss': test_loss_G/ (idx+1), 'D_loss': test_loss_D/ (idx+1)})\n",
    "    test_loss_FC /= len(test_loader)\n",
    "    test_loss_G /= len(test_loader)\n",
    "    test_loss_D /= len(test_loader)\n",
    "    test_losses_FC.append(test_loss_FC)\n",
    "    test_losses_G.append(test_loss_G)\n",
    "    test_losses_D.append(test_loss_D)\n",
    "    ######################################  Test ######################################\n",
    "    \n",
    "    ######################################  Save ######################################\n",
    "    hasSaved = False\n",
    "    # 任一個loss小於最佳loss就存檔\n",
    "    if train_loss_FC < best_train_loss_FC and test_loss_FC < best_test_loss_FC:\n",
    "        best_train_loss_FC = train_loss_FC\n",
    "        best_test_loss_FC = test_loss_FC\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    if train_loss_G < best_train_loss_G and test_loss_G < best_test_loss_G:\n",
    "        best_train_loss_G = train_loss_G\n",
    "        best_test_loss_G = test_loss_G\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    if train_loss_D < best_train_loss_D and test_loss_D < best_test_loss_D:\n",
    "        best_train_loss_D = train_loss_D\n",
    "        best_test_loss_D = test_loss_D\n",
    "        hasSaved = True\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetG.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_G.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetG_'+ str(epoch + present_epoch) +'.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + present_epoch,\n",
    "            'model_state_dict': NetD.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_D.state_dict(),\n",
    "            'FC_loss': loss_FC,\n",
    "            'G_loss': loss_G,\n",
    "            'D_loss': loss_D,\n",
    "        }, './Model/' + save_name + \"/\" + save_name + '_NetD_'+ str(epoch + present_epoch) +'.pth')\n",
    "    \n",
    "    if hasSaved:\n",
    "        save.append(\"V\")\n",
    "    else:\n",
    "        save.append(\" \")\n",
    "\n",
    "    loss_data['train_FC'] = train_losses_FC\n",
    "    loss_data['train_G'] = train_losses_G\n",
    "    loss_data['train_D'] = train_losses_D\n",
    "    loss_data['test_FC'] = test_losses_FC\n",
    "    loss_data['test_G'] = test_losses_G\n",
    "    loss_data['test_D'] = test_losses_D\n",
    "    loss_data['save'] = save\n",
    "    loss_data.to_csv('./Model/' + save_name + \"/\" + save_name + '_loss.csv', index=False)\n",
    "    ######################################  Save ######################################\n",
    "# 0%|          | 4/84954 [01:24<513:13:51, 21.75s/batch, FC_loss=1.33, G_loss=1.71, D_loss=37.2]\n",
    "# 0%|          | 118/84954 [40:27<488:29:01, 20.73s/batch, FC_loss=0.0704, G_loss=1.73, D_loss=2.83]"
   ],
   "id": "bb691e03b98cb61e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- epoch 1 ---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/84952 [03:26<609:32:12, 25.83s/batch, Now=201, Status=Generator Forward]               \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 30\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# logits_detach = logits.clone().detach()\u001B[39;00m\n\u001B[0;32m     29\u001B[0m tepoch\u001B[38;5;241m.\u001B[39mset_postfix({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNow\u001B[39m\u001B[38;5;124m'\u001B[39m: tepoch\u001B[38;5;241m.\u001B[39mformat_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124melapsed\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStatus\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Generator Forward\u001B[39m\u001B[38;5;124m\"\u001B[39m})\n\u001B[1;32m---> 30\u001B[0m logits, output_funny_score \u001B[38;5;241m=\u001B[39m \u001B[43mNetG\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m g_con_logits, g_unc_logits \u001B[38;5;241m=\u001B[39m NetD(text\u001B[38;5;241m.\u001B[39mto(device), logits, image\u001B[38;5;241m.\u001B[39mto(device), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mG\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     33\u001B[0m tepoch\u001B[38;5;241m.\u001B[39mset_postfix({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNow\u001B[39m\u001B[38;5;124m'\u001B[39m: tepoch\u001B[38;5;241m.\u001B[39mformat_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124melapsed\u001B[39m\u001B[38;5;124m'\u001B[39m], \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mStatus\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Generator Backward - FunnyScore\u001B[39m\u001B[38;5;124m\"\u001B[39m})\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[9], line 60\u001B[0m, in \u001B[0;36mGenerator.forward\u001B[1;34m(self, text, image)\u001B[0m\n\u001B[0;32m     58\u001B[0m feature_fusion \u001B[38;5;241m=\u001B[39m feature_fusion\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     59\u001B[0m \u001B[38;5;66;03m####################### gemma  generate #######################\u001B[39;00m\n\u001B[1;32m---> 60\u001B[0m last_hidden_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgemmaGenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeature_fusion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m output_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgemmaLm_head(last_hidden_state)\n\u001B[0;32m     62\u001B[0m \u001B[38;5;66;03m###############################################################\u001B[39;00m\n\u001B[0;32m     63\u001B[0m \n\u001B[0;32m     64\u001B[0m \u001B[38;5;66;03m######################### funny score #########################\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[9], line 32\u001B[0m, in \u001B[0;36mGenerator.gemmaGenerate\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     30\u001B[0m     toGemma \u001B[38;5;241m=\u001B[39m textExtractReverse(top_k_indices)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m# 使用gemma作為model的一部分\u001B[39;00m\n\u001B[1;32m---> 32\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgemma\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoGemma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;66;03m# output[0] = last_hidden_state\u001B[39;00m\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;66;03m# output[1] = past_key_values\u001B[39;00m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 250\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\gemma2\\modeling_gemma2.py:890\u001B[0m, in \u001B[0;36mGemma2Model.forward\u001B[1;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[0m\n\u001B[0;32m    879\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[0;32m    880\u001B[0m         decoder_layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[0;32m    881\u001B[0m         hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    887\u001B[0m         cache_position,\n\u001B[0;32m    888\u001B[0m     )\n\u001B[0;32m    889\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 890\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    891\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    892\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    893\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    894\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    895\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    896\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    897\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    898\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    900\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    902\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\gemma2\\modeling_gemma2.py:618\u001B[0m, in \u001B[0;36mGemma2DecoderLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001B[0m\n\u001B[0;32m    616\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[0;32m    617\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpre_feedforward_layernorm(hidden_states)\n\u001B[1;32m--> 618\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    619\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost_feedforward_layernorm(hidden_states)\n\u001B[0;32m    620\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\gemma2\\modeling_gemma2.py:84\u001B[0m, in \u001B[0;36mGemma2MLP.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     83\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 84\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdown_proj(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact_fn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgate_proj(x)) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mup_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:37:30.366022600Z",
     "start_time": "2024-10-03T00:46:37.439345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_losses_FC, label='train')\n",
    "plt.plot(train_losses_G, label='train')\n",
    "plt.plot(train_losses_D, label='train')\n",
    "plt.plot(test_losses_FC, label='test')\n",
    "plt.plot(test_losses_G, label='test')\n",
    "plt.plot(test_losses_D, label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# save plot\n",
    "plt.savefig('./Model/' + save_name + \"/\" + save_name + '_loss.png')"
   ],
   "id": "65770e7128728d32",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Generate Test",
   "id": "4c44d1e70ced399f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:37:30.366022600Z",
     "start_time": "2024-10-14T06:05:20.980447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#load model\n",
    "NetG = Generator().to(device)\n",
    "NetD = Discriminator().to(device)\n",
    "NetG.load_state_dict(torch.load('./Model/test_batch32/test_batch32_5NetG.pth'))\n",
    "NetD.load_state_dict(torch.load('./Model/test_batch32/test_batch32_5NetD.pth'))\n",
    "# train with load model\n",
    "NetG.train()\n",
    "NetD.train()\n"
   ],
   "id": "c3f72adc27a5f410",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#load model\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m NetG \u001B[38;5;241m=\u001B[39m \u001B[43mGenerator\u001B[49m()\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      3\u001B[0m NetD \u001B[38;5;241m=\u001B[39m Discriminator()\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      4\u001B[0m NetG\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./Model/test_batch32/test_batch32_5NetG.pth\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'Generator' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T07:37:30.366022600Z",
     "start_time": "2024-10-02T23:43:27.978751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generate\n",
    "NetG.eval()\n",
    "NetD.eval()\n",
    "image = imageExtraction(\"./test_img.jpg\")\n",
    "output = NetG.generate(image, 200)\n",
    "output"
   ],
   "id": "aa601343f448bd19",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 991.56it/s]\n",
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.12it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.80it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.55it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 998.64it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.10it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.62it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.60it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 999.83it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.07it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.16it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 499.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.2329,  0.4325,  0.4199,  ..., -0.4702, -0.1142, -0.0098]],\n",
       " \n",
       "         [[ 0.1089,  0.3625,  0.8224,  ...,  0.2763,  0.0975, -0.0043]],\n",
       " \n",
       "         [[-0.1282, -0.1454, -0.1592,  ...,  0.1126,  0.7090,  0.6611]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.4177,  0.1868,  0.0531,  ..., -0.5458,  0.0338, -1.4983]],\n",
       " \n",
       "         [[-0.1912,  0.1032,  0.4763,  ...,  0.7547,  0.7066, -0.5460]],\n",
       " \n",
       "         [[-0.1003, -0.3331, -0.0245,  ..., -0.5132,  0.0633,  0.8948]]],\n",
       "        device='cuda:0'),\n",
       " [2,\n",
       "  540,\n",
       "  235248,\n",
       "  236193,\n",
       "  235248,\n",
       "  235248,\n",
       "  235250,\n",
       "  235274,\n",
       "  35351,\n",
       "  235254,\n",
       "  605,\n",
       "  6935,\n",
       "  235276,\n",
       "  235248,\n",
       "  235248,\n",
       "  235248,\n",
       "  132385,\n",
       "  235248,\n",
       "  235265,\n",
       "  235248,\n",
       "  2173,\n",
       "  235274,\n",
       "  235248,\n",
       "  2465,\n",
       "  3682,\n",
       "  236193,\n",
       "  18824,\n",
       "  235274,\n",
       "  235248,\n",
       "  11200,\n",
       "  235276,\n",
       "  235276,\n",
       "  616,\n",
       "  235248,\n",
       "  235248,\n",
       "  235248,\n",
       "  235274,\n",
       "  235248,\n",
       "  235248,\n",
       "  2012,\n",
       "  235276,\n",
       "  2012,\n",
       "  236193,\n",
       "  235248,\n",
       "  235248,\n",
       "  235248,\n",
       "  235265,\n",
       "  618,\n",
       "  235276,\n",
       "  669,\n",
       "  235248,\n",
       "  235248,\n",
       "  24255,\n",
       "  618,\n",
       "  14383,\n",
       "  236193,\n",
       "  235248,\n",
       "  235362,\n",
       "  236193,\n",
       "  5862,\n",
       "  1420,\n",
       "  236193,\n",
       "  235248,\n",
       "  235248,\n",
       "  46816,\n",
       "  235248,\n",
       "  235248,\n",
       "  1820,\n",
       "  235248,\n",
       "  235276,\n",
       "  235265,\n",
       "  690,\n",
       "  235256,\n",
       "  235274,\n",
       "  236193,\n",
       "  235256,\n",
       "  235248,\n",
       "  235248,\n",
       "  235248,\n",
       "  236193,\n",
       "  819,\n",
       "  235248,\n",
       "  235248,\n",
       "  235254,\n",
       "  235265,\n",
       "  84521,\n",
       "  3550,\n",
       "  235276,\n",
       "  42113,\n",
       "  235265,\n",
       "  235345,\n",
       "  509,\n",
       "  235248,\n",
       "  235276,\n",
       "  4943,\n",
       "  235248,\n",
       "  235248,\n",
       "  236193,\n",
       "  53152,\n",
       "  235269,\n",
       "  235265],\n",
       " tensor([-0.4983], device='cuda:0'))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
