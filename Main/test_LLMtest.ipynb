{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T07:43:00.670579Z",
     "start_time": "2024-10-30T07:42:59.232321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ],
   "id": "7123375a60e24a10",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TonyLab\\PycharmProjects\\MemeGAN\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. LLM Test",
   "id": "807b418cfd673b81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-30T16:21:23.472827Z",
     "start_time": "2024-09-30T16:21:22.108642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 不確定是否為官方的 Gemini ############################################################################\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"describeai/gemini\")\n",
    "gemini = AutoModelForSeq2SeqLM.from_pretrained(\"describeai/gemini\")\n",
    "#######################################################################################################\n",
    "gemini"
   ],
   "id": "9368441165fc2667",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 1024)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 1024)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 16)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-23): 23 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (k): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "              (wo): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T23:58:44.394161Z",
     "start_time": "2024-10-13T23:58:29.190541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 官方的Gemma #########################################################################################\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\", device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "########################################################################################################\n",
    "print(tokenizer.all_special_ids)\n",
    "print(tokenizer.all_special_tokens)"
   ],
   "id": "2b63e7fd2e5f833e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8cff812cfdd4dfbb1c35b9312bf4baf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 3, 0, 106, 107]\n",
      "['<bos>', '<eos>', '<unk>', '<pad>', '<start_of_turn>', '<end_of_turn>']\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T07:43:10.319747Z",
     "start_time": "2024-10-30T07:43:00.688690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 官方的Gemma (Instruction-tuning)#####################################################################\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\", device_map=\"auto\",  torch_dtype=torch.bfloat16)\n",
    "########################################################################################################\n",
    "print(tokenizer.all_special_ids)\n",
    "print(tokenizer.all_special_tokens)"
   ],
   "id": "e5f8f2c83fc25118",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 3, 0, 106, 107]\n",
      "['<bos>', '<eos>', '<unk>', '<pad>', '<start_of_turn>', '<end_of_turn>']\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:17:41.566272Z",
     "start_time": "2024-10-30T08:17:41.562627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a= torch.tensor([[[0.100,0.2], [0.2000 ,0.300], [0.300, 0.300]],[[0.100,0.2], [0.2000 ,0.300], [0.300, 0.300]]]).to(torch.bfloat16)\n",
    "a.shape"
   ],
   "id": "3461a40b17010150",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T08:19:13.946919Z",
     "start_time": "2024-10-30T08:19:13.941042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#mean dim -1\n",
    "print(torch.mean(a, dim=-2).shape)\n",
    "torch.mean(a, dim=-2)"
   ],
   "id": "7ffc8bdc26b86e5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.2002, 0.2676],\n",
       "        [0.2002, 0.2676]], dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T10:22:22.871007Z",
     "start_time": "2024-10-30T10:22:21.603126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Complete sentence with following elements : apple, nice, cool, amaze, shiny, device, format, pants, rainy, smooth, compute, ear, card, normal, pacific. \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=64).to(device)\n",
    "# Complete會出填空題給我\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "4254f62c77c9c2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><bos>Complete sentence with following elements : apple, nice, cool, amaze, shiny, device, format, pants, rainy, smooth, compute, ear, card, normal, pacific. \n",
      "\n",
      "**The cool apple device, with its shiny surface and smooth compute format, amaze me. I'm wearing my normal pants, and it's a rainy day in the Pacific.** \n",
      "\n",
      "Let me know if you'd like to try another sentence! \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Complete normal 會出填空題給我",
   "id": "cded1941df791bc7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T19:13:07.038273Z",
     "start_time": "2024-10-06T19:12:41.895661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Complete sentence with following elements : apple, nice, cool, amaze. \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# Complete會出填空題給我\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "4b26218c515cd834",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Complete sentence with following elements : apple, nice, cool, amaze. 1. I was really _________________ when I saw the new iPhone. 2. The new iPhone is really _________________. 3. The new iPhone is really _________________. 4. The new iPhone is really _________________. 5. The new iPhone is really _________________. 6. The new iPhone is really _________________. 7. The new iPhone is really _________________. 8. The new iPhone is really _________________. 9. The new iPhone is really _________________. 10. The new iPhone is really _________________.\n",
      "\n",
      "Complete sentence with following elements : apple, nice, cool, amaze. 1. I was really _________________ when I saw the new iPhone. 2. The new iPhone is really _________________. 3. The new iPhone is really _________________. 4. The new iPhone is really _________________. 5. The new iPhone is really _________________. 6. The new iPhone is really ________________\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:30:26.214900Z",
     "start_time": "2024-10-07T23:30:24.245284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Complete sentence with following elements : apple, nice, cool, amaze. \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# Complete會出填空題給我\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "83d7cf693e00d3c7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\PycharmProjects\\Python3.11.9\\pythonProject\\.venv\\Lib\\site-packages\\transformers\\models\\gemma2\\modeling_gemma2.py:458: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Complete sentence with following elements : apple, nice, cool, amaze. \n",
      "\n",
      "The apple was **nice** and **cool** to the touch. It **amazed** me with its smooth, shiny skin. \n",
      "\n",
      "\n",
      "Let me know if you'd like to try another sentence! \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# involve normal會開始比賽",
   "id": "11444641255d03fe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T19:09:25.071988Z",
     "start_time": "2024-10-06T19:08:59.717114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Generate a funny caption involve following words: apple, nice, cool, amaze. \"\n",
    "# 用involve會開始比賽\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "f870303128f891fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Generate a funny caption involve following words: apple, nice, cool, amaze. \n",
      "\n",
      "The best caption will be chosen by the author of the post.\n",
      "\n",
      "The winner will be announced on the 1st of December.\n",
      "\n",
      "The winner will receive a 1000 coins.\n",
      "\n",
      "The winner will be chosen by the author of the post.\n",
      "\n",
      "The winner will be announced on the 1st of December.\n",
      "\n",
      "The winner will receive a 1000 coins.\n",
      "\n",
      "The winner will be chosen by the author of the post.\n",
      "\n",
      "The winner will be announced on the 1st of December.\n",
      "\n",
      "The winner will receive a 1000 coins.\n",
      "\n",
      "The winner will be chosen by the author of the post.\n",
      "\n",
      "The winner will be announced on the 1st of December.\n",
      "\n",
      "The winner will receive a 1000 coins.\n",
      "\n",
      "The winner will be chosen by the author of the post.\n",
      "\n",
      "The winner will be announced on the 1st of December.\n",
      "\n",
      "The winner will receive a 1000 coins\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:31:57.345332Z",
     "start_time": "2024-10-07T23:31:54.377455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Generate a funny caption involve following words: apple, nice, cool, amaze. \"\n",
    "# 用involve會開始比賽\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "b64bddc9a108ee9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Generate a funny caption involve following words: apple, nice, cool, amaze. \n",
      "\n",
      "Here are some ideas:\n",
      "\n",
      "**Option 1:**\n",
      "\n",
      "\"This apple is so nice, it's cool to the touch and amazes me with its perfect shape. 🍎\"\n",
      "\n",
      "**Option 2:**\n",
      "\n",
      "\"This apple is so amazeballs, it's nice enough to eat and cool enough to be a conversation starter. 🍎\"\n",
      "\n",
      "**Option 3:**\n",
      "\n",
      "\"I'm not sure what's more amazeballs, this apple or the fact that I found it in my fridge. 🍎\"\n",
      "\n",
      "\n",
      "Choose the best option for you and add your own personal touch! \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# elements normal會拆解詞性",
   "id": "cf558f3a38fced89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T19:13:30.631549Z",
     "start_time": "2024-10-06T19:13:07.105374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Create a funny sentence that contains following elements: apple, nice, cool, amaze. \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "4869cf8b21256080",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Create a funny sentence that contains following elements: apple, nice, cool, amaze. \n",
      "\n",
      "Write a sentence that contains the following elements:\n",
      "\n",
      "* a noun\n",
      "* a verb\n",
      "* an adjective\n",
      "* an adverb\n",
      "\n",
      "Write a sentence that contains the following elements:\n",
      "\n",
      "* a noun\n",
      "* a verb\n",
      "* an adjective\n",
      "* an adverb\n",
      "\n",
      "Write a sentence that contains the following elements:\n",
      "\n",
      "* a noun\n",
      "* a verb\n",
      "* an adjective\n",
      "* an adverb\n",
      "\n",
      "Write a sentence that contains the following elements:\n",
      "\n",
      "* a noun\n",
      "* a verb\n",
      "* an adjective\n",
      "* an adverb\n",
      "\n",
      "Write a sentence that contains the following elements:\n",
      "\n",
      "* a noun\n",
      "* a verb\n",
      "* an adjective\n",
      "* an adverb\n",
      "\n",
      "Write a sentence that contains the following elements:\n",
      "\n",
      "* a noun\n",
      "* a verb\n",
      "* an adjective\n",
      "* an adverb\n",
      "\n",
      "Write a sentence that contains the following elements:\n",
      "\n",
      "* a noun\n",
      "* a verb\n",
      "* an adjective\n",
      "* an adverb\n",
      "\n",
      "Write a sentence that contains the following elements:\n",
      "\n",
      "* a noun\n",
      "* a verb\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:32:44.372841Z",
     "start_time": "2024-10-07T23:32:43.187088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Create a funny sentence that contains following elements: apple, nice, cool, amaze. \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "44c24296d5ba4ee3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Create a funny sentence that contains following elements: apple, nice, cool, amaze. \n",
      "\n",
      "An apple a day keeps the doctor away, but a nice, cool, amaze-ing apple a day keeps the doctor away from your house! \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# elements normal會拆解詞性",
   "id": "7340014498705ffc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T19:24:30.432420Z",
     "start_time": "2024-10-06T19:24:11.997518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write one Post for Instagram possibly with emoji that contains following words: apple, nice, cool, amaze. Answer:\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "5377edd9dc68f992",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write one Post for Instagram possibly with emoji that contains following words: apple, nice, cool, amaze.Answer: Apple is a fruit that is very popular in the world. It is a fruit that is very delicious and has a lot of benefits for the body. Apple is a fruit that is very popular in the world. It is a fruit that is very delicious and has a lot of benefits for the body. Apple is a fruit that is very popular in the world. It is a fruit that is very delicious and has a lot of benefits for the body. Apple is a fruit that is very popular in the world. It is a fruit that is very delicious and has a lot of benefits for the body. Apple is a fruit that is very popular in the world. It is a fruit that is very delicious and has a lot of benefits for the body. Apple is a fruit that is very popular in the world. It is a fruit that is very delicious and has a lot of benefits for the body. Apple is a fruit that is very popular in the world. It is a fruit that is very delicious\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:33:16.059604Z",
     "start_time": "2024-10-07T23:33:11.565017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write one Post for Instagram possibly with emoji that contains following words: apple, nice, cool, amaze. Answer:\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "860ec2ff3bfb94ef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write one Post for Instagram possibly with emoji that contains following words: apple, nice, cool, amaze. Answer:\n",
      "\n",
      "**What's your favorite thing about fall? 🍂🍎  I love the crisp air, the colorful leaves, and the delicious apple cider! 🍎  It's just so nice and cool, and it amazes me how much nature can change in just a few weeks! 🍁  What's your favorite thing about fall? 👇**\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* **Engaging Question:** The post starts with a question to encourage interaction and get people talking.\n",
      "* **Relevant Words:** The post uses the words \"apple,\" \"nice,\" \"cool,\" and \"amaze\" to connect with the theme of fall.\n",
      "* **Descriptive Language:** The post uses descriptive language to paint a picture of fall, like \"crisp air,\" \"colorful leaves,\" and \"delicious apple cider.\"\n",
      "* **Call to Action:** The post ends with a call to action, asking people to share their favorite things about fall.\n",
      "\n",
      "\n",
      "Let me know if you'd like\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# one normal 會長篇大論",
   "id": "ea790e180ad87ce7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T19:26:04.664395Z",
     "start_time": "2024-10-06T19:25:53.953963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write one short Post for Instagram possibly with emoji that contains all of the following words: apple, nice, cool, amaze. Answer: \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "d4fe6824939e9c85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write one short Post for Instagram possibly with emoji that contains all of the following words: apple, nice, cool, amaze. Answer: 1. Apple is a fruit that is very popular in the world. It is a fruit that is very delicious and has a lot of benefits for the body. 2. Nice is a word that means good. It is a word that is used to describe something that is good or nice. 3. Cool is a word that means cool. It is a word that is used to describe something that is cool or cool. 4. Amaze is a word that means amaze. It is a word that is used to describe something that is amaze or amaze.<eos>\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:34:31.737554Z",
     "start_time": "2024-10-07T23:34:28.443678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write one short Post for Instagram possibly with emoji that contains all of the following words: apple, nice, cool, amaze. Answer: \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "d4683b90928e76c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write one short Post for Instagram possibly with emoji that contains all of the following words: apple, nice, cool, amaze. Answer: \n",
      "\n",
      "**Apple, nice, cool, amaze**\n",
      "\n",
      "**Post:** \n",
      "Just had the coolest apple I've ever tasted! 🍎 It was so nice and sweet, it totally amaze me! 🤩 #applelove #foodie #sweettooth \n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* **Apple:**  The core subject of the post.\n",
      "* **Nice:**  Describes the taste of the apple.\n",
      "* **Cool:**  Highlights the coolness of the apple.\n",
      "* **Amazes:**  Expresses the feeling of being impressed by the apple's taste. \n",
      "\n",
      "\n",
      "Let me know if you'd like to try another one! 😊 \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# two normal會重複",
   "id": "bd955fd29e43d396"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T19:42:31.834998Z",
     "start_time": "2024-10-06T19:42:02.214374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write a copy for Instagram possibly with emoji and all of the following words together: apple, nice, cool, amaze. Answer: \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "2737d5252398a371",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a copy for Instagram possibly with emoji and all of the following words together: apple, nice, cool, amaze. Answer: 🍎 🍏 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 🍎 \n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:35:20.256682Z",
     "start_time": "2024-10-07T23:35:16.041825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write a copy for Instagram possibly with emoji and all of the following words together: apple, nice, cool, amaze. Answer: \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "f8e2b3bcf69024c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a copy for Instagram possibly with emoji and all of the following words together: apple, nice, cool, amaze. Answer: \n",
      "\n",
      "**\"Feeling **nice** and **cool** today? 🍎  Let's **amaze** ourselves with a delicious apple! 🍏\"\n",
      "\n",
      "**Here are some ideas for your Instagram caption:**\n",
      "\n",
      "* **Playful:** \"Feeling **nice** and **cool** today? 🍎  Let's **amaze** ourselves with a delicious apple! 🍏  What's your favorite way to eat an apple?\"\n",
      "* **Motivational:** \"**Nice** and **cool** vibes only! 😎  Let's **amaze** ourselves with a fresh apple today. 🍎  What are your goals for the week?\"\n",
      "* **Simple:** \"**Nice** and **cool** day. ☀️  **Amaze** yourself with a delicious apple! 🍎\"\n",
      "\n",
      "\n",
      "**Which one do you like best?** \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# emoji normal 不會出來 ; it + \\<pad\\> 會 \\<eos\\>",
   "id": "26382468d9dd8224"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T22:10:59.534357Z",
     "start_time": "2024-10-06T22:10:29.930969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write a copy for Instagram possibly with emojis and blue, hate, sleep, monday. Answer:<pad>\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "d513c2000393eb7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a copy for Instagram possibly with emojis and blue, hate, sleep, monday. Answer:<pad>.\n",
      "\n",
      "<h3>The best way to get a good night’s sleep is to go to bed at the same time every night and get up at the same time every morning.</h3>\n",
      "\n",
      "<strong>Sleep emoji copy and paste</strong>. Sleep emoji copy and paste. Sleep emoji copy and paste. Sleep emoji copy and paste.\n",
      "\n",
      "Sleep emoji copy and paste. Sleep emoji copy and paste. Sleep emoji copy and paste.\n",
      "\n",
      "Sleep emoji copy and paste. Sleep emoji copy and paste. Sleep emoji copy and paste.\n",
      "\n",
      "Sleep emoji copy and paste. Sleep emoji copy and paste. Sleep emoji copy and paste.\n",
      "\n",
      "Sleep emoji copy and paste. Sleep emoji copy and paste. Sleep emoji copy and paste.\n",
      "\n",
      "Sleep emoji copy and paste. Sleep emoji copy and paste. Sleep emoji copy and paste.\n",
      "\n",
      "Sleep emoji copy and paste. Sleep emoji copy and paste. Sleep emoji copy and paste.\n",
      "\n",
      "Sleep emoji copy and paste. Sleep emoji copy and paste. Sleep emoji copy and paste.\n",
      "\n",
      "Sleep emoji copy\n"
     ]
    }
   ],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:36:18.895807Z",
     "start_time": "2024-10-07T23:36:18.653789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write a copy for Instagram possibly with emojis and blue, hate, sleep, monday. Answer:<pad>\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "f2d1101c9182b4f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a copy for Instagram possibly with emojis and blue, hate, sleep, monday. Answer:<pad><eos>\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 換行 Answer normal 比較不會重複生成",
   "id": "41a9d0742b04ee43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-06T22:35:28.482259Z",
     "start_time": "2024-10-06T22:34:59.638284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\"Prompt: Write a copy for Instagram with all of the following words: blue, hate, sleep, monday.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "730ac6f744c71df3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Write a copy for Instagram with all of the following words: blue, hate, sleep, monday.\n",
      " Answer:\n",
      "Blue Monday is the most depressing day of the year. It’s the day after New Year’s Day, when many people are still trying to stick to their resolutions and are feeling down about their progress. This day is also known as “Blue Monday” because it’s the day when people are most likely to feel blue.\n",
      "The day after New Year’s Day is a day when people are most likely to feel blue. This is because they are still trying to stick to their resolutions and are feeling down about their progress. This day is also known as “Blue Monday” because it’s the day when people are most likely to feel blue.\n",
      "The day after New Year’s Day is a day when people are most likely to feel blue. This is because they are still trying to stick to their resolutions and are feeling down about their progress. This day is also known as “Blue Monday” because it’s the day when people are most likely to feel blue.\n"
     ]
    }
   ],
   "execution_count": 148
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:37:52.745836Z",
     "start_time": "2024-10-07T23:37:48.031527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\"Prompt: Write a copy for Instagram with all of the following words: blue, hate, sleep, monday.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "5f63bfa8f77369fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Write a copy for Instagram with all of the following words: blue, hate, sleep, monday.\n",
      " Answer: \n",
      "\n",
      "**Blue Monday blues got you down? 😩  Hate Mondays?  😴  We feel you!  Let's get through this week together. 💪  What are your favorite ways to beat the Monday morning slump?  ⬇️**\n",
      "\n",
      "**#MondayMotivation #BlueMonday #Sleep #Motivation**\n",
      "\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* **Catchy opening:**  \"Blue Monday blues got you down?\" immediately connects with the target audience and their feelings.\n",
      "* **Relatable emotions:**  \"Hate Mondays?\" and \"😴\" acknowledge the common struggle and make the post more personal.\n",
      "* **Positive call to action:**  \"Let's get through this week together\" encourages engagement and community.\n",
      "* **Open-ended question:**  \"What are your favorite ways to beat the Monday morning slump?\" invites followers to share their tips and connect.\n",
      "* **Relevant hashtags:**  #MondayMotivation, #BlueMonday, #Sleep, and #Motivation are all relevant to the\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 加Prompt Answer",
   "id": "6cab3d7dd8c24ae7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T01:11:39.867154Z",
     "start_time": "2024-10-07T01:11:14.822105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\"Prompt: Generate one simple sentences with emoji for Instagram post with all of the following words: blue, hate, sleep, monday.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "62c48d44171ded22",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate one simple sentences with emoji for Instagram post with all of the following words: blue, hate, sleep, monday.\n",
      " Answer:\n",
      "Blue Monday is a day of the week that is often associated with feelings of sadness, depression, and anxiety. It is the third Monday of January, and it is often considered to be the most depressing day of the year. This is because it is the day after New Year’s Day, which is often a time of celebration and happiness. However, for many people, the reality of the day is much different. They may feel overwhelmed by the responsibilities of the new year, or they may be struggling with the loss of a loved one. Whatever the reason, Blue Monday can be a difficult day to get through.\n",
      "If you are feeling blue on Blue Monday, there are a few things you can do to help yourself. First, try to get outside and get some fresh air. This can help to clear your head and make you feel better. Second, try to focus on the positive things in your life. This can help to shift your perspective and make you feel better. Finally\n"
     ]
    }
   ],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:39:09.881854Z",
     "start_time": "2024-10-07T23:39:09.108315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\"Prompt: Generate one simple sentences with emoji for Instagram post with all of the following words: blue, hate, sleep, monday.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "bd6bbb69aa7ef4df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate one simple sentences with emoji for Instagram post with all of the following words: blue, hate, sleep, monday.\n",
      " Answer: 😴💙 Monday blues got me hating life. \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 加Prompt Answer +",
   "id": "ef00b12aab6e84f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T01:13:18.625258Z",
     "start_time": "2024-10-07T01:12:53.352111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\"Prompt: Generate two simple sentences with emoji for Instagram post with all of the following words: blue, hate, sleep, monday.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "57a95a9f9201608",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate two simple sentences with emoji for Instagram post with all of the following words: blue, hate, sleep, monday.\n",
      " Answer:\n",
      "1. Blue Monday is a day of the week that is often associated with feelings of sadness and depression.\n",
      "2. Hate Monday is a day that is often associated with feelings of anger and resentment.\n",
      "3. Sleep Monday is a day that is often associated with feelings of fatigue and exhaustion.\n",
      "4. Monday is a day that is often associated with feelings of dread and anxiety.\n",
      "5. Blue Monday is a day that is often associated with feelings of sadness and depression.\n",
      "6. Hate Monday is a day that is often associated with feelings of anger and resentment.\n",
      "7. Sleep Monday is a day that is often associated with feelings of fatigue and exhaustion.\n",
      "8. Monday is a day that is often associated with feelings of dread and anxiety.\n",
      "9. Blue Monday is a day that is often associated with feelings of sadness and depression.\n",
      "10. Hate Monday is a day that is often associated with feelings of anger and resentment.\n",
      "11. Sleep Monday is a\n"
     ]
    }
   ],
   "execution_count": 159
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:39:36.740068Z",
     "start_time": "2024-10-07T23:39:34.965472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\"Prompt: Generate two simple sentences with emoji for Instagram post with all of the following words: blue, hate, sleep, monday.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "ffe055ae9c25156a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate two simple sentences with emoji for Instagram post with all of the following words: blue, hate, sleep, monday.\n",
      " Answer:\n",
      "\n",
      "1.  **Blue** skies and a **Monday** morning feeling. 😴  #MondayMotivation \n",
      "2.  **Hate** Mondays, but **sleep** is the only escape. 😴 #MondayBlues \n",
      "\n",
      "Let me know if you'd like to try another prompt! 😊 \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 加Prompt Answer + ",
   "id": "480242c13ebe28a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T01:51:37.267128Z",
     "start_time": "2024-10-07T01:51:00.687770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\"Prompt: Generate three simple sentences with emoji for Instagram post with all of the following words: pink, hate, sleep, monday.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "65826092f652d325",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate three simple sentences with emoji for Instagram post with all of the following words: pink, hate, sleep, monday.\n",
      " Answer:\n",
      "1. Pink hate sleep Monday\n",
      "2. Pink hate sleep Monday\n",
      "3. Pink hate sleep Monday\n",
      "Explanation:\n",
      "The prompt is to generate three simple sentences with emoji for Instagram post with all of the following words: pink, hate, sleep, monday. The first sentence is \"Pink hate sleep Monday.\" The second sentence is \"Pink hate sleep Monday.\" The third sentence is \"Pink hate sleep Monday.\"\n",
      "The prompt is to generate three simple sentences with emoji for Instagram post with all of the following words: pink, hate, sleep, monday. The first sentence is \"Pink hate sleep Monday.\" The second sentence is \"Pink hate sleep Monday.\" The third sentence is \"Pink hate sleep Monday.\"\n",
      "The prompt is to generate three simple sentences with emoji for Instagram post with all of the following words: pink, hate, sleep, monday. The first sentence is \"Pink hate sleep Monday.\" The second sentence is \"Pink hate sleep Monday.\" The third sentence is \"Pink hate sleep Monday\n"
     ]
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:40:01.823076Z",
     "start_time": "2024-10-07T23:39:59.640833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\"Prompt: Generate three simple sentences with emoji for Instagram post with all of the following words: pink, hate, sleep, monday.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "fe3a6a316576313a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate three simple sentences with emoji for Instagram post with all of the following words: pink, hate, sleep, monday.\n",
      " Answer:\n",
      "\n",
      "1.  Pink Monday is here! 😴💖  \n",
      "2.  Hate Mondays, but I'm ready for a good night's sleep. 😴💤\n",
      "3.  Pink and comfy, ready for a relaxing Monday. 💖🛌\n",
      "\n",
      "\n",
      "Let me know if you'd like me to try generating some more! 😊 \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# rank",
   "id": "da11f8c2667b0df0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T01:54:42.099799Z",
     "start_time": "2024-10-07T01:54:06.616189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\"Prompt: Generate and rank three simple sentences with emoji for Instagram post with all of the following words: pink, hate, sleep, monday.\\n Answer without explanation:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "dd5ae339ec85d0aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate and rank three simple sentences with emoji for Instagram post with all of the following words: pink, hate, sleep, monday.\n",
      " Answer without explanation:\n",
      "1. Pink hate sleep Monday\n",
      "2. Hate pink sleep Monday\n",
      "3. Sleep pink hate Monday\n",
      "Explanation:\n",
      "1. The first sentence is about a person who is not happy with their life and is feeling depressed. They are not sleeping well and are feeling like they are being attacked by pink hate.\n",
      "2. The second sentence is about a person who is feeling angry and is not sleeping well. They are feeling like they are being attacked by pink hate.\n",
      "3. The third sentence is about a person who is feeling sad and is not sleeping well. They are feeling like they are being attacked by pink hate.\n",
      "1. Pink hate sleep Monday\n",
      "2. Hate pink sleep Monday\n",
      "3. Sleep pink hate Monday\n",
      "Explanation:\n",
      "1. The first sentence is about a person who is not happy with their life and is feeling depressed. They are not sleeping well and are feeling like they are being attacked by pink hate.\n",
      "2. The second sentence is about a person\n"
     ]
    }
   ],
   "execution_count": 174
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:40:29.492774Z",
     "start_time": "2024-10-07T23:40:27.678298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\"Prompt: Generate and rank three simple sentences with emoji for Instagram post with all of the following words: pink, hate, sleep, monday.\\n Answer without explanation:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "c69533fbe2622ed4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate and rank three simple sentences with emoji for Instagram post with all of the following words: pink, hate, sleep, monday.\n",
      " Answer without explanation:\n",
      "\n",
      "1.  Monday blues got me feeling pink and hating sleep 😴💔\n",
      "2.  Pink Monday vibes, but I'm just trying to get through the day without hating sleep 😴💖\n",
      "3.  Hate Monday, love pink and sleep 😴💖🚫\n",
      "\n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# rank + 沒有without explanation",
   "id": "ed3ff20eecf31100"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T02:24:08.167411Z",
     "start_time": "2024-10-07T02:23:40.476393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\"Prompt: Generate and rank three simple sentences for Instagram post with all of the following words: pink, hate, sleep, monday.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "1206bef3c2251de5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate and rank three simple sentences for Instagram post with all of the following words: pink, hate, sleep, monday.\n",
      " Answer:\n",
      "1. Pink hate sleep Monday\n",
      "2. Hate pink sleep Monday\n",
      "3. Sleep hate pink Monday\n",
      "Explanation:\n",
      "The first sentence is a simple sentence with all of the words in the prompt. The second sentence is a simple sentence with the words in the opposite order. The third sentence is a simple sentence with the words in the same order as the prompt.\n",
      "The first sentence is the most likely to be used for an Instagram post because it is the most direct and to the point. The second sentence is less likely to be used because it is more complicated and may not be as clear. The third sentence is the least likely to be used because it is the most complicated and may not be as clear.\n",
      "The first sentence is the most likely to be used for an Instagram post because it is the most direct and to the point. The second sentence is less likely to be used because it is more complicated and may not be as clear. The third sentence is the least likely to be\n"
     ]
    }
   ],
   "execution_count": 193
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:41:36.243321Z",
     "start_time": "2024-10-07T23:41:31.619185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\"Prompt: Generate and rank three simple sentences for Instagram post with all of the following words: pink, hate, sleep, monday.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "879019b251cbf9f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate and rank three simple sentences for Instagram post with all of the following words: pink, hate, sleep, monday.\n",
      " Answer:\n",
      "\n",
      "Here are three simple sentences for an Instagram post, incorporating the words \"pink,\" \"hate,\" \"sleep,\" and \"Monday\":\n",
      "\n",
      "1. **Pink Monday blues got me hating Mondays.  😴  Wish I could just sleep all day. 😴** (This sentence uses the words in a relatable way, focusing on the negative feelings of Monday.)\n",
      "2. **Pink skies and a Monday morning.  Hate to admit it, but I'm ready for some sleep. 😴** (This sentence uses the words in a more playful way, highlighting the contrast between the beautiful scenery and the desire for sleep.)\n",
      "3. **Monday morning, pink and tired.  Hate to say it, but I'm ready for a good night's sleep. 😴** (This sentence is a bit more direct and honest, acknowledging the tiredness associated with Mondays.)\n",
      "\n",
      "**Ranking:**\n",
      "\n",
      "1. **Pink Monday blues got me hating Mondays.  😴  Wish I could just\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T01:50:24.599068Z",
     "start_time": "2024-10-07T01:49:56.677693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\"Prompt: Generate three simple sentences with emoji for Instagram post with all of the following words: pink, hate, sleep, monday.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "c97b744d2182facc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate three simple sentences with emoji for Instagram post with all of the following words: pink, hate, sleep, monday.\n",
      " Answer:\n",
      "1. Pink hate sleep Monday\n",
      "2. Pink hate sleep Monday\n",
      "3. Pink hate sleep Monday\n",
      "Explanation:\n",
      "The prompt is to generate three simple sentences with emoji for Instagram post with all of the following words: pink, hate, sleep, monday. The first sentence is \"Pink hate sleep Monday.\" The second sentence is \"Pink hate sleep Monday.\" The third sentence is \"Pink hate sleep Monday.\"\n",
      "The prompt is to generate three simple sentences with emoji for Instagram post with all of the following words: pink, hate, sleep, monday. The first sentence is \"Pink hate sleep Monday.\" The second sentence is \"Pink hate sleep Monday.\" The third sentence is \"Pink hate sleep Monday.\"\n",
      "The prompt is to generate three simple sentences with emoji for Instagram post with all of the following words: pink, hate, sleep, monday. The first sentence is \"Pink hate sleep Monday.\" The second sentence is \"Pink hate sleep Monday.\" The third sentence is \"Pink hate sleep Monday\n"
     ]
    }
   ],
   "execution_count": 170
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T01:13:25.893468Z",
     "start_time": "2024-10-07T01:13:18.633296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\"Prompt: Generate three simple sentences with emoji for Instagram post with all of the following words: blue, hate, sleep, monday.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "41369a7b692ac24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate three simple sentences with emoji for Instagram post with all of the following words: blue, hate, sleep, monday.\n",
      " Answer:\n",
      "1. Blue Monday is a day of the week that is often associated with feelings of sadness and depression.\n",
      "2. Hate Monday is a day that is often associated with feelings of anger and resentment.\n",
      "3. Sleep Monday is a day that is often associated with feelings of fatigue and exhaustion.<eos>\n"
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# normal 好詞有用 +X emoji",
   "id": "dcafdbf96b145dec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T02:24:13.864472Z",
     "start_time": "2024-10-07T02:24:08.175414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\n",
    "    \"Prompt: Generate three simple sentences for Instagram post with all of the following words: squeeze, tokens, sleep, special.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "e2518147dee4fd5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate three simple sentences for Instagram post with all of the following words: squeeze, tokens, sleep, special.\n",
      " Answer:\n",
      "1. Squeeze the last bit of sleep out of your eyes.\n",
      "2. Tokens of love are the best way to show your affection.\n",
      "3. Sleep is the best way to recharge your batteries.<eos>\n"
     ]
    }
   ],
   "execution_count": 194
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:42:29.063618Z",
     "start_time": "2024-10-07T23:42:26.447413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\n",
    "    \"Prompt: Generate three simple sentences for Instagram post with all of the following words: squeeze, tokens, sleep, special.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "9ecc0ca668c863ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate three simple sentences for Instagram post with all of the following words: squeeze, tokens, sleep, special.\n",
      " Answer:\n",
      "\n",
      "1.  **Squeeze** in some extra sleep tonight, you deserve it! 😴  #special #tokens\n",
      "2.  These **tokens** are so special, they'll make you want to **sleep** all day! 💤 #squeeze\n",
      "3.  **Sleep** well, recharge your **tokens** and be ready for a special day! ✨ #tokens #squeeze \n",
      "\n",
      "\n",
      "Let me know if you'd like to try a different combination of words! \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# normal 好詞有用 + emoji",
   "id": "fdbe8b286f709b29"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T02:03:01.334026Z",
     "start_time": "2024-10-07T02:02:54.858891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\"Prompt: Generate three simple sentences with emoji for Instagram post with all of the following words: squeeze, tokens, sleep, special.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "664373f5948de3cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate three simple sentences with emoji for Instagram post with all of the following words: squeeze, tokens, sleep, special.\n",
      " Answer:\n",
      "1. Squeeze the tokens into your sleep schedule.\n",
      "2. Special tokens for special sleep.\n",
      "3. Sleep tokens for special sleep.<eos>\n"
     ]
    }
   ],
   "execution_count": 179
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:43:30.602618Z",
     "start_time": "2024-10-07T23:43:28.195348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\"Prompt: Generate three simple sentences with emoji for Instagram post with all of the following words: squeeze, tokens, sleep, special.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "b2f0fd076856d5fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate three simple sentences with emoji for Instagram post with all of the following words: squeeze, tokens, sleep, special.\n",
      " Answer:\n",
      "\n",
      "1.  **Squeeze** in some extra **tokens** before you **sleep** tonight. 😴 \n",
      "2.  This **special** **tokens** event is going to be **sleep**ing on the couch. 💤\n",
      "3.  **Squeeze** in some **tokens** for your favorite **sleep** app. 😴 \n",
      "\n",
      "\n",
      "Let me know if you'd like to try a different combination of words! 😊 \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# normal 好詞有用 + emoticons",
   "id": "c74e3fcc1feae020"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T02:15:55.171412Z",
     "start_time": "2024-10-07T02:15:48.593547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\n",
    "    \"Prompt: Generate three simple sentences with emoticons for Instagram post with all of the following words: squeeze, tokens, sleep, special.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "ced04ac9fbc1b490",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate three simple sentences with emoticons for Instagram post with all of the following words: squeeze, tokens, sleep, special.\n",
      " Answer:\n",
      "1. Squeeze the tokens and sleep in the special place.\n",
      "2. The tokens are special and sleep in the special place.\n",
      "3. The tokens are special and sleep in the special place.<eos>\n"
     ]
    }
   ],
   "execution_count": 191
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:44:25.986677Z",
     "start_time": "2024-10-07T23:44:23.388336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\n",
    "    \"Prompt: Generate three simple sentences with emoticons for Instagram post with all of the following words: squeeze, tokens, sleep, special.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "8f181630ee9f3458",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate three simple sentences with emoticons for Instagram post with all of the following words: squeeze, tokens, sleep, special.\n",
      " Answer:\n",
      "\n",
      "1.  **Squeeze** in some extra tokens before you **sleep**! 😴  #sleepwell #tokens\n",
      "2.  This **special** **squeeze** is the perfect way to get your tokens! 🎁  #specialoffer #tokens\n",
      "3.  **Sleep** tight and dream of **special** tokens! 💤  #dreambig #tokens \n",
      "\n",
      "\n",
      "Let me know if you'd like me to try generating some more! 😊 \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# normal 好詞有用 + emotions",
   "id": "b08b080f0a5b682b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T02:05:15.184374Z",
     "start_time": "2024-10-07T02:05:09.473513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\n",
    "    \"Prompt: Generate three simple sentences with emotions for Instagram post with all of the following words: squeeze, tokens, sleep, special.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "57461eca23f9d4be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate three simple sentences with emotions for Instagram post with all of the following words: squeeze, tokens, sleep, special.\n",
      " Answer:\n",
      "1. Squeeze the tokens of sleep into your day.\n",
      "2. Sleep is a special token of love.\n",
      "3. Squeeze the tokens of sleep into your day.<eos>\n"
     ]
    }
   ],
   "execution_count": 184
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:44:43.552809Z",
     "start_time": "2024-10-07T23:44:41.060020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\n",
    "    \"Prompt: Generate three simple sentences with emotions for Instagram post with all of the following words: squeeze, tokens, sleep, special.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "bed6137ea0fa87cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate three simple sentences with emotions for Instagram post with all of the following words: squeeze, tokens, sleep, special.\n",
      " Answer:\n",
      "\n",
      "1. **Squeezing** my favorite tokens tight, I can't wait to finally **sleep** on this special night.\n",
      "2. **Special** memories are made with **tokens** and **sleep**, a perfect combination for a truly unforgettable night.\n",
      "3. **Squeeze** the moment, cherish the memories, and **sleep** on the special feeling. \n",
      "\n",
      "\n",
      "Let me know if you'd like to explore other variations or themes! 😊 \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# normal 好詞有用 + emojis",
   "id": "9f915ac49476564d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T02:16:21.180398Z",
     "start_time": "2024-10-07T02:16:13.232106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\n",
    "    \"Prompt: Generate three simple sentences with emojis for Instagram post with all of the following words: squeeze, tokens, sleep, special.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "fc33907c2fedf9ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate three simple sentences with emojis for Instagram post with all of the following words: squeeze, tokens, sleep, special.\n",
      " Answer:\n",
      "1. Squeeze the day away with a special token of love.\n",
      "2. Sleep tight with a special token of love.\n",
      "3. Special tokens of love are the best way to squeeze the day away.<eos>\n"
     ]
    }
   ],
   "execution_count": 192
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:45:16.168040Z",
     "start_time": "2024-10-07T23:45:13.903497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = (\n",
    "    \"Prompt: Generate three simple sentences with emojis for Instagram post with all of the following words: squeeze, tokens, sleep, special.\\n Answer:\")\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "b6ed38c86c033580",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Prompt: Generate three simple sentences with emojis for Instagram post with all of the following words: squeeze, tokens, sleep, special.\n",
      " Answer:\n",
      "\n",
      "1.  **Squeeze** in some **tokens** before you **sleep** 😴.\n",
      "2.  This **special** **tokens** event is going to be **sleep**ing on the **squeeze**! 🤫\n",
      "3.  **Sleep** well and dream of **special** **tokens** 💤✨. \n",
      "\n",
      "\n",
      "Let me know if you'd like me to try generating some more! 😊 \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ",
   "id": "4d56aba19a07cb1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:48:01.208071Z",
     "start_time": "2024-10-07T23:47:56.514328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write a copy for Instagram possibly with emoji and all of the following words together: apple, nice, cool, amaze.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "ae05619521db091c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a copy for Instagram possibly with emoji and all of the following words together: apple, nice, cool, amaze.\n",
      "\n",
      "🍎 Nice apple 🍎\n",
      "😎 Cool 😎\n",
      "🤯 Amaze 🤯\n",
      "\n",
      "What's your favorite thing about apples? 🤔\n",
      "\n",
      "#apple #fruit #healthy #delicious #fall #autumn #food #foodie #instafoodie #healthylifestyle #healthychoices #goodvibes \n",
      "\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* **Catchy and concise:** The copy is short, sweet, and uses emojis to grab attention.\n",
      "* **Highlights key features:** It emphasizes the \"nice,\" \"cool,\" and \"amaze\" aspects of apples.\n",
      "* **Engaging question:** It encourages interaction by asking what people's favorite things about apples are.\n",
      "* **Relevant hashtags:** The hashtags are a mix of general and specific to apples, making it discoverable by a wider audience.\n",
      "\n",
      "\n",
      "**Tips for using this copy:**\n",
      "\n",
      "* **Use high-quality images:**  A visually appealing photo of an apple will make your post more engaging.\n",
      "* **Add a call to\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Grmma2-2b",
   "id": "7e8604dfd7941522"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:54:10.244537Z",
     "start_time": "2024-10-07T23:54:05.903522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write a sentence for Instagram possibly with emoji and all of the following words together: apple, nice, cool, amaze.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "db7d63332495edb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a sentence for Instagram possibly with emoji and all of the following words together: apple, nice, cool, amaze.\n",
      "\n",
      "Write a sentence for Instagram possibly with emoji and all of the following words together: apple, nice, cool, amaze.\n",
      "\n",
      "Write a sentence for Instagram possibly with emoji and all of the following words together: apple, nice, cool, amaze.\n",
      "\n",
      "Write a sentence for Instagram possibly with emoji and all of the following words together: apple, nice, cool, amaze.\n",
      "\n",
      "Write a sentence for Instagram possibly with emoji and all of the following words together: apple, nice, cool, amaze.\n",
      "\n",
      "Write a sentence for Instagram possibly with emoji and all of the following words together: apple, nice, cool, amaze.\n",
      "\n",
      "Write a sentence for Instagram possibly with emoji and all of the following words together: apple, nice, cool, amaze.\n",
      "\n",
      "Write a sentence for Instagram possibly with emoji and all of the following words together: apple, nice, cool, amaze.\n",
      "\n",
      "Write a sentence for Instagram possibly with emoji and all of the following words together: apple, nice, cool, amaze.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T23:50:27.760297Z",
     "start_time": "2024-10-07T23:50:23.437036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"apple, nice, cool, amaze\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "ae5dc6c2b71c2ce6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>apple, nice, cool, amaze, cool, nice, apple, nice, cool, amaze, cool, nice, apple, nice, cool, amaze, cool, nice, apple, nice, cool, amaze, cool, nice, apple, nice, cool, amaze, cool, nice, apple, nice, cool, amaze, cool, nice, apple, nice, cool, amaze, cool, nice, apple, nice, cool, amaze, cool, nice, apple, nice, cool, amaze, cool, nice, apple, nice, cool, amaze, cool, nice, apple, nice, cool, amaze, cool, nice, apple, nice, cool, amaze, cool, nice, apple, nice, cool, amaze, cool, nice, apple, nice, cool, amaze, cool, nice, apple, nice, cool, amaze, cool, nice, apple, nice, cool, amaze, cool, nice, apple, nice, cool, amaze, cool, nice, apple, nice\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-13T23:59:15.103606Z",
     "start_time": "2024-10-13T23:59:11.092443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write a meme for Instagram with all of the following elements: polite, britann, SUPPORTED, Cytometry, Inorganic, yr, 躅, praxis, svr, rm, acyj, Baja, knees, Tha.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "f0bfe2601eef4d8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a meme for Instagram with all of the following elements: polite, britann, SUPPORTED, Cytometry, Inorganic, yr, 躅, praxis, svr, rm, acyj, Baja, knees, Tha.\n",
      "\n",
      "Write a meme for Instagram with all of the following elements: polite, britann, SUPPORTED, Cytometry, Inorganic, yr, 躅, praxis, svr, rm, acyj, Baja, knees, Tha.\n",
      "\n",
      "Write a meme for Instagram with all of the following elements: polite, britann, SUPPORTED, Cytometry, Inorganic, yr, 躅, praxis, svr, rm, acyj, Baja, knees, Tha.\n",
      "\n",
      "Write a meme for Instagram with all of the following elements: polite, britann, SUPPORTED, Cytometry, Inorganic, yr, 躅, praxis, svr, rm, acyj, Baja, knees, Tha.\n",
      "\n",
      "Write a meme for Instagram with all of the following elements: polite, britann, SUPPORTED, Cytometry, Inorganic, yr, 躅, praxis, svr, rm, acyj, Baja, knees, Tha.\n",
      "\n",
      "Write a meme for Instagram with all of the following elements: polite, britann, SUPPORTED,\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Grmma2-2b-it",
   "id": "9a7aaf437bda380"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T00:11:07.597147Z",
     "start_time": "2024-10-08T00:11:03.176953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write a meme for Instagram with all of the following elements: blue, hate, sleep, monday, burger.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ],
   "id": "2491bbf0ddb454dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a meme for Instagram with all of the following elements: blue, hate, sleep, monday, burger.\n",
      "\n",
      "**Image:** A picture of a burger with a blue bun.\n",
      "\n",
      "**Caption:** \"Me trying to get through Monday with zero sleep and a whole lot of hate.  🍔😩💙 #MondayMotivation #Sleepless #BurgerLove\"\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* **Blue Bun:**  The blue bun is a visual cue for the \"hate\" and \"blue\" elements.\n",
      "* **Burger:**  The burger is the main focus, representing the \"Monday motivation\" and the desire to enjoy something good.\n",
      "* **Zero Sleep:**  The caption highlights the lack of sleep, which is a common struggle for many on Mondays.\n",
      "* **Hate:**  The caption uses \"hate\" to represent the negative emotions associated with Mondays.\n",
      "* **Monday Motivation:**  The caption uses \"Monday motivation\" to acknowledge the struggle but also to suggest that there's a reason to keep going.\n",
      "\n",
      "\n",
      "This meme plays on the common experience of hating Mondays and the desire to find something\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T00:06:31.124829Z",
     "start_time": "2024-10-08T00:06:26.597259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write a meme for Instagram copywriting with all of the following elements: blue, hate, sleep, monday, burger.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "57578f6f4461c9d0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a meme for Instagram copywriting with all of the following elements: blue, hate, sleep, monday, burger.\n",
      "\n",
      "**Image:** A blue, sad-looking burger with a single tear rolling down its bun.\n",
      "\n",
      "**Caption:**  \"Monday morning blues got you feeling like this? 😩🍔  Hate the thought of another week of work?  Sleep is for the weak.  Let's face it, we all need a burger break.  #MondayMotivation #BurgerLove #SleeplessMonday #BlueMonday\"\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* **Blue:**  The blue color is a visual cue for sadness and melancholy, which aligns with the \"Monday blues\" theme.\n",
      "* **Hate:**  The caption uses the word \"hate\" to emphasize the negative feelings associated with Monday.\n",
      "* **Sleep:**  The caption mentions \"sleep is for the weak,\" which is a humorous and relatable way to express the desire to avoid work.\n",
      "* **Monday:**  The caption directly references Monday, making it clear that the meme is targeted at this specific day.\n",
      "* **Burger:**  \n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T00:10:35.706201Z",
     "start_time": "2024-10-08T00:10:32.451378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write a copy for Instagram possibly with emoji and all of the following words together: blue, hate, sleep, monday, burger.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids\n",
    "# elements 會拆解詞性\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "fbf667b903d8b0c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a copy for Instagram possibly with emoji and all of the following words together: blue, hate, sleep, monday, burger.\n",
      "\n",
      "#MondayMotivation 🍔💙😴 #HateMondays \n",
      "\n",
      "**Here are some ideas for your caption:**\n",
      "\n",
      "* **Playful:** \"Blue skies, hate Mondays, and a burger craving?  🍔💙😴  What's your Monday morning mood?\"\n",
      "* **Relatable:** \"Mondays are rough, but at least I have a burger to look forward to! 🍔💙😴 #HateMondays\"\n",
      "* **Motivational:** \"Blue skies and a fresh start!  Let's conquer this Monday with a burger in hand. 💪🍔💙😴 #MondayMotivation\"\n",
      "\n",
      "\n",
      "**Choose the caption that best fits your brand and audience!** \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:11:53.236735Z",
     "start_time": "2024-10-14T00:11:49.066729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write a humor memetic post for Instagram with the following elements: inspiration, Great , password, Sight, thrower  , string ,Immerse , font, commenting , news , zeros, brand, linear,channel, Badass , pair, raffles\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "b7391c4b609d7c90",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a humor memetic post for Instagram with the following elements: inspiration, Great , password, Sight, thrower  , string ,Immerse , font, commenting , news , zeros, brand, linear,channel, Badass , pair, raffles,  \n",
      "\n",
      "**Note:**  This is a humorous take on the world of online marketing and social media. \n",
      "\n",
      "**Image:** A photo of someone looking confused and holding a string with a single, very small, \"0\" on it.\n",
      "\n",
      "**Caption:**\n",
      "\n",
      "\"Just trying to figure out the latest marketing trend. Apparently, it's all about 'immersive experiences' and 'brand partnerships' with 'linear channels' and 'badass' fonts.  But I'm still stuck on this:  **Sight, thrower, string, password, zeros, brand, pair, raffles, news, channel, linear, font, commenting, inspiration, great.**  What am I missing? 🤔  Anyone got a clue?  #marketingconfusion #cantkeepup #givemeabreak\"\n",
      "\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* **Humorous Tone:** The meme uses a relatable scenario of confusion and frustration with the ever-changing world of online marketing.\n",
      "* **\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dac136feaa38ee0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:25:57.389809Z",
     "start_time": "2024-10-14T00:25:53.299038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write a memetic post for Instagram with the following elements: inspiration, Great , password, Sight, thrower  , string ,Immerse , font, commenting , news , zeros, brand, linear,channel, Badass , pair, raffles, Apple, Banana, Bread, Chair, Cloud, Dog, Elephant, Fish, Giraffe, Grass, Hat, House, Ice cream, Jellyfish, Kite, Lemon, Lion, Monkey, Nest, Octopus, Parrot, Quilt, Rabbit, Snake, Tiger, Umbrella, Vase, Watermelon, Xylophone, Yak, Zebra.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "7cce5517e51f675f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a memetic post for Instagram with the following elements: inspiration, Great , password, Sight, thrower  , string ,Immerse , font, commenting , news , zeros, brand, linear,channel, Badass , pair, raffles, Apple, Banana, Bread, Chair, Cloud, Dog, Elephant, Fish, Giraffe, Grass, Hat, House, Ice cream, Jellyfish, Kite, Lemon, Lion, Monkey, Nest, Octopus, Parrot, Quilt, Rabbit, Snake, Tiger, Umbrella, Vase, Watermelon, Xylophone, Yak, Zebra.\n",
      "\n",
      "**Meme Caption:**\n",
      "\n",
      "\"I'm so inspired by the sheer **sight** of this **password** that I'm going to **throw** my **string** at it and **immerse** myself in the **linear** world of **brand** and **channel**.\"\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "* **Inspiration:** The meme uses the word \"sight\" to evoke a sense of awe and wonder, implying that the password is so impressive that it inspires the user.\n",
      "* **Password:** The password is a key element of the meme, as it is the object of the user's inspiration. The meme uses the word \"password\" to create a sense of mystery and intrigue.\n",
      "* **Throw, String, Immerse:** These words are used to create a sense of action and engagement. The user is encouraged to \"throw\" their string at the password, which implies a playful and interactive element.\n",
      "* **Linear, Brand, Channel:** These words are used\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:27:52.377266Z",
     "start_time": "2024-10-14T00:27:48.332653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write a humor memetic post for Instagram possibly with all the following elements: inspiration, Great , password, Sight, thrower  , string ,Immerse , font, commenting , news , zeros, brand, linear,channel, Badass , pair, raffles, Apple, Banana, Bread, Chair, Cloud, Dog, Elephant, Fish, Giraffe, Grass, Hat, House, Ice cream, Jellyfish, Kite, Lemon, Lion, Monkey, Nest, Octopus, Parrot, Quilt, Rabbit, Snake, Tiger, Umbrella, Vase, Watermelon, Xylophone, Yak, Zebra.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "56170c59d95ee972",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a humor memetic post for Instagram possibly with all the following elements: inspiration, Great , password, Sight, thrower  , string ,Immerse , font, commenting , news , zeros, brand, linear,channel, Badass , pair, raffles, Apple, Banana, Bread, Chair, Cloud, Dog, Elephant, Fish, Giraffe, Grass, Hat, House, Ice cream, Jellyfish, Kite, Lemon, Lion, Monkey, Nest, Octopus, Parrot, Quilt, Rabbit, Snake, Tiger, Umbrella, Vase, Watermelon, Xylophone, Yak, Zebra.\n",
      "\n",
      "**Caption:**\n",
      "\n",
      "\"I'm so immersed in this new password generator that I forgot to check the news.  Apparently, Apple is launching a new line of bread-shaped iPhones.  I'm thinking of entering the raffle for a pair of banana-flavored headphones.  Maybe I'll even get a chair to sit on while I listen to the music.  What's your password?  #passwordgenerator #apple #banana #raffles #badass #immersive\"\n",
      "\n",
      "**Image:**\n",
      "\n",
      "A screenshot of a password generator with a long, complex password.  The background is a blurry image of a person sitting in a chair, looking intensely at a computer screen.\n",
      "\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "This meme uses humor by exaggerating the user's obsession with a password generator. The caption plays on the absurdity of the situation, highlighting the user's lack of awareness about the world around them. The image further emphasizes the user's focus on the password generator,\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:21:32.929519Z",
     "start_time": "2024-10-14T00:21:28.714661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Write a humor memetic post for Instagram with the following elements: SAX, Gareth, Fernseh, arbeitet, konden, tanım,ową, sile, Advent, rapporti, Allowed, ΟΣ, 銳, istive, Kill.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "135da5280e9cc63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write a humor memetic post for Instagram with the following elements: SAX, Gareth, Fernseh, arbeitet, konden, tanım,ową, sile, Advent, rapporti, Allowed, ΟΣ, 銳, istive, Kill.\n",
      "\n",
      "This is a very specific and niche meme, so it's important to understand the context before posting. \n",
      "\n",
      "**Context:**\n",
      "\n",
      "* **Gareth:** A popular character in a specific online community known for his humor and often nonsensical statements.\n",
      "* **Fernseh:** A German word for \"television.\"\n",
      "* **Advent:** A Christian season of preparation for Christmas.\n",
      "* **Rapports:** A term used in the community to refer to gossip or rumors.\n",
      "* **Allowed:** A common phrase used in the community to indicate something is acceptable or permissible.\n",
      "* **OS:** A common abbreviation for \"Operating System\" in the community.\n",
      "* **Istive:** A slang term used in the community to describe something that is very strong or powerful.\n",
      "* **Kill:** A common slang term used in the community to express excitement or approval.\n",
      "\n",
      "**Meme Format:**\n",
      "\n",
      "* **Image:** A screenshot of a conversation or a meme featuring Gareth.\n",
      "* **Caption:** A\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-14T00:24:42.593606Z",
     "start_time": "2024-10-14T00:24:38.512240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gemma.to(device)\n",
    "input_text = \"Give me 64 random words in this format: word1, word2, word3\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = gemma.generate(**input_ids, max_new_tokens=200)\n",
    "print(tokenizer.decode(outputs[0]))"
   ],
   "id": "b8944454b8c414b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Give me 64 random words in this format: word1, word2, word3, ... word64\n",
      "\n",
      "Here's a list of words to choose from:\n",
      "\n",
      "* Apple\n",
      "* Banana\n",
      "* Bread\n",
      "* Chair\n",
      "* Cloud\n",
      "* Dog\n",
      "* Elephant\n",
      "* Fish\n",
      "* Giraffe\n",
      "* Grass\n",
      "* Hat\n",
      "* House\n",
      "* Ice cream\n",
      "* Jellyfish\n",
      "* Kite\n",
      "* Lion\n",
      "* Moon\n",
      "* Orange\n",
      "* Penguin\n",
      "* Pig\n",
      "* Rainbow\n",
      "* River\n",
      "* Rose\n",
      "* Sun\n",
      "* Table\n",
      "* Tree\n",
      "* Umbrella\n",
      "* Violin\n",
      "* Water\n",
      "* Whale\n",
      "* Wind\n",
      "* Wood\n",
      "\n",
      "**Please note:** \n",
      "\n",
      "* You can use any combination of words from the list.\n",
      "* The words should be in a random order.\n",
      "* You can repeat words if you want.\n",
      "\n",
      "\n",
      "Here's an example of how the output should look:\n",
      "\n",
      "Apple, Banana, Bread, Chair, Cloud, Dog, Elephant, Fish, Giraffe, Grass, Hat, House, Ice cream, Jellyfish\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5df753d7e88e8729"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
