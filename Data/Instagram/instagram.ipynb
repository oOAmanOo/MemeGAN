{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T11:39:12.756623Z",
     "start_time": "2024-09-08T11:39:12.262964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import time\n",
    "import lxml\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import requests\n",
    "import os"
   ],
   "id": "a08ed5fb6bbbabd7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Login + Page",
   "id": "7ee6472647de7328"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-07T22:51:45.116867Z",
     "start_time": "2024-09-07T22:09:40.503609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 抓取貼文內容--------------------\n",
    "browser = webdriver.Chrome()\n",
    "# 想要抓取資料的頁面網址\n",
    "url = 'https://www.instagram.com/' \n",
    "\n",
    "# 前往頁面\n",
    "browser.get(url)\n",
    "# ------ 填入帳號與密碼 ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.NAME, 'username')))\n",
    "\n",
    "# ------ 網頁元素定位 ------\n",
    "username_input = browser.find_element(By.NAME, \"username\")\n",
    "password_input = browser.find_element(By.NAME, \"password\")\n",
    "print(\"inputing username and password...\")\n",
    "\n",
    "# ------ 輸入帳號密碼 ------\n",
    "username_input.send_keys(\"14han_15felix_\")\n",
    "password_input.send_keys(\"Yaya0329@Instagram\")\n",
    "\n",
    "# ------ 登入 ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.XPATH,\n",
    "'//*[@id=\"loginForm\"]/div/div[3]/button/div')))\n",
    "# ------ 網頁元素定位 ------\n",
    "login_click = browser.find_element(By.XPATH, '//*[@id=\"loginForm\"]/div/div[3]/button/div')\n",
    "# ------ 點擊登入鍵 ------\n",
    "login_click.click()\n",
    "time.sleep(20)\n",
    "print(\"login successfully\")\n",
    "\n",
    "# 想要抓取資料的頁面網址\n",
    "url = 'https://www.instagram.com' \n",
    "subUrlList = ['wendys', 'sonicdrivein', 'mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada']\n",
    "\n",
    "for subUrl in subUrlList:\n",
    "    # 前往頁面\n",
    "    browser.get(url + '/' + subUrl)\n",
    "    time.sleep(2)\n",
    "    # 往下滑並取得新的貼文連結\n",
    "    post_all = []\n",
    "    postList = []\n",
    "\n",
    "    # Get scroll height.\n",
    "    last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    with tqdm(total=100) as pbar:\n",
    "        while True:\n",
    "            # Scroll down to the bottom.\n",
    "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # Wait to load the page.\n",
    "            time.sleep(5)\n",
    "    \n",
    "            soup = Soup(browser.page_source, 'lxml')\n",
    "            # 尋找所有的貼文連結\n",
    "        \n",
    "            for rowPost in soup.select('div._ac7v'):\n",
    "                for elem in rowPost.select('div a'):\n",
    "                    if elem['href'] in postList:\n",
    "                        continue\n",
    "                    postList.append(elem['href'])\n",
    "                    img_elem = elem.select('div div img')\n",
    "                    temp = []\n",
    "                    temp.append(elem['href'])\n",
    "                    try: \n",
    "                        temp.append(img_elem[0]['alt'])\n",
    "                    except:\n",
    "                        temp.append(\"\")\n",
    "                    temp.append(img_elem[0]['src'])\n",
    "                    post_all.append(temp)\n",
    "                    \n",
    "            # Calculate new scroll height and compare with last scroll height.\n",
    "            new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "            pbar.update(1)\n",
    "        \n",
    "    print(subUrl+\" 總共 \" + str(len(post_all)) + \" 篇貼文\")\n",
    "    \n",
    "    # 存成csv\n",
    "    postDataframe= pd.DataFrame(post_all)\n",
    "    postDataframe.rename(columns = {0:'url', 1:'imgAlt', 2:'imgSrc'}, inplace = True)\n",
    "    postDataframe.to_csv('Home_'+subUrl+'.csv', index = False)"
   ],
   "id": "361ec343d6ee3bd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputing username and password...\n",
      "login successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [02:37<06:07,  5.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wendys 總共 371 篇貼文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "186it [15:51,  5.11s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sonicdrivein 總共 2240 篇貼文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [02:12<06:36,  5.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcdonalds 總共 307 篇貼文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "173it [14:45,  5.12s/it]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcdonalds_switzerland 總共 2087 篇貼文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [06:01<02:34,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcdonaldscanada 總共 850 篇貼文\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T12:59:43.930196Z",
     "start_time": "2024-09-08T11:39:19.342961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 抓取貼文內容--------------------\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--window-size=334,778\")\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "# 想要抓取資料的頁面網址\n",
    "url = 'https://www.instagram.com/' \n",
    "\n",
    "# 前往頁面\n",
    "browser.get(url)\n",
    "# ------ 填入帳號與密碼 ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.NAME, 'username')))\n",
    "\n",
    "# ------ 網頁元素定位 ------\n",
    "username_input = browser.find_element(By.NAME, \"username\")\n",
    "password_input = browser.find_element(By.NAME, \"password\")\n",
    "print(\"inputing username and password...\")\n",
    "\n",
    "# ------ 輸入帳號密碼 ------\n",
    "username_input.send_keys(\"_blood_mango_\")\n",
    "password_input.send_keys(\"wangzeng1\")\n",
    "\n",
    "# ------ 登入 ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.XPATH,\n",
    "'//*[@id=\"loginForm\"]/div/div[3]/button/div')))\n",
    "# ------ 網頁元素定位 ------\n",
    "login_click = browser.find_element(By.XPATH, '//*[@id=\"loginForm\"]/div/div[3]/button/div')\n",
    "# ------ 點擊登入鍵 ------\n",
    "login_click.click()\n",
    "time.sleep(15)\n",
    "print(\"login successfully\")\n",
    "\n",
    "# 想要抓取資料的頁面網址\n",
    "url = 'https://www.instagram.com' \n",
    "subUrlList = ['sonicdrivein']\n",
    "# 'wendys', 'mcdonalds', 'mcdonalds_switzerland', 'mcdonaldscanada', 'mcdonalds_switzerland'\n",
    "# ,'sonicdrivein'\n",
    "\n",
    "for subUrl in subUrlList:\n",
    "    print(\"parsing \"+subUrl+\".csv\")\n",
    "    # insHome = pd.read_csv('Home_'+subUrl+'.csv')\n",
    "    insHome = pd.read_csv('Done_'+subUrl+'.csv')\n",
    "    print(\"parsing \"+subUrl+\".csv done\")\n",
    "    with tqdm(total=insHome.shape[0]) as pbar:\n",
    "        counter = 0\n",
    "        for index, row in insHome.iterrows(): #< 922\n",
    "            if '/p/' in row['url'] and counter > 406 and counter < 923:\n",
    "                browser.get(url + row['url'])\n",
    "                time.sleep(10)\n",
    "                html = browser.page_source\n",
    "                soup = Soup(html, 'lxml')\n",
    "                img_elem = soup.select('img.x5yr21d')\n",
    "                text_elem = soup.select('h1._ap3a')\n",
    "\n",
    "                insHome.at[index, 'imgSrc'] = img_elem[1]['src']\n",
    "                try:\n",
    "                    insHome.at[index, 'imgAlt'] = text_elem[0].text\n",
    "                except:\n",
    "                    insHome.at[index, 'imgAlt'] = \"\"\n",
    "                insHome.to_csv('Done_'+subUrl+'.csv', index = False)\n",
    "            pbar.update(1)\n",
    "            counter = counter+1\n",
    "            \n",
    "    insHome['imgName'] = subUrl+'_'+insHome.index.astype(str)\n",
    "    insHome.to_csv('Done_'+subUrl+'.csv', index = False)"
   ],
   "id": "1d5ed7c96325588",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputing username and password...\n",
      "login successfully\n",
      "parsing mcdonalds_switzerland.csv\n",
      "parsing mcdonalds_switzerland.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2087/2087 [1:20:06<00:00,  2.30s/it] \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "93f19a117438e8ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Download Image",
   "id": "457f3553f0219cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T13:45:27.133714Z",
     "start_time": "2024-09-08T13:42:42.877470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subUrlList = ['wendys','mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada']\n",
    "#  'sonicdrivein', \n",
    "for subUrl in subUrlList:\n",
    "    print(\"parsing \"+subUrl+\".csv\")\n",
    "    insDone = pd.read_csv('Done_'+subUrl+'.csv')\n",
    "    print(\"parsing \"+subUrl+\".csv done\")\n",
    "    with tqdm(total=insDone.shape[0]) as pbar:\n",
    "        for index, row in insDone.iterrows():\n",
    "            #create img folder\n",
    "            if not os.path.exists(subUrl+'_img'):\n",
    "                os.makedirs(subUrl+'_img')\n",
    "            with open(subUrl+'_img/'+row['imgName']+'.jpg', 'wb') as f:\n",
    "                try :\n",
    "                    f.write(requests.get(row['imgSrc']).content)\n",
    "                except:\n",
    "                    print('Failed to download' + row['imgName']+'.jpg')\n",
    "            pbar.update(1)"
   ],
   "id": "f6c3dbf9b57dfe81",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing wendys.csv\n",
      "parsing wendys.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 371/371 [00:15<00:00, 23.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing mcdonalds.csv\n",
      "parsing mcdonalds.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:20<00:00, 14.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing mcdonalds_switzerland.csv\n",
      "parsing mcdonalds_switzerland.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2087/2087 [01:35<00:00, 21.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing mcdonaldscanada.csv\n",
      "parsing mcdonaldscanada.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [00:31<00:00, 26.81it/s]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-08T20:09:01.284582Z",
     "start_time": "2024-09-08T19:54:24.582398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subUrlList = ['sonicdrivein']\n",
    "#  'sonicdrivein', \n",
    "for subUrl in subUrlList:\n",
    "    print(\"parsing \"+subUrl+\".csv\")\n",
    "    insDone = pd.read_csv('Done_'+subUrl+'.csv')\n",
    "    print(\"parsing \"+subUrl+\".csv done\")\n",
    "    with tqdm(total=insDone.shape[0]) as pbar:\n",
    "        for index, row in insDone.iterrows():\n",
    "            #create img folder\n",
    "            if not os.path.exists(subUrl+'_img'):\n",
    "                os.makedirs(subUrl+'_img')\n",
    "            with open(subUrl+'_img/'+row['imgName']+'.jpg', 'wb') as f:\n",
    "                try :\n",
    "                    f.write(requests.get(row['imgSrc']).content)\n",
    "                except:\n",
    "                    print('Failed to download' + row['imgName']+'.jpg')\n",
    "            pbar.update(1)"
   ],
   "id": "38fc462f5c9e036d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing sonicdrivein.csv\n",
      "parsing sonicdrivein.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2240/2240 [14:36<00:00,  2.56it/s] \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9863f0b218a175c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
