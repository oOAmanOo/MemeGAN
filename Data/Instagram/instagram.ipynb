{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T11:33:44.266874Z",
     "start_time": "2024-09-18T11:33:40.936655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pipelining\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import time\n",
    "import lxml\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "from torch.cuda import device\n",
    "from tqdm import tqdm\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import requests\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.distributed.pipelining import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ],
   "id": "a08ed5fb6bbbabd7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Login + Page",
   "id": "7ee6472647de7328"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-16T14:07:35.818878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 抓取貼文內容--------------------\n",
    "browser = webdriver.Chrome()\n",
    "# 想要抓取資料的頁面網址\n",
    "url = 'https://www.instagram.com/' \n",
    "\n",
    "# 前往頁面\n",
    "browser.get(url)\n",
    "# ------ 填入帳號與密碼 ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.NAME, 'username')))\n",
    "\n",
    "# ------ 網頁元素定位 ------\n",
    "username_input = browser.find_element(By.NAME, \"username\")\n",
    "password_input = browser.find_element(By.NAME, \"password\")\n",
    "print(\"inputing username and password...\")\n",
    "\n",
    "# ------ 輸入帳號密碼 ------\n",
    "username_input.send_keys(\"14han_15felix_\")\n",
    "password_input.send_keys(\"Yaya0329@Instagram\")\n",
    "\n",
    "# ------ 登入 ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.XPATH,\n",
    "'//*[@id=\"loginForm\"]/div/div[3]/button/div')))\n",
    "# ------ 網頁元素定位 ------\n",
    "login_click = browser.find_element(By.XPATH, '//*[@id=\"loginForm\"]/div/div[3]/button/div')\n",
    "# ------ 點擊登入鍵 ------\n",
    "login_click.click()\n",
    "time.sleep(20)\n",
    "print(\"login successfully\")\n",
    "\n",
    "# 想要抓取資料的頁面網址\n",
    "url = 'https://www.instagram.com' \n",
    "subUrlList = ['wendys', 'sonicdrivein', 'mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada']\n",
    "\n",
    "for subUrl in subUrlList:\n",
    "    # 前往頁面\n",
    "    browser.get(url + '/' + subUrl)\n",
    "    time.sleep(2)\n",
    "    # 往下滑並取得新的貼文連結\n",
    "    post_all = []\n",
    "    postList = []\n",
    "\n",
    "    # Get scroll height.\n",
    "    last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    with tqdm(total=100) as pbar:\n",
    "        while True:\n",
    "            # Scroll down to the bottom.\n",
    "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # Wait to load the page.\n",
    "            time.sleep(5)\n",
    "    \n",
    "            soup = Soup(browser.page_source, 'lxml')\n",
    "            # 尋找所有的貼文連結\n",
    "        \n",
    "            for rowPost in soup.select('div._ac7v'):\n",
    "                for elem in rowPost.select('div a'):\n",
    "                    if elem['href'] in postList:\n",
    "                        continue\n",
    "                    postList.append(elem['href'])\n",
    "                    img_elem = elem.select('div div img')\n",
    "                    temp = []\n",
    "                    temp.append(elem['href'])\n",
    "                    try: \n",
    "                        temp.append(img_elem[0]['alt'])\n",
    "                    except:\n",
    "                        temp.append(\"\")\n",
    "                    temp.append(img_elem[0]['src'])\n",
    "                    post_all.append(temp)\n",
    "                    \n",
    "            # Calculate new scroll height and compare with last scroll height.\n",
    "            new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "            pbar.update(1)\n",
    "        \n",
    "    print(subUrl+\" 總共 \" + str(len(post_all)) + \" 篇貼文\")\n",
    "    \n",
    "    # 存成csv\n",
    "    postDataframe= pd.DataFrame(post_all)\n",
    "    postDataframe.rename(columns = {0:'url', 1:'imgAlt', 2:'imgSrc'}, inplace = True)\n",
    "    postDataframe.to_csv('Home_'+subUrl+'.csv', index = False)"
   ],
   "id": "361ec343d6ee3bd1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There was an error managing chromedriver (error sending request for url (https://googlechromelabs.github.io/chrome-for-testing/known-good-versions-with-downloads.json)); using driver found in the cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputing username and password...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 27\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# ------ 點擊登入鍵 ------\u001B[39;00m\n\u001B[0;32m     26\u001B[0m login_click\u001B[38;5;241m.\u001B[39mclick()\n\u001B[1;32m---> 27\u001B[0m \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogin successfully\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# 想要抓取資料的頁面網址\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:07:56.998817400Z",
     "start_time": "2024-09-08T11:39:19.342961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 抓取貼文內容--------------------\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--window-size=334,778\")\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "# 想要抓取資料的頁面網址\n",
    "url = 'https://www.instagram.com/' \n",
    "\n",
    "# 前往頁面\n",
    "browser.get(url)\n",
    "# ------ 填入帳號與密碼 ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.NAME, 'username')))\n",
    "\n",
    "# ------ 網頁元素定位 ------\n",
    "username_input = browser.find_element(By.NAME, \"username\")\n",
    "password_input = browser.find_element(By.NAME, \"password\")\n",
    "print(\"inputing username and password...\")\n",
    "\n",
    "# ------ 輸入帳號密碼 ------\n",
    "username_input.send_keys(\"_blood_mango_\")\n",
    "password_input.send_keys(\"wangzeng1\")\n",
    "\n",
    "# ------ 登入 ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.XPATH,\n",
    "'//*[@id=\"loginForm\"]/div/div[3]/button/div')))\n",
    "# ------ 網頁元素定位 ------\n",
    "login_click = browser.find_element(By.XPATH, '//*[@id=\"loginForm\"]/div/div[3]/button/div')\n",
    "# ------ 點擊登入鍵 ------\n",
    "login_click.click()\n",
    "time.sleep(15)\n",
    "print(\"login successfully\")\n",
    "\n",
    "# 想要抓取資料的頁面網址\n",
    "url = 'https://www.instagram.com' \n",
    "subUrlList = ['sonicdrivein']\n",
    "# 'wendys', 'mcdonalds', 'mcdonalds_switzerland', 'mcdonaldscanada', 'mcdonalds_switzerland'\n",
    "# ,'sonicdrivein'\n",
    "\n",
    "for subUrl in subUrlList:\n",
    "    print(\"parsing \"+subUrl+\".csv\")\n",
    "    # insHome = pd.read_csv('Home_'+subUrl+'.csv')\n",
    "    insHome = pd.read_csv('Done_'+subUrl+'.csv')\n",
    "    print(\"parsing \"+subUrl+\".csv done\")\n",
    "    with tqdm(total=insHome.shape[0]) as pbar:\n",
    "        counter = 0\n",
    "        for index, row in insHome.iterrows(): #< 922\n",
    "            if '/p/' in row['url'] and counter > 406 and counter < 923:\n",
    "                browser.get(url + row['url'])\n",
    "                time.sleep(10)\n",
    "                html = browser.page_source\n",
    "                soup = Soup(html, 'lxml')\n",
    "                img_elem = soup.select('img.x5yr21d')\n",
    "                text_elem = soup.select('h1._ap3a')\n",
    "\n",
    "                insHome.at[index, 'imgSrc'] = img_elem[1]['src']\n",
    "                try:\n",
    "                    insHome.at[index, 'imgAlt'] = text_elem[0].text\n",
    "                except:\n",
    "                    insHome.at[index, 'imgAlt'] = \"\"\n",
    "                insHome.to_csv('Done_'+subUrl+'.csv', index = False)\n",
    "            pbar.update(1)\n",
    "            counter = counter+1\n",
    "            \n",
    "    insHome['imgName'] = subUrl+'_'+insHome.index.astype(str)\n",
    "    insHome.to_csv('Done_'+subUrl+'.csv', index = False)"
   ],
   "id": "1d5ed7c96325588",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputing username and password...\n",
      "login successfully\n",
      "parsing mcdonalds_switzerland.csv\n",
      "parsing mcdonalds_switzerland.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2087/2087 [1:20:06<00:00,  2.30s/it] \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "93f19a117438e8ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Download Image",
   "id": "457f3553f0219cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:07:56.998817400Z",
     "start_time": "2024-09-16T13:41:17.696361Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing wendys.csv\n",
      "parsing wendys.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 371/371 [00:06<00:00, 54.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing mcdonalds.csv\n",
      "parsing mcdonalds.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 307/307 [00:10<00:00, 28.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing mcdonalds_switzerland.csv\n",
      "parsing mcdonalds_switzerland.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2087/2087 [00:47<00:00, 43.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing mcdonaldscanada.csv\n",
      "parsing mcdonaldscanada.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 850/850 [00:15<00:00, 53.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing sonicdrivein.csv\n",
      "parsing sonicdrivein.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 514/2240 [01:42<06:35,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to downloadsonicdrivein_513.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1757/2240 [06:08<03:22,  2.38it/s]"
     ]
    }
   ],
   "execution_count": null,
   "source": [
    "subUrlList = ['wendys','mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada','sonicdrivein']\n",
    "\n",
    "for subUrl in subUrlList:\n",
    "    print(\"parsing \"+subUrl+\".csv\")\n",
    "    insDone = pd.read_csv('Done_'+subUrl+'.csv')\n",
    "    print(\"parsing \"+subUrl+\".csv done\")\n",
    "    with tqdm(total=insDone.shape[0]) as pbar:\n",
    "        for index, row in insDone.iterrows():\n",
    "            #create img folder\n",
    "            if not os.path.exists(subUrl+'_img'):\n",
    "                os.makedirs(subUrl+'_img')\n",
    "            with open(subUrl+'_img/'+row['imgName']+'.jpg', 'wb') as f:\n",
    "                try :\n",
    "                    f.write(requests.get(row['imgSrc']).content)\n",
    "                except:\n",
    "                    print('Failed to download' + row['imgName']+'.jpg')\n",
    "            pbar.update(1)"
   ],
   "id": "f6c3dbf9b57dfe81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T15:18:39.179947Z",
     "start_time": "2024-09-16T14:31:46.152064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 抓取貼文內容--------------------\n",
    "browser = webdriver.Chrome()\n",
    "# 想要抓取資料的頁面網址\n",
    "url = 'https://www.instagram.com/' \n",
    "\n",
    "# 前往頁面\n",
    "browser.get(url)\n",
    "# ------ 填入帳號與密碼 ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.NAME, 'username')))\n",
    "\n",
    "# ------ 網頁元素定位 ------\n",
    "username_input = browser.find_element(By.NAME, \"username\")\n",
    "password_input = browser.find_element(By.NAME, \"password\")\n",
    "print(\"inputing username and password...\")\n",
    "\n",
    "# ------ 輸入帳號密碼 ------\n",
    "username_input.send_keys(\"_wang_zeng\")\n",
    "password_input.send_keys(\"wangzeng1\")\n",
    "\n",
    "# ------ 登入 ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.XPATH,\n",
    "'//*[@id=\"loginForm\"]/div/div[3]/button/div')))\n",
    "# ------ 網頁元素定位 ------\n",
    "login_click = browser.find_element(By.XPATH, '//*[@id=\"loginForm\"]/div/div[3]/button/div')\n",
    "# ------ 點擊登入鍵 ------\n",
    "login_click.click()\n",
    "time.sleep(20)\n",
    "print(\"login successfully\")\n",
    "\n",
    "# 想要抓取資料的頁面網址\n",
    "url = 'https://www.instagram.com' \n",
    "subUrlList = ['wendys', 'sonicdrivein', 'mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada']\n",
    "post_count=[371, 2240, 307, 2087, 850]\n",
    "counter = -1\n",
    "for subUrl in subUrlList:\n",
    "    download_counter = 0\n",
    "    counter = counter+1\n",
    "    print(\"parsing \"+subUrl+\".csv\")\n",
    "    insDone = pd.read_csv('Done_'+subUrl+'.csv')\n",
    "    print(\"parsing \"+subUrl+\".csv done\")\n",
    "    \n",
    "    # 前往頁面\n",
    "    browser.get(url + '/' + subUrl)\n",
    "    time.sleep(2)\n",
    "    # 往下滑並取得新的貼文連結\n",
    "    post_all = []\n",
    "    postList = []\n",
    "\n",
    "    # Get scroll height.\n",
    "    last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    with tqdm(total=post_count[counter]) as pbar:\n",
    "        while True:\n",
    "            # Scroll down to the bottom.\n",
    "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # Wait to load the page.\n",
    "            time.sleep(5)\n",
    "    \n",
    "            soup = Soup(browser.page_source, 'lxml')\n",
    "            # 尋找所有的貼文連結\n",
    "        \n",
    "            for rowPost in soup.select('div._ac7v'):\n",
    "                for elem in rowPost.select('div a'):\n",
    "                    if elem['href'] in postList:\n",
    "                        continue\n",
    "                        \n",
    "                    imgName = insDone[insDone['url'] == elem['href']]['imgName']\n",
    "                    if imgName.shape[0] > 0:\n",
    "                        img_elem = elem.select('div div img')\n",
    "                        imgUrl = img_elem[0]['src']\n",
    "                        if not os.path.exists(subUrl+'_img'):\n",
    "                            os.makedirs(subUrl+'_img')\n",
    "                        with open(subUrl+'_img/'+imgName.values[0]+'.jpg', 'wb') as f:\n",
    "                            try :\n",
    "                                f.write(requests.get(imgUrl).content)\n",
    "                                download_counter = download_counter+1\n",
    "                            except:\n",
    "                                print('Failed to download' + imgName+'.jpg')\n",
    "                        postList.append(elem['href'])\n",
    "                    pbar.update(1)\n",
    "            # Calculate new scroll height and compare with last scroll height.\n",
    "            new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "            \n",
    "    print(subUrl+\" 總共 \" + str(download_counter) + \" 篇圖片已下載\")\n",
    "    "
   ],
   "id": "e6f01d01cbfdc14b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputing username and password...\n",
      "login successfully\n",
      "parsing wendys.csv\n",
      "parsing wendys.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "377it [02:55,  2.15it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wendys 總共 371 篇圖片已下載\n",
      "parsing sonicdrivein.csv\n",
      "parsing sonicdrivein.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2301it [17:40,  2.17it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sonicdrivein 總共 2219 篇圖片已下載\n",
      "parsing mcdonalds.csv\n",
      "parsing mcdonalds.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [02:27,  2.12it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcdonalds 總共 307 篇圖片已下載\n",
      "parsing mcdonalds_switzerland.csv\n",
      "parsing mcdonalds_switzerland.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2202it [16:28,  2.23it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcdonalds_switzerland 總共 2050 篇圖片已下載\n",
      "parsing mcdonaldscanada.csv\n",
      "parsing mcdonaldscanada.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "877it [06:41,  2.18it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcdonaldscanada 總共 844 篇圖片已下載\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:07:57.000820800Z",
     "start_time": "2024-09-08T19:54:24.582398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 抓取貼文內容--------------------\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--window-size=334,778\")\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "# 想要抓取資料的頁面網址\n",
    "url = 'https://www.instagram.com/' \n",
    "\n",
    "# 前往頁面\n",
    "browser.get(url)\n",
    "# ------ 填入帳號與密碼 ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.NAME, 'username')))\n",
    "\n",
    "# ------ 網頁元素定位 ------\n",
    "username_input = browser.find_element(By.NAME, \"username\")\n",
    "password_input = browser.find_element(By.NAME, \"password\")\n",
    "print(\"inputing username and password...\")\n",
    "\n",
    "# ------ 輸入帳號密碼 ------\n",
    "username_input.send_keys(\"_blood_mango_\")\n",
    "password_input.send_keys(\"wangzeng1\")\n",
    "\n",
    "# ------ 登入 ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.XPATH,\n",
    "'//*[@id=\"loginForm\"]/div/div[3]/button/div')))\n",
    "# ------ 網頁元素定位 ------\n",
    "login_click = browser.find_element(By.XPATH, '//*[@id=\"loginForm\"]/div/div[3]/button/div')\n",
    "# ------ 點擊登入鍵 ------\n",
    "login_click.click()\n",
    "time.sleep(15)\n",
    "print(\"login successfully\")\n",
    "\n",
    "# 想要抓取資料的頁面網址\n",
    "url = 'https://www.instagram.com' \n",
    "\n",
    "\n",
    "for subUrl in subUrlList:\n",
    "    print(\"parsing \"+subUrl+\".csv\")\n",
    "    # insHome = pd.read_csv('Home_'+subUrl+'.csv')\n",
    "    insHome = pd.read_csv('Done_'+subUrl+'.csv')\n",
    "    print(\"parsing \"+subUrl+\".csv done\")\n",
    "    with tqdm(total=insHome.shape[0]) as pbar:\n",
    "        counter = 0\n",
    "        for index, row in insHome.iterrows(): #< 922\n",
    "            if '/p/' in row['url'] and counter > 406 and counter < 923:\n",
    "                browser.get(url + row['url'])\n",
    "                time.sleep(10)\n",
    "                html = browser.page_source\n",
    "                soup = Soup(html, 'lxml')\n",
    "                img_elem = soup.select('img.x5yr21d')\n",
    "                text_elem = soup.select('h1._ap3a')\n",
    "\n",
    "                insHome.at[index, 'imgSrc'] = img_elem[1]['src']\n",
    "                \n",
    "                    if '/reel/' in elem['href'] and imgName.shape[0] > 0:\n",
    "                        img_elem = elem.select('div div img')\n",
    "                        imgUrl = img_elem[0]['src']\n",
    "                        if not os.path.exists(subUrl+'_img'):\n",
    "                            os.makedirs(subUrl+'_img')\n",
    "                        with open(subUrl+'_img/'+imgName.values[0]+'.jpg', 'wb') as f:\n",
    "                            try :\n",
    "                                f.write(requests.get(imgUrl).content)\n",
    "                                download_counter = download_counter+1\n",
    "                            except:\n",
    "                                print('Failed to download' + imgName+'.jpg')\n",
    "                        postList.append(elem['href'])\n",
    "                    pbar.update(1)\n",
    "            pbar.update(1)\n",
    "            counter = counter+1\n",
    "            \n",
    "    insHome['imgName'] = subUrl+'_'+insHome.index.astype(str)\n",
    "    insHome.to_csv('Done_'+subUrl+'.csv', index = False)"
   ],
   "id": "38fc462f5c9e036d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing sonicdrivein.csv\n",
      "parsing sonicdrivein.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2240/2240 [14:36<00:00,  2.56it/s] \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Preprocessing",
   "id": "eca12f0d63594a70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c5ac5bfecfe38133"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T11:33:52.006491Z",
     "start_time": "2024-09-18T11:33:51.982094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(device)\n",
    "torch.cuda.current_device()"
   ],
   "id": "17163f841f85f5e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu124\n",
      "True\n",
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T11:33:53.041131Z",
     "start_time": "2024-09-18T11:33:53.037376Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.current_device()",
   "id": "58e33be0307de450",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T12:01:42.219453Z",
     "start_time": "2024-09-18T12:01:40.005613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-emoji\", device=0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")\n",
    "test_token = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")\n",
    "# 将标题字段转换为tokens\n",
    "def tokenize_bert(words):\n",
    "    # 使用BERT Tokenizer对标题进行tokenize\n",
    "    # tokens = tokenizer(words, truncation=True, padding='max_length', return_tensors='pt')\n",
    "    tokens = test_token(words, truncation=True, padding='max_length', return_tensors='pt')\n",
    "    return tokens['input_ids'].tolist()[0]"
   ],
   "id": "bffc84c0edfea356",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T11:33:58.040842Z",
     "start_time": "2024-09-18T11:33:58.032569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"👻🔥😂😁👍🤦‍♀️🤦‍♂️🤷‍♀️🤷‍♂️✌🤞😉😎🎶😢💖😜🐱‍🏍🐱‍👤🤳🎂🎉🌹💋👏🐱‍💻🐱‍🐉🐱‍👓✔👀😃✨😆🤔🤢🎁🫢 ha ha\"\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))\n",
    "print(\"=====================================\")\n",
    "print(test_token.encode(text))\n",
    "print(test_token.decode(test_token.encode(text)))"
   ],
   "id": "7277b74ffaf2fdf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 31193, 2023, 6569, 10674, 8210, 18636, 9264, 18636, 10172, 31193, 8384, 6569, 10470, 18164, 17, 8384, 38718, 7471, 12605, 6569, 10470, 18164, 17, 8384, 38718, 9264, 12605, 6569, 10470, 18400, 17, 8384, 38718, 7471, 12605, 6569, 10470, 18400, 17, 8384, 38718, 9264, 12605, 39817, 14285, 6569, 10470, 17772, 18636, 23171, 18636, 12736, 6569, 12736, 19002, 18636, 7258, 6569, 10659, 25448, 18636, 48, 6569, 16948, 15389, 17, 8384, 6569, 9357, 8384, 6569, 16948, 15389, 17, 8384, 31193, 10470, 6569, 10470, 15264, 6569, 12736, 9264, 6569, 12736, 23171, 6569, 14285, 9253, 6569, 10659, 13859, 31193, 9357, 6569, 16948, 15389, 17, 8384, 6569, 10659, 2023, 6569, 16948, 15389, 17, 8384, 6569, 16948, 23171, 6569, 16948, 15389, 17, 8384, 31193, 9085, 39817, 10674, 31193, 7471, 18636, 862, 39817, 11423, 18636, 27819, 6569, 10470, 10674, 6569, 10470, 7258, 6569, 12736, 10172, 6569, 4958, 7258, 2489, 2489, 2]\n",
      "<s>👻🔥😂😁👍🤦‍♀️🤦‍♂️🤷‍♀️🤷‍♂️✌🤞😉😎🎶😢💖😜🐱‍🏍🐱‍👤🤳🎂🎉🌹💋👏🐱‍💻🐱‍🐉🐱‍👓✔👀😃✨😆🤔🤢🎁🫢 ha ha</s>\n",
      "=====================================\n",
      "[101, 100, 5292, 5292, 102]\n",
      "[CLS] [UNK] ha ha [SEP]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T12:01:52.552293Z",
     "start_time": "2024-09-18T12:01:52.548915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(tokenize_bert(text)))\n",
    "print(len(tokenize_bert(\"text\")))"
   ],
   "id": "bf9a2ed625eb84c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "512\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T12:01:10.904140Z",
     "start_time": "2024-09-18T12:01:10.901216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(tokenize_bert(text)))\n",
    "print(len(tokenize_bert(\"text\")))"
   ],
   "id": "20eb5a5aee73fa47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n",
      "3\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T11:52:44.493609Z",
     "start_time": "2024-09-18T11:52:44.385825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from swin_transformer_v2 import SwinTransformerV2\n",
    "from swin_transformer_v2 import swin_transformer_v2_t\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "# 載入預訓練的 SwinTransformerV2 模型\n",
    "SwinTransformerV2 = swin_transformer_v2_t(in_channels=3,\n",
    "                                          window_size=8,\n",
    "                                          input_resolution=(256, 256),\n",
    "                                          sequential_self_attention=False,\n",
    "                                          use_checkpoint=False)\n",
    "\n",
    "class SwinFeatures(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SwinFeatures, self).__init__()\n",
    "        # 取出 ResNet-50 的前面部分，不包含最後的全連接層\n",
    "        # self.features = nn.Sequential(*list(model.children())[:-1])\n",
    "        self.features = nn.Sequential(*list(SwinTransformerV2.children())[:-1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 創建一個新的模型實例\n",
    "feature_extractor = SwinFeatures()\n",
    "feature_extractor.eval()  # 設置為評估模式\n",
    "\n",
    "# 定義提取特徵的方法\n",
    "def extract_features(image_path):\n",
    "    # 加載並預處理圖像\n",
    "    image = Image.open(image_path)\n",
    "    # 確保圖像是RGB格式的\n",
    "    image = image.convert('RGB')\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = preprocess(image)\n",
    "    image = image.unsqueeze(0)  # 添加批次維度\n",
    "\n",
    "    # 通過新的模型提取特徵\n",
    "    with torch.no_grad():\n",
    "        features = feature_extractor(image)\n",
    "\n",
    "    return features\n",
    "\n",
    "# 以示例圖像路徑調用提取特徵的函數\n",
    "image_path = './wendys_img/wendys_0.jpg'\n",
    "features = extract_features(image_path)\n",
    "\n",
    "# 檢視提取的特徵的形狀\n",
    "print(\"Extracted features shape:\", features.shape)"
   ],
   "id": "226c610572705a87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: torch.Size([1, 96, 64, 64])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T17:50:33.227399Z",
     "start_time": "2024-09-18T17:50:33.193692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "insFilter = pd.read_csv('./Original_File/Filter_mcdonalds_switzerland.csv')\n",
    "#drop if isMeme is -2\n",
    "print(insFilter.shape)\n",
    "insFilter = insFilter[insFilter['isMeme'] != -2]\n",
    "print(insFilter.shape)\n",
    "insFilter.to_csv('Filter_mcdonalds_switzerland.csv', index = False)"
   ],
   "id": "49d227f5a5beb7c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2087, 3)\n",
      "(1129, 3)\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T17:52:43.782331Z",
     "start_time": "2024-09-18T17:52:43.749585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subUrlList = ['wendys','mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada','sonicdrivein']\n",
    "\n",
    "for subUrl in subUrlList:\n",
    "    print(\"parsing \"+subUrl+\".csv\")\n",
    "    insFilter = pd.read_csv('./Original_File/Filter_'+subUrl+'.csv')\n",
    "    print(\"parsing \"+subUrl+\".csv done\")\n",
    "    \n",
    "    insFilter['isMeme'] = insFilter['isMeme'].apply(lambda x: 1000 if x == 1 else(100 if x == 0 else 0))\n",
    "    insFilter.to_csv('Filter_'+subUrl+'.csv', index = False)"
   ],
   "id": "737116bd9f545623",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing wendys.csv\n",
      "parsing wendys.csv done\n",
      "parsing mcdonalds.csv\n",
      "parsing mcdonalds.csv done\n",
      "parsing mcdonalds_switzerland.csv\n",
      "parsing mcdonalds_switzerland.csv done\n",
      "parsing mcdonaldscanada.csv\n",
      "parsing mcdonaldscanada.csv done\n",
      "parsing sonicdrivein.csv\n",
      "parsing sonicdrivein.csv done\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "insFilter",
   "id": "1e01ce4539bb2101"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
