{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T11:33:44.266874Z",
     "start_time": "2024-09-18T11:33:40.936655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pipelining\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "import time\n",
    "import lxml\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd\n",
    "from torch.cuda import device\n",
    "from tqdm import tqdm\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import requests\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.distributed.pipelining import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ],
   "id": "a08ed5fb6bbbabd7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Login + Page",
   "id": "7ee6472647de7328"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-16T14:07:35.818878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# æŠ“å–è²¼æ–‡å…§å®¹--------------------\n",
    "browser = webdriver.Chrome()\n",
    "# æƒ³è¦æŠ“å–è³‡æ–™çš„é é¢ç¶²å€\n",
    "url = 'https://www.instagram.com/' \n",
    "\n",
    "# å‰å¾€é é¢\n",
    "browser.get(url)\n",
    "# ------ å¡«å…¥å¸³è™Ÿèˆ‡å¯†ç¢¼ ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.NAME, 'username')))\n",
    "\n",
    "# ------ ç¶²é å…ƒç´ å®šä½ ------\n",
    "username_input = browser.find_element(By.NAME, \"username\")\n",
    "password_input = browser.find_element(By.NAME, \"password\")\n",
    "print(\"inputing username and password...\")\n",
    "\n",
    "# ------ è¼¸å…¥å¸³è™Ÿå¯†ç¢¼ ------\n",
    "username_input.send_keys(\"14han_15felix_\")\n",
    "password_input.send_keys(\"Yaya0329@Instagram\")\n",
    "\n",
    "# ------ ç™»å…¥ ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.XPATH,\n",
    "'//*[@id=\"loginForm\"]/div/div[3]/button/div')))\n",
    "# ------ ç¶²é å…ƒç´ å®šä½ ------\n",
    "login_click = browser.find_element(By.XPATH, '//*[@id=\"loginForm\"]/div/div[3]/button/div')\n",
    "# ------ é»æ“Šç™»å…¥éµ ------\n",
    "login_click.click()\n",
    "time.sleep(20)\n",
    "print(\"login successfully\")\n",
    "\n",
    "# æƒ³è¦æŠ“å–è³‡æ–™çš„é é¢ç¶²å€\n",
    "url = 'https://www.instagram.com' \n",
    "subUrlList = ['wendys', 'sonicdrivein', 'mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada']\n",
    "\n",
    "for subUrl in subUrlList:\n",
    "    # å‰å¾€é é¢\n",
    "    browser.get(url + '/' + subUrl)\n",
    "    time.sleep(2)\n",
    "    # å¾€ä¸‹æ»‘ä¸¦å–å¾—æ–°çš„è²¼æ–‡é€£çµ\n",
    "    post_all = []\n",
    "    postList = []\n",
    "\n",
    "    # Get scroll height.\n",
    "    last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    with tqdm(total=100) as pbar:\n",
    "        while True:\n",
    "            # Scroll down to the bottom.\n",
    "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # Wait to load the page.\n",
    "            time.sleep(5)\n",
    "    \n",
    "            soup = Soup(browser.page_source, 'lxml')\n",
    "            # å°‹æ‰¾æ‰€æœ‰çš„è²¼æ–‡é€£çµ\n",
    "        \n",
    "            for rowPost in soup.select('div._ac7v'):\n",
    "                for elem in rowPost.select('div a'):\n",
    "                    if elem['href'] in postList:\n",
    "                        continue\n",
    "                    postList.append(elem['href'])\n",
    "                    img_elem = elem.select('div div img')\n",
    "                    temp = []\n",
    "                    temp.append(elem['href'])\n",
    "                    try: \n",
    "                        temp.append(img_elem[0]['alt'])\n",
    "                    except:\n",
    "                        temp.append(\"\")\n",
    "                    temp.append(img_elem[0]['src'])\n",
    "                    post_all.append(temp)\n",
    "                    \n",
    "            # Calculate new scroll height and compare with last scroll height.\n",
    "            new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "            pbar.update(1)\n",
    "        \n",
    "    print(subUrl+\" ç¸½å…± \" + str(len(post_all)) + \" ç¯‡è²¼æ–‡\")\n",
    "    \n",
    "    # å­˜æˆcsv\n",
    "    postDataframe= pd.DataFrame(post_all)\n",
    "    postDataframe.rename(columns = {0:'url', 1:'imgAlt', 2:'imgSrc'}, inplace = True)\n",
    "    postDataframe.to_csv('Home_'+subUrl+'.csv', index = False)"
   ],
   "id": "361ec343d6ee3bd1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There was an error managing chromedriver (error sending request for url (https://googlechromelabs.github.io/chrome-for-testing/known-good-versions-with-downloads.json)); using driver found in the cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputing username and password...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 27\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# ------ é»æ“Šç™»å…¥éµ ------\u001B[39;00m\n\u001B[0;32m     26\u001B[0m login_click\u001B[38;5;241m.\u001B[39mclick()\n\u001B[1;32m---> 27\u001B[0m \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogin successfully\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# æƒ³è¦æŠ“å–è³‡æ–™çš„é é¢ç¶²å€\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:07:56.998817400Z",
     "start_time": "2024-09-08T11:39:19.342961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# æŠ“å–è²¼æ–‡å…§å®¹--------------------\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--window-size=334,778\")\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "# æƒ³è¦æŠ“å–è³‡æ–™çš„é é¢ç¶²å€\n",
    "url = 'https://www.instagram.com/' \n",
    "\n",
    "# å‰å¾€é é¢\n",
    "browser.get(url)\n",
    "# ------ å¡«å…¥å¸³è™Ÿèˆ‡å¯†ç¢¼ ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.NAME, 'username')))\n",
    "\n",
    "# ------ ç¶²é å…ƒç´ å®šä½ ------\n",
    "username_input = browser.find_element(By.NAME, \"username\")\n",
    "password_input = browser.find_element(By.NAME, \"password\")\n",
    "print(\"inputing username and password...\")\n",
    "\n",
    "# ------ è¼¸å…¥å¸³è™Ÿå¯†ç¢¼ ------\n",
    "username_input.send_keys(\"_blood_mango_\")\n",
    "password_input.send_keys(\"wangzeng1\")\n",
    "\n",
    "# ------ ç™»å…¥ ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.XPATH,\n",
    "'//*[@id=\"loginForm\"]/div/div[3]/button/div')))\n",
    "# ------ ç¶²é å…ƒç´ å®šä½ ------\n",
    "login_click = browser.find_element(By.XPATH, '//*[@id=\"loginForm\"]/div/div[3]/button/div')\n",
    "# ------ é»æ“Šç™»å…¥éµ ------\n",
    "login_click.click()\n",
    "time.sleep(15)\n",
    "print(\"login successfully\")\n",
    "\n",
    "# æƒ³è¦æŠ“å–è³‡æ–™çš„é é¢ç¶²å€\n",
    "url = 'https://www.instagram.com' \n",
    "subUrlList = ['sonicdrivein']\n",
    "# 'wendys', 'mcdonalds', 'mcdonalds_switzerland', 'mcdonaldscanada', 'mcdonalds_switzerland'\n",
    "# ,'sonicdrivein'\n",
    "\n",
    "for subUrl in subUrlList:\n",
    "    print(\"parsing \"+subUrl+\".csv\")\n",
    "    # insHome = pd.read_csv('Home_'+subUrl+'.csv')\n",
    "    insHome = pd.read_csv('Done_'+subUrl+'.csv')\n",
    "    print(\"parsing \"+subUrl+\".csv done\")\n",
    "    with tqdm(total=insHome.shape[0]) as pbar:\n",
    "        counter = 0\n",
    "        for index, row in insHome.iterrows(): #< 922\n",
    "            if '/p/' in row['url'] and counter > 406 and counter < 923:\n",
    "                browser.get(url + row['url'])\n",
    "                time.sleep(10)\n",
    "                html = browser.page_source\n",
    "                soup = Soup(html, 'lxml')\n",
    "                img_elem = soup.select('img.x5yr21d')\n",
    "                text_elem = soup.select('h1._ap3a')\n",
    "\n",
    "                insHome.at[index, 'imgSrc'] = img_elem[1]['src']\n",
    "                try:\n",
    "                    insHome.at[index, 'imgAlt'] = text_elem[0].text\n",
    "                except:\n",
    "                    insHome.at[index, 'imgAlt'] = \"\"\n",
    "                insHome.to_csv('Done_'+subUrl+'.csv', index = False)\n",
    "            pbar.update(1)\n",
    "            counter = counter+1\n",
    "            \n",
    "    insHome['imgName'] = subUrl+'_'+insHome.index.astype(str)\n",
    "    insHome.to_csv('Done_'+subUrl+'.csv', index = False)"
   ],
   "id": "1d5ed7c96325588",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputing username and password...\n",
      "login successfully\n",
      "parsing mcdonalds_switzerland.csv\n",
      "parsing mcdonalds_switzerland.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2087/2087 [1:20:06<00:00,  2.30s/it] \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "93f19a117438e8ec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Download Image",
   "id": "457f3553f0219cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:07:56.998817400Z",
     "start_time": "2024-09-16T13:41:17.696361Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing wendys.csv\n",
      "parsing wendys.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 371/371 [00:06<00:00, 54.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing mcdonalds.csv\n",
      "parsing mcdonalds.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 307/307 [00:10<00:00, 28.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing mcdonalds_switzerland.csv\n",
      "parsing mcdonalds_switzerland.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2087/2087 [00:47<00:00, 43.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing mcdonaldscanada.csv\n",
      "parsing mcdonaldscanada.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 850/850 [00:15<00:00, 53.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing sonicdrivein.csv\n",
      "parsing sonicdrivein.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–       | 514/2240 [01:42<06:35,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to downloadsonicdrivein_513.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1757/2240 [06:08<03:22,  2.38it/s]"
     ]
    }
   ],
   "execution_count": null,
   "source": [
    "subUrlList = ['wendys','mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada','sonicdrivein']\n",
    "\n",
    "for subUrl in subUrlList:\n",
    "    print(\"parsing \"+subUrl+\".csv\")\n",
    "    insDone = pd.read_csv('Done_'+subUrl+'.csv')\n",
    "    print(\"parsing \"+subUrl+\".csv done\")\n",
    "    with tqdm(total=insDone.shape[0]) as pbar:\n",
    "        for index, row in insDone.iterrows():\n",
    "            #create img folder\n",
    "            if not os.path.exists(subUrl+'_img'):\n",
    "                os.makedirs(subUrl+'_img')\n",
    "            with open(subUrl+'_img/'+row['imgName']+'.jpg', 'wb') as f:\n",
    "                try :\n",
    "                    f.write(requests.get(row['imgSrc']).content)\n",
    "                except:\n",
    "                    print('Failed to download' + row['imgName']+'.jpg')\n",
    "            pbar.update(1)"
   ],
   "id": "f6c3dbf9b57dfe81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T15:18:39.179947Z",
     "start_time": "2024-09-16T14:31:46.152064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# æŠ“å–è²¼æ–‡å…§å®¹--------------------\n",
    "browser = webdriver.Chrome()\n",
    "# æƒ³è¦æŠ“å–è³‡æ–™çš„é é¢ç¶²å€\n",
    "url = 'https://www.instagram.com/' \n",
    "\n",
    "# å‰å¾€é é¢\n",
    "browser.get(url)\n",
    "# ------ å¡«å…¥å¸³è™Ÿèˆ‡å¯†ç¢¼ ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.NAME, 'username')))\n",
    "\n",
    "# ------ ç¶²é å…ƒç´ å®šä½ ------\n",
    "username_input = browser.find_element(By.NAME, \"username\")\n",
    "password_input = browser.find_element(By.NAME, \"password\")\n",
    "print(\"inputing username and password...\")\n",
    "\n",
    "# ------ è¼¸å…¥å¸³è™Ÿå¯†ç¢¼ ------\n",
    "username_input.send_keys(\"_wang_zeng\")\n",
    "password_input.send_keys(\"wangzeng1\")\n",
    "\n",
    "# ------ ç™»å…¥ ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.XPATH,\n",
    "'//*[@id=\"loginForm\"]/div/div[3]/button/div')))\n",
    "# ------ ç¶²é å…ƒç´ å®šä½ ------\n",
    "login_click = browser.find_element(By.XPATH, '//*[@id=\"loginForm\"]/div/div[3]/button/div')\n",
    "# ------ é»æ“Šç™»å…¥éµ ------\n",
    "login_click.click()\n",
    "time.sleep(20)\n",
    "print(\"login successfully\")\n",
    "\n",
    "# æƒ³è¦æŠ“å–è³‡æ–™çš„é é¢ç¶²å€\n",
    "url = 'https://www.instagram.com' \n",
    "subUrlList = ['wendys', 'sonicdrivein', 'mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada']\n",
    "post_count=[371, 2240, 307, 2087, 850]\n",
    "counter = -1\n",
    "for subUrl in subUrlList:\n",
    "    download_counter = 0\n",
    "    counter = counter+1\n",
    "    print(\"parsing \"+subUrl+\".csv\")\n",
    "    insDone = pd.read_csv('Done_'+subUrl+'.csv')\n",
    "    print(\"parsing \"+subUrl+\".csv done\")\n",
    "    \n",
    "    # å‰å¾€é é¢\n",
    "    browser.get(url + '/' + subUrl)\n",
    "    time.sleep(2)\n",
    "    # å¾€ä¸‹æ»‘ä¸¦å–å¾—æ–°çš„è²¼æ–‡é€£çµ\n",
    "    post_all = []\n",
    "    postList = []\n",
    "\n",
    "    # Get scroll height.\n",
    "    last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    with tqdm(total=post_count[counter]) as pbar:\n",
    "        while True:\n",
    "            # Scroll down to the bottom.\n",
    "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # Wait to load the page.\n",
    "            time.sleep(5)\n",
    "    \n",
    "            soup = Soup(browser.page_source, 'lxml')\n",
    "            # å°‹æ‰¾æ‰€æœ‰çš„è²¼æ–‡é€£çµ\n",
    "        \n",
    "            for rowPost in soup.select('div._ac7v'):\n",
    "                for elem in rowPost.select('div a'):\n",
    "                    if elem['href'] in postList:\n",
    "                        continue\n",
    "                        \n",
    "                    imgName = insDone[insDone['url'] == elem['href']]['imgName']\n",
    "                    if imgName.shape[0] > 0:\n",
    "                        img_elem = elem.select('div div img')\n",
    "                        imgUrl = img_elem[0]['src']\n",
    "                        if not os.path.exists(subUrl+'_img'):\n",
    "                            os.makedirs(subUrl+'_img')\n",
    "                        with open(subUrl+'_img/'+imgName.values[0]+'.jpg', 'wb') as f:\n",
    "                            try :\n",
    "                                f.write(requests.get(imgUrl).content)\n",
    "                                download_counter = download_counter+1\n",
    "                            except:\n",
    "                                print('Failed to download' + imgName+'.jpg')\n",
    "                        postList.append(elem['href'])\n",
    "                    pbar.update(1)\n",
    "            # Calculate new scroll height and compare with last scroll height.\n",
    "            new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "            \n",
    "    print(subUrl+\" ç¸½å…± \" + str(download_counter) + \" ç¯‡åœ–ç‰‡å·²ä¸‹è¼‰\")\n",
    "    "
   ],
   "id": "e6f01d01cbfdc14b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputing username and password...\n",
      "login successfully\n",
      "parsing wendys.csv\n",
      "parsing wendys.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "377it [02:55,  2.15it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wendys ç¸½å…± 371 ç¯‡åœ–ç‰‡å·²ä¸‹è¼‰\n",
      "parsing sonicdrivein.csv\n",
      "parsing sonicdrivein.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2301it [17:40,  2.17it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sonicdrivein ç¸½å…± 2219 ç¯‡åœ–ç‰‡å·²ä¸‹è¼‰\n",
      "parsing mcdonalds.csv\n",
      "parsing mcdonalds.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "313it [02:27,  2.12it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcdonalds ç¸½å…± 307 ç¯‡åœ–ç‰‡å·²ä¸‹è¼‰\n",
      "parsing mcdonalds_switzerland.csv\n",
      "parsing mcdonalds_switzerland.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2202it [16:28,  2.23it/s]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcdonalds_switzerland ç¸½å…± 2050 ç¯‡åœ–ç‰‡å·²ä¸‹è¼‰\n",
      "parsing mcdonaldscanada.csv\n",
      "parsing mcdonaldscanada.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "877it [06:41,  2.18it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcdonaldscanada ç¸½å…± 844 ç¯‡åœ–ç‰‡å·²ä¸‹è¼‰\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T14:07:57.000820800Z",
     "start_time": "2024-09-08T19:54:24.582398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# æŠ“å–è²¼æ–‡å…§å®¹--------------------\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "chrome_options.add_argument(\"--window-size=334,778\")\n",
    "browser = webdriver.Chrome(options=chrome_options)\n",
    "# æƒ³è¦æŠ“å–è³‡æ–™çš„é é¢ç¶²å€\n",
    "url = 'https://www.instagram.com/' \n",
    "\n",
    "# å‰å¾€é é¢\n",
    "browser.get(url)\n",
    "# ------ å¡«å…¥å¸³è™Ÿèˆ‡å¯†ç¢¼ ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.NAME, 'username')))\n",
    "\n",
    "# ------ ç¶²é å…ƒç´ å®šä½ ------\n",
    "username_input = browser.find_element(By.NAME, \"username\")\n",
    "password_input = browser.find_element(By.NAME, \"password\")\n",
    "print(\"inputing username and password...\")\n",
    "\n",
    "# ------ è¼¸å…¥å¸³è™Ÿå¯†ç¢¼ ------\n",
    "username_input.send_keys(\"_blood_mango_\")\n",
    "password_input.send_keys(\"wangzeng1\")\n",
    "\n",
    "# ------ ç™»å…¥ ------\n",
    "WebDriverWait(browser, 30).until(EC.presence_of_element_located((By.XPATH,\n",
    "'//*[@id=\"loginForm\"]/div/div[3]/button/div')))\n",
    "# ------ ç¶²é å…ƒç´ å®šä½ ------\n",
    "login_click = browser.find_element(By.XPATH, '//*[@id=\"loginForm\"]/div/div[3]/button/div')\n",
    "# ------ é»æ“Šç™»å…¥éµ ------\n",
    "login_click.click()\n",
    "time.sleep(15)\n",
    "print(\"login successfully\")\n",
    "\n",
    "# æƒ³è¦æŠ“å–è³‡æ–™çš„é é¢ç¶²å€\n",
    "url = 'https://www.instagram.com' \n",
    "\n",
    "\n",
    "for subUrl in subUrlList:\n",
    "    print(\"parsing \"+subUrl+\".csv\")\n",
    "    # insHome = pd.read_csv('Home_'+subUrl+'.csv')\n",
    "    insHome = pd.read_csv('Done_'+subUrl+'.csv')\n",
    "    print(\"parsing \"+subUrl+\".csv done\")\n",
    "    with tqdm(total=insHome.shape[0]) as pbar:\n",
    "        counter = 0\n",
    "        for index, row in insHome.iterrows(): #< 922\n",
    "            if '/p/' in row['url'] and counter > 406 and counter < 923:\n",
    "                browser.get(url + row['url'])\n",
    "                time.sleep(10)\n",
    "                html = browser.page_source\n",
    "                soup = Soup(html, 'lxml')\n",
    "                img_elem = soup.select('img.x5yr21d')\n",
    "                text_elem = soup.select('h1._ap3a')\n",
    "\n",
    "                insHome.at[index, 'imgSrc'] = img_elem[1]['src']\n",
    "                \n",
    "                    if '/reel/' in elem['href'] and imgName.shape[0] > 0:\n",
    "                        img_elem = elem.select('div div img')\n",
    "                        imgUrl = img_elem[0]['src']\n",
    "                        if not os.path.exists(subUrl+'_img'):\n",
    "                            os.makedirs(subUrl+'_img')\n",
    "                        with open(subUrl+'_img/'+imgName.values[0]+'.jpg', 'wb') as f:\n",
    "                            try :\n",
    "                                f.write(requests.get(imgUrl).content)\n",
    "                                download_counter = download_counter+1\n",
    "                            except:\n",
    "                                print('Failed to download' + imgName+'.jpg')\n",
    "                        postList.append(elem['href'])\n",
    "                    pbar.update(1)\n",
    "            pbar.update(1)\n",
    "            counter = counter+1\n",
    "            \n",
    "    insHome['imgName'] = subUrl+'_'+insHome.index.astype(str)\n",
    "    insHome.to_csv('Done_'+subUrl+'.csv', index = False)"
   ],
   "id": "38fc462f5c9e036d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing sonicdrivein.csv\n",
      "parsing sonicdrivein.csv done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2240/2240 [14:36<00:00,  2.56it/s] \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Preprocessing",
   "id": "eca12f0d63594a70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c5ac5bfecfe38133"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T11:33:52.006491Z",
     "start_time": "2024-09-18T11:33:51.982094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(device)\n",
    "torch.cuda.current_device()"
   ],
   "id": "17163f841f85f5e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu124\n",
      "True\n",
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T11:33:53.041131Z",
     "start_time": "2024-09-18T11:33:53.037376Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.current_device()",
   "id": "58e33be0307de450",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T12:01:42.219453Z",
     "start_time": "2024-09-18T12:01:40.005613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipe = pipeline(\"text-classification\", model=\"cardiffnlp/twitter-roberta-base-emoji\", device=0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")\n",
    "test_token = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-emoji\")\n",
    "# å°†æ ‡é¢˜å­—æ®µè½¬æ¢ä¸ºtokens\n",
    "def tokenize_bert(words):\n",
    "    # ä½¿ç”¨BERT Tokenizerå¯¹æ ‡é¢˜è¿›è¡Œtokenize\n",
    "    # tokens = tokenizer(words, truncation=True, padding='max_length', return_tensors='pt')\n",
    "    tokens = test_token(words, truncation=True, padding='max_length', return_tensors='pt')\n",
    "    return tokens['input_ids'].tolist()[0]"
   ],
   "id": "bffc84c0edfea356",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T11:33:58.040842Z",
     "start_time": "2024-09-18T11:33:58.032569Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"ğŸ‘»ğŸ”¥ğŸ˜‚ğŸ˜ğŸ‘ğŸ¤¦â€â™€ï¸ğŸ¤¦â€â™‚ï¸ğŸ¤·â€â™€ï¸ğŸ¤·â€â™‚ï¸âœŒğŸ¤ğŸ˜‰ğŸ˜ğŸ¶ğŸ˜¢ğŸ’–ğŸ˜œğŸ±â€ğŸğŸ±â€ğŸ‘¤ğŸ¤³ğŸ‚ğŸ‰ğŸŒ¹ğŸ’‹ğŸ‘ğŸ±â€ğŸ’»ğŸ±â€ğŸ‰ğŸ±â€ğŸ‘“âœ”ğŸ‘€ğŸ˜ƒâœ¨ğŸ˜†ğŸ¤”ğŸ¤¢ğŸğŸ«¢ ha ha\"\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))\n",
    "print(\"=====================================\")\n",
    "print(test_token.encode(text))\n",
    "print(test_token.decode(test_token.encode(text)))"
   ],
   "id": "7277b74ffaf2fdf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 31193, 2023, 6569, 10674, 8210, 18636, 9264, 18636, 10172, 31193, 8384, 6569, 10470, 18164, 17, 8384, 38718, 7471, 12605, 6569, 10470, 18164, 17, 8384, 38718, 9264, 12605, 6569, 10470, 18400, 17, 8384, 38718, 7471, 12605, 6569, 10470, 18400, 17, 8384, 38718, 9264, 12605, 39817, 14285, 6569, 10470, 17772, 18636, 23171, 18636, 12736, 6569, 12736, 19002, 18636, 7258, 6569, 10659, 25448, 18636, 48, 6569, 16948, 15389, 17, 8384, 6569, 9357, 8384, 6569, 16948, 15389, 17, 8384, 31193, 10470, 6569, 10470, 15264, 6569, 12736, 9264, 6569, 12736, 23171, 6569, 14285, 9253, 6569, 10659, 13859, 31193, 9357, 6569, 16948, 15389, 17, 8384, 6569, 10659, 2023, 6569, 16948, 15389, 17, 8384, 6569, 16948, 23171, 6569, 16948, 15389, 17, 8384, 31193, 9085, 39817, 10674, 31193, 7471, 18636, 862, 39817, 11423, 18636, 27819, 6569, 10470, 10674, 6569, 10470, 7258, 6569, 12736, 10172, 6569, 4958, 7258, 2489, 2489, 2]\n",
      "<s>ğŸ‘»ğŸ”¥ğŸ˜‚ğŸ˜ğŸ‘ğŸ¤¦â€â™€ï¸ğŸ¤¦â€â™‚ï¸ğŸ¤·â€â™€ï¸ğŸ¤·â€â™‚ï¸âœŒğŸ¤ğŸ˜‰ğŸ˜ğŸ¶ğŸ˜¢ğŸ’–ğŸ˜œğŸ±â€ğŸğŸ±â€ğŸ‘¤ğŸ¤³ğŸ‚ğŸ‰ğŸŒ¹ğŸ’‹ğŸ‘ğŸ±â€ğŸ’»ğŸ±â€ğŸ‰ğŸ±â€ğŸ‘“âœ”ğŸ‘€ğŸ˜ƒâœ¨ğŸ˜†ğŸ¤”ğŸ¤¢ğŸğŸ«¢ ha ha</s>\n",
      "=====================================\n",
      "[101, 100, 5292, 5292, 102]\n",
      "[CLS] [UNK] ha ha [SEP]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T12:01:52.552293Z",
     "start_time": "2024-09-18T12:01:52.548915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(tokenize_bert(text)))\n",
    "print(len(tokenize_bert(\"text\")))"
   ],
   "id": "bf9a2ed625eb84c6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "512\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T12:01:10.904140Z",
     "start_time": "2024-09-18T12:01:10.901216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(tokenize_bert(text)))\n",
    "print(len(tokenize_bert(\"text\")))"
   ],
   "id": "20eb5a5aee73fa47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n",
      "3\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T11:52:44.493609Z",
     "start_time": "2024-09-18T11:52:44.385825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from swin_transformer_v2 import SwinTransformerV2\n",
    "from swin_transformer_v2 import swin_transformer_v2_t\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "# è¼‰å…¥é è¨“ç·´çš„ SwinTransformerV2 æ¨¡å‹\n",
    "SwinTransformerV2 = swin_transformer_v2_t(in_channels=3,\n",
    "                                          window_size=8,\n",
    "                                          input_resolution=(256, 256),\n",
    "                                          sequential_self_attention=False,\n",
    "                                          use_checkpoint=False)\n",
    "\n",
    "class SwinFeatures(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SwinFeatures, self).__init__()\n",
    "        # å–å‡º ResNet-50 çš„å‰é¢éƒ¨åˆ†ï¼Œä¸åŒ…å«æœ€å¾Œçš„å…¨é€£æ¥å±¤\n",
    "        # self.features = nn.Sequential(*list(model.children())[:-1])\n",
    "        self.features = nn.Sequential(*list(SwinTransformerV2.children())[:-1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# å‰µå»ºä¸€å€‹æ–°çš„æ¨¡å‹å¯¦ä¾‹\n",
    "feature_extractor = SwinFeatures()\n",
    "feature_extractor.eval()  # è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼\n",
    "\n",
    "# å®šç¾©æå–ç‰¹å¾µçš„æ–¹æ³•\n",
    "def extract_features(image_path):\n",
    "    # åŠ è¼‰ä¸¦é è™•ç†åœ–åƒ\n",
    "    image = Image.open(image_path)\n",
    "    # ç¢ºä¿åœ–åƒæ˜¯RGBæ ¼å¼çš„\n",
    "    image = image.convert('RGB')\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = preprocess(image)\n",
    "    image = image.unsqueeze(0)  # æ·»åŠ æ‰¹æ¬¡ç¶­åº¦\n",
    "\n",
    "    # é€šéæ–°çš„æ¨¡å‹æå–ç‰¹å¾µ\n",
    "    with torch.no_grad():\n",
    "        features = feature_extractor(image)\n",
    "\n",
    "    return features\n",
    "\n",
    "# ä»¥ç¤ºä¾‹åœ–åƒè·¯å¾‘èª¿ç”¨æå–ç‰¹å¾µçš„å‡½æ•¸\n",
    "image_path = './wendys_img/wendys_0.jpg'\n",
    "features = extract_features(image_path)\n",
    "\n",
    "# æª¢è¦–æå–çš„ç‰¹å¾µçš„å½¢ç‹€\n",
    "print(\"Extracted features shape:\", features.shape)"
   ],
   "id": "226c610572705a87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: torch.Size([1, 96, 64, 64])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T17:50:33.227399Z",
     "start_time": "2024-09-18T17:50:33.193692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "insFilter = pd.read_csv('./Original_File/Filter_mcdonalds_switzerland.csv')\n",
    "#drop if isMeme is -2\n",
    "print(insFilter.shape)\n",
    "insFilter = insFilter[insFilter['isMeme'] != -2]\n",
    "print(insFilter.shape)\n",
    "insFilter.to_csv('Filter_mcdonalds_switzerland.csv', index = False)"
   ],
   "id": "49d227f5a5beb7c5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2087, 3)\n",
      "(1129, 3)\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-18T17:52:43.782331Z",
     "start_time": "2024-09-18T17:52:43.749585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "subUrlList = ['wendys','mcdonalds', 'mcdonalds_switzerland','mcdonaldscanada','sonicdrivein']\n",
    "\n",
    "for subUrl in subUrlList:\n",
    "    print(\"parsing \"+subUrl+\".csv\")\n",
    "    insFilter = pd.read_csv('./Original_File/Filter_'+subUrl+'.csv')\n",
    "    print(\"parsing \"+subUrl+\".csv done\")\n",
    "    \n",
    "    insFilter['isMeme'] = insFilter['isMeme'].apply(lambda x: 1000 if x == 1 else(100 if x == 0 else 0))\n",
    "    insFilter.to_csv('Filter_'+subUrl+'.csv', index = False)"
   ],
   "id": "737116bd9f545623",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing wendys.csv\n",
      "parsing wendys.csv done\n",
      "parsing mcdonalds.csv\n",
      "parsing mcdonalds.csv done\n",
      "parsing mcdonalds_switzerland.csv\n",
      "parsing mcdonalds_switzerland.csv done\n",
      "parsing mcdonaldscanada.csv\n",
      "parsing mcdonaldscanada.csv done\n",
      "parsing sonicdrivein.csv\n",
      "parsing sonicdrivein.csv done\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "insFilter",
   "id": "1e01ce4539bb2101"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
